<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <title>Network Representation of Constructional Semantics</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="ICCG11-Presentation_files/reveal.js-3.3.0.1/css/reveal.css"/>



<link rel="stylesheet" href="ICCG11-Presentation_files/reveal.js-3.3.0.1/css/theme/simple.css" id="theme">


  <!-- some tweaks to reveal css -->
  <style type="text/css">
    .reveal h1 { font-size: 2.0em; }
    .reveal h2 { font-size: 1.5em;  }
    .reveal h3 { font-size: 1.25em;	}
    .reveal h4 { font-size: 1em;	}

    .reveal .slides>section,
    .reveal .slides>section>section {
      padding: 0px 0px;
    }



    .reveal table {
      border-width: 1px;
      border-spacing: 2px;
      border-style: dotted;
      border-color: gray;
      border-collapse: collapse;
      font-size: 0.7em;
    }

    .reveal table th {
      border-width: 1px;
      padding-left: 10px;
      padding-right: 25px;
      font-weight: bold;
      border-style: dotted;
      border-color: gray;
    }

    .reveal table td {
      border-width: 1px;
      padding-left: 10px;
      padding-right: 25px;
      border-style: dotted;
      border-color: gray;
    }

    .reveal .slide-menu-button {
      left: 105px !important;
    }

  </style>

    <style type="text/css">code{white-space: pre;}</style>

    <link rel="stylesheet" href="_static/revealstyle.css"/>

<!-- Printing and PDF exports -->
<script id="paper-css" type="application/dynamic-css">

/* Default Print Stylesheet Template
   by Rob Glazebrook of CSSnewbie.com
   Last Updated: June 4, 2008

   Feel free (nay, compelled) to edit, append, and
   manipulate this file as you see fit. */


@media print {

	/* SECTION 1: Set default width, margin, float, and
	   background. This prevents elements from extending
	   beyond the edge of the printed page, and prevents
	   unnecessary background images from printing */
	html {
		background: #fff;
		width: auto;
		height: auto;
		overflow: visible;
	}
	body {
		background: #fff;
		font-size: 20pt;
		width: auto;
		height: auto;
		border: 0;
		margin: 0 5%;
		padding: 0;
		overflow: visible;
		float: none !important;
	}

	/* SECTION 2: Remove any elements not needed in print.
	   This would include navigation, ads, sidebars, etc. */
	.nestedarrow,
	.controls,
	.fork-reveal,
	.share-reveal,
	.state-background,
	.reveal .progress,
	.reveal .backgrounds {
		display: none !important;
	}

	/* SECTION 3: Set body font face, size, and color.
	   Consider using a serif font for readability. */
	body, p, td, li, div {
		font-size: 20pt!important;
		font-family: Georgia, "Times New Roman", Times, serif !important;
		color: #000;
	}

	/* SECTION 4: Set heading font face, sizes, and color.
	   Differentiate your headings from your body text.
	   Perhaps use a large sans-serif for distinction. */
	h1,h2,h3,h4,h5,h6 {
		color: #000!important;
		height: auto;
		line-height: normal;
		font-family: Georgia, "Times New Roman", Times, serif !important;
		text-shadow: 0 0 0 #000 !important;
		text-align: left;
		letter-spacing: normal;
	}
	/* Need to reduce the size of the fonts for printing */
	h1 { font-size: 28pt !important;  }
	h2 { font-size: 24pt !important; }
	h3 { font-size: 22pt !important; }
	h4 { font-size: 22pt !important; font-variant: small-caps; }
	h5 { font-size: 21pt !important; }
	h6 { font-size: 20pt !important; font-style: italic; }

	/* SECTION 5: Make hyperlinks more usable.
	   Ensure links are underlined, and consider appending
	   the URL to the end of the link for usability. */
	a:link,
	a:visited {
		color: #000 !important;
		font-weight: bold;
		text-decoration: underline;
	}
	/*
	.reveal a:link:after,
	.reveal a:visited:after {
		content: " (" attr(href) ") ";
		color: #222 !important;
		font-size: 90%;
	}
	*/


	/* SECTION 6: more reveal.js specific additions by @skypanther */
	ul, ol, div, p {
		visibility: visible;
		position: static;
		width: auto;
		height: auto;
		display: block;
		overflow: visible;
		margin: 0;
		text-align: left !important;
	}
	.reveal pre,
	.reveal table {
		margin-left: 0;
		margin-right: 0;
	}
	.reveal pre code {
		padding: 20px;
		border: 1px solid #ddd;
	}
	.reveal blockquote {
		margin: 20px 0;
	}
	.reveal .slides {
		position: static !important;
		width: auto !important;
		height: auto !important;

		left: 0 !important;
		top: 0 !important;
		margin-left: 0 !important;
		margin-top: 0 !important;
		padding: 0 !important;
		zoom: 1 !important;

		overflow: visible !important;
		display: block !important;

		text-align: left !important;
		-webkit-perspective: none;
		   -moz-perspective: none;
		    -ms-perspective: none;
		        perspective: none;

		-webkit-perspective-origin: 50% 50%;
		   -moz-perspective-origin: 50% 50%;
		    -ms-perspective-origin: 50% 50%;
		        perspective-origin: 50% 50%;
	}
	.reveal .slides section {
		visibility: visible !important;
		position: static !important;
		width: auto !important;
		height: auto !important;
		display: block !important;
		overflow: visible !important;

		left: 0 !important;
		top: 0 !important;
		margin-left: 0 !important;
		margin-top: 0 !important;
		padding: 60px 20px !important;
		z-index: auto !important;

		opacity: 1 !important;

		page-break-after: always !important;

		-webkit-transform-style: flat !important;
		   -moz-transform-style: flat !important;
		    -ms-transform-style: flat !important;
		        transform-style: flat !important;

		-webkit-transform: none !important;
		   -moz-transform: none !important;
		    -ms-transform: none !important;
		        transform: none !important;

		-webkit-transition: none !important;
		   -moz-transition: none !important;
		    -ms-transition: none !important;
		        transition: none !important;
	}
	.reveal .slides section.stack {
		padding: 0 !important;
	}
	.reveal section:last-of-type {
		page-break-after: avoid !important;
	}
	.reveal section .fragment {
		opacity: 1 !important;
		visibility: visible !important;

		-webkit-transform: none !important;
		   -moz-transform: none !important;
		    -ms-transform: none !important;
		        transform: none !important;
	}
	.reveal section img {
		display: block;
		margin: 15px 0px;
		background: rgba(255,255,255,1);
		border: 1px solid #666;
		box-shadow: none;
	}

	.reveal section small {
		font-size: 0.8em;
	}

}  
</script>


<script id="pdf-css" type="application/dynamic-css">
    
/**
 * This stylesheet is used to print reveal.js
 * presentations to PDF.
 *
 * https://github.com/hakimel/reveal.js#pdf-export
 */

* {
	-webkit-print-color-adjust: exact;
}

body {
	margin: 0 auto !important;
	border: 0;
	padding: 0;
	float: none !important;
	overflow: visible;
}

html {
	width: 100%;
	height: 100%;
	overflow: visible;
}

/* Remove any elements not needed in print. */
.nestedarrow,
.reveal .controls,
.reveal .progress,
.reveal .playback,
.reveal.overview,
.fork-reveal,
.share-reveal,
.state-background {
	display: none !important;
}

h1, h2, h3, h4, h5, h6 {
	text-shadow: 0 0 0 #000 !important;
}

.reveal pre code {
	overflow: hidden !important;
	font-family: Courier, 'Courier New', monospace !important;
}

ul, ol, div, p {
	visibility: visible;
	position: static;
	width: auto;
	height: auto;
	display: block;
	overflow: visible;
	margin: auto;
}
.reveal {
	width: auto !important;
	height: auto !important;
	overflow: hidden !important;
}
.reveal .slides {
	position: static;
	width: 100%;
	height: auto;

	left: auto;
	top: auto;
	margin: 0 !important;
	padding: 0 !important;

	overflow: visible;
	display: block;

	-webkit-perspective: none;
	   -moz-perspective: none;
	    -ms-perspective: none;
	        perspective: none;

	-webkit-perspective-origin: 50% 50%; /* there isn't a none/auto value but 50-50 is the default */
	   -moz-perspective-origin: 50% 50%;
	    -ms-perspective-origin: 50% 50%;
	        perspective-origin: 50% 50%;
}

.reveal .slides section {
	page-break-after: always !important;

	visibility: visible !important;
	position: relative !important;
	display: block !important;
	position: relative !important;

	margin: 0 !important;
	padding: 0 !important;
	box-sizing: border-box !important;
	min-height: 1px;

	opacity: 1 !important;

	-webkit-transform-style: flat !important;
	   -moz-transform-style: flat !important;
	    -ms-transform-style: flat !important;
	        transform-style: flat !important;

	-webkit-transform: none !important;
	   -moz-transform: none !important;
	    -ms-transform: none !important;
	        transform: none !important;
}

.reveal section.stack {
	margin: 0 !important;
	padding: 0 !important;
	page-break-after: avoid !important;
	height: auto !important;
	min-height: auto !important;
}

.reveal img {
	box-shadow: none;
}

.reveal .roll {
	overflow: visible;
	line-height: 1em;
}

/* Slide backgrounds are placed inside of their slide when exporting to PDF */
.reveal section .slide-background {
	display: block !important;
	position: absolute;
	top: 0;
	left: 0;
	width: 100%;
	z-index: -1;
}

/* All elements should be above the slide-background */
.reveal section>* {
	position: relative;
	z-index: 1;
}

/* Display slide speaker notes when 'showNotes' is enabled */
.reveal .speaker-notes-pdf {
	display: block;
	width: 100%;
	max-height: none;
	left: auto;
	top: auto;
	z-index: 100;
}

/* Display slide numbers when 'slideNumber' is enabled */
.reveal .slide-number-pdf {
	display: block;
	position: absolute;
	font-size: 14px;
}

</script>


<script>
var style = document.createElement( 'style' );
style.type = 'text/css';
var style_script_id = window.location.search.match( /print-pdf/gi ) ? 'pdf-css' : 'paper-css';
var style_script = document.getElementById(style_script_id).text;
style.innerHTML = style_script;
document.getElementsByTagName('head')[0].appendChild(style);
</script>

    <script src="ICCG11-Presentation_files/header-attrs-2.10/header-attrs.js"></script>
    <link href="ICCG11-Presentation_files/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
    <link href="ICCG11-Presentation_files/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section>
    <h1 class="title">Network Representation of Constructional Semantics</h1>
    <h2 class="author"><div class="line-block"><br />
<span style="font-size: 0.8em;font-family:&#39;Times New Roman&#39;, Times, serif;color:&#39;darkgrey&#39;">Alvin Cheng-Hsien Chen</span><br />
<span style="font-size:0.4em;">(陳正賢)</span><br />
<br />
<small>National Taiwan Normal University, Taiwan <br><a href="mailto:alvinchen@ntnu.edu.tw" class="email">alvinchen@ntnu.edu.tw</a></small></div></h2>
    <h3 class="date"><small>20 August, 2021, ICCG11</small></h3>
</section>

<section id="outline" class="title-slide slide level2">
<h2>Outline</h2>
<ul>
<li>Background of the Study</li>
<li>Space Particle Construction</li>
<li>From Corpora to Network</li>
<li>Network Analysis of Constructional Semantics</li>
<li>Conclusion</li>
</ul>
</section>

<section>
<section id="background-of-the-study" class="title-slide slide level2">
<h2>Background of the Study</h2>
<!-- ### Lexical Semantic Representation -->
<!-- -   [How to represent lexical semantics]{.note} (Semantic Representation)？ -->
<!--     > -   Meaning by **reference** -->
<!--     > -   Meaning by **contrast** -->
<!--     > -   Meaning by **uses** -->
<!-- ::: {.footnotes} -->
<!-- 1.  Riemer, Nick. 2010. *Introducing semantics*. Cambridge: Cambridge University Press. -->
<!-- ::: -->
</section>
<section id="co-occurrences-of-linguistic-units" class="title-slide slide level3">
<h3>Co-occurrences of Linguistic Units</h3>
<ul>
<li><p><strong>Co-occurrences</strong> have been the central focus of usage-based linguistics.</p>
<ul>
<li>Collocation, Colligation, Collostruction</li>
</ul></li>
</ul>
<ul>
<li class="fragment">With the increasing availability of corpora data, sequential orders of words in use are closely connected to the semantics of the units, forming the basis for their semantic representation (i.e., <strong>Distributed Semantic Representation</strong>)</li>
</ul>
<!-- ### Distributional Hypothesis -->
<!-- -   Co-occurrence patterns of a linguistic unit reflect its semantics to a great deal. -->
<!-- > You shall know a word by the comany it keeps. (Firth, 1957, p.11) -->
<!-- > [D]ifference of meaning correlates with difference of distribution. (Harris, 1970, p.785) -->
<!-- ### Constructional Semantics? -->
<!-- > John baked me a cake. -->
<!-- - "Transfer" meaning from the constructional schema of Di-transitive -->
<!-- ### Words and Constructions are alike -->
<!-- > [G]rammatical knowledge represents a continuum on two dimensions, from the substantive to the schematic and from the atomic to the complex. This continuum is widely referred to as the **syntax-lexicon** continuum. (Croft & Cruse, 2004, p.255-6) -->
<!-- ### Our mental grammar includes an inventory of units at varying granularities, from words, phrases, chunks to fully schematic constructions. -->
</section>

<section id="how-to-represent-constructional-semantics" class="title-slide slide level3">
<h3>How to represent constructional semantics?</h3>
<ul>
<li class="fragment">When we study lexical semantics, we look at words’ collocates and identify the emerging semantic coherence from these collocates.</li>
<li class="fragment">The distributional hypothesis has been widely applied to the study of constructional semantics.</li>
</ul>
</section>

<section id="grammar-as-network" class="title-slide slide level3">
<h3>Grammar as Network</h3>
<p><img src="images/network.png" width="40%" style="border:1px solid darkgrey;" style="display: block; margin: auto;" /></p>
<!-- ### Meanings are in the grammar network -->
<ul>
<li class="fragment">Language consists of <strong>words</strong> and <strong>constructions</strong>.</li>
<li class="fragment">Every <strong>word</strong> or <strong>construction</strong> would develop their own preferred linguistic co-texts/contexts in language use.</li>
<li class="fragment">The emerging patterns in these preferred linguistic co-texts/contexts are the semantic traces of these <strong>words</strong> or <strong>constructions</strong>.</li>
</ul>
</section>
</section>
<section>
<section id="space-particle-constructions" class="title-slide slide level2">
<h2>Space Particle Constructions</h2>

</section>
<section id="spc-in-mandarin" class="title-slide slide level3">
<h3>SPC in Mandarin</h3>
<blockquote>
<p>「在zai + Reference_Landmark + Space_Particle」</p>
</blockquote>
<ul>
<li>在<em>zai</em>…<span class="mystrong1">LM</span> <strong>下<em>xia</em></strong> ‘DOWN/BELOW’</li>
<li>在<em>zai</em>…<span class="mystrong1">LM</span> <strong>上<em>shang</em></strong> ‘UP/ON’</li>
<li>在<em>zai</em>…<span class="mystrong1">LM</span> <strong>前<em>quian</em></strong> ‘IN FRONT OF’</li>
<li>在<em>zai</em>…<span class="mystrong1">LM</span> <strong>後<em>hou</em></strong> ‘behind’</li>
<li>在<em>zai</em>…<span class="mystrong1">LM</span> <strong>中<em>zhong</em></strong> ‘IN’</li>
<li>在<em>zai</em>…<span class="mystrong1">LM</span> <strong>內<em>nei</em></strong> ‘IN’</li>
<li>在<em>zai</em>…<span class="mystrong1">LM</span> <strong>裡<em>li</em></strong> ‘IN’</li>
<li>在<em>zai</em>…<span class="mystrong1">LM</span> <strong>外<em>zhong</em></strong> ‘OUTSIDE OF’</li>
</ul>
</section>

<section id="keys-for-constructional-semantic-representation" class="title-slide slide level3">
<h3>Keys for constructional semantic representation?</h3>
<ul>
<li><p>Hard to make a list of <strong>definitions</strong></p></li>
<li><p>Constructional meaning by <strong>use</strong>:</p>
<ul>
<li class="fragment">Links between <strong>construction</strong> and <span class="mystrong1">word</span>：Which words tend to co-occurr with the construction?</li>
<li class="fragment">Links between <span class="mystrong1">word</span> and <span class="mystrong1">word</span>：For those words co-occurring with the construction, how are these words connected？</li>
<li class="fragment">Links between <strong>construction</strong> and <strong>construction</strong>：Is one SPC more connected to a specific SPC? (e.g., <em>zai</em>+LM+<em>xia</em> vs. <em>zai</em>+LM+<em>shang</em>, <em>zai</em>+LM+<em>xia</em> vs. <em>zai</em>+LM+<em>nei</em>)</li>
</ul></li>
</ul>
</section>
</section>
<section>
<section id="from-corpora-to-network" class="title-slide slide level2">
<h2>From Corpora to Network</h2>

</section>
<section id="corpus-learning" class="title-slide slide level3">
<h3>Corpus Learning</h3>
<ul>
<li class="fragment">Corpus data are a good source for the modeling of the linguistic <strong>co-occurrences</strong>, i.e., the three types of links (word-construction, word-word, construction-construction).</li>
<li class="fragment">With these links, we can create a grammar <strong>network</strong>, which could quantitatively represent the <strong>distributional patterns</strong> of the constructions.</li>
</ul>
</section>
<section class="slide level4">

<ul>
<li>We can then utilize the <span class="note"><strong>network science methods</strong></span> to both visualize and analyze the constructional semantics quantitatively, which can be a very important step to the modeling of our knowledge of <span class="note">grammar network</span>.</li>
</ul>
<div class="footnotes">
<ol type="1">
<li>Barabási, Albert-László. (2016). <em>Network Science</em>. Cambridge University Press.</li>
<li>Diessel, Holger. (2019). <em>The Grammar Network: How Linguistic Structure is Shaped by Language Use</em>. Cambridge University Press.</li>
</ol>
</div>
</section>

<section id="word-construction-links" class="title-slide slide level3">
<h3>(1) Word-Construction Links</h3>
<ul>
<li><p>Collostruction Analysis</p>
<ul>
<li><span class="note">Collexeme Analysis</span></li>
<li><span class="note">Co-varying Collexeme Analysis</span></li>
<li><span class="note">Distinctive Collexeme Analysis</span></li>
</ul></li>
</ul>
<!--         -   e.g., Verbs in ditransitives and imperatives; Noun-*waiting-to-happen* construction -->
<!--         -   e.g., V-*into*-Ving constructions (trick someone into buying, force someone into accepting), Verb-*way*-PREP-construction (weave your way through) -->
<!--         -   e.g., ditransitive vs. prepositional datives; *will*- vs. *going-to* futures -->
<div class="footnotes">
<ol type="1">
<li>Stefanowitsch, A. &amp; Gries, S.T. 2003. Collostructions: Investigating the interaction of words and constructions. International Journal of Corpus Linguistics, 8(2), 209-243.</li>
<li>Gries, S. T., &amp; Stefanowitsch, A. 2004. Co-varying collexemes in the into-causative. Language, Culture, and Mind, 225-236.</li>
<li>Gries, S. T., &amp; Stefanowitsch, A. 2004. Extending collostructional analysis: A corpus-based perspective on alternations’. International Journal of Corpus Linguistics, 9(1), 97-129.</li>
</ol>
</div>
</section>

<section id="word-word-links" class="title-slide slide level3">
<h3>(2) Word-Word Links</h3>
<ul>
<li>Words (landmark collexemes) can be connected via multiple relationships (e.g., morphosyntactic, semantic, pragmatic ones).</li>
<li>Here we simplify the matters by modeling the semantic relationships between words (e.g. 情況, 情形, 狀況, 環境).</li>
<li>We used the <strong>pre-trained word embeddings</strong> to identify the semantic connections among these collexemes.</li>
</ul>
<!-- ```{r fig.width = 10, fig.height=1} -->
<!-- # Documentation: http://rich-iannone.github.io/DiagrammeR/graphviz_and_mermaid.html -->
<!-- DiagrammeR::grViz("digraph { -->
<!-- graph[layout = dot, rankdir = LR] -->
<!-- node[style=filled,shape=box, fillcolor = Azure] -->
<!-- edge[dir=both] -->
<!-- a [label = 'Linguistic\n(Manual)'] -->
<!-- b [label = 'Computational\n(Automatic)'] -->
<!-- a -> b  -->
<!-- } -->
<!-- }") -->
<!-- ``` -->
<!-- #### Word Embeddings -->
<!-- -   **Word Embeddings** utilize the deep learning methods to learn the vectorized representations of lexical semantics via the word distributions in a representative large corpus.  -->
<!-- -   These embeddings are learned by creating a deep neural network to achieve two objectives: -->
<!--     -   Within a specific window frame, given all the contextg words in that frame, how likely will we observe a target word, $W_i$？(Continuous Bag-of-Words) -->
<!--     -   Given a target word, $W_i$, how likely would a context word appear within the defined window frame of the target word? (Skip-Gram) -->
<!-- #### 詞彙、搭配詞和語意 -->
<!-- - 簡言之，透過[大型語料庫]{.note}，如果兩個詞彙周圍共現的搭配詞類似，則我們可以推測它們語意相近。 -->
<!-- ::: {.footnotes} -->
<!-- 1.  此範例是根據[中央研究院漢語對話語料庫](http://www.aclclp.org.tw/use_mat_c.php#mcdc)計算。 -->
<!-- ::: -->
<!-- ```{r} -->
<!-- library(tidyverse) -->
<!-- library(kableExtra) -->
<!-- require(text2vec) -->
<!-- # create sinica co-occurrence matrix -->
<!--   # dir(path ="/Users/Alvin/Dropbox/Projects/MandarinPhraseology/data/Sinica_4WordEmbeddings/",full.names = T) %>% -->
<!--   #   sapply(function(x) readLines(x) %>% strsplit(x, split = "\u3000|\\s") %>% unlist) -> input_files -->
<!--   # it = itoken(input_files, progressbar = FALSE) -->
<!--   # vocab <- create_vocabulary(it) -->
<!--   # # pruning vocabulary -->
<!--   # vocab <- prune_vocabulary(vocab, term_count_min = 50L) -->
<!--   # # Use our filtered vocabulary -->
<!--   # vectorizer <- vocab_vectorizer(vocab) -->
<!--   # # use window of 5 for context words -->
<!--   # print("creat term co-occurrence matrix...") -->
<!--   # tcm <- create_tcm(it, vectorizer, skip_grams_window = 5, weights = rep(1,5)) -->
<!--   # saveRDS(tcm, "SINICA_TCM.rds") -->
<!-- tcm <- readRDS("SINICA_TCM.rds") -->
<!-- words_examples<- c("叔叔", "阿姨","男孩", "女孩", "爺爺", "奶奶","爸爸","媽媽", "兒子", "女兒") -->
<!-- #words_examples<-c("希臘","雅典","北京","中國", "日本","東京", "美國", "紐約") -->
<!-- tcm %>% as.matrix %>% .[words_examples, words_examples] %>% data.frame ->df -->
<!-- colnames(df) <- paste0("CW", seq(1:dim(df)[2])) -->
<!-- #df[, c("家","孩子","工作","學校","看","想")] -->
<!-- df %>% -->
<!--   kable(row.names=T, format="html", caption="Target and Context Words Co-occurrence Matrix") %>% -->
<!--   kable_styling(font_size = 18,bootstrap_options = c("striped", "hover", "condensed")) %>%  -->
<!--   row_spec(5:6, bold = T, color = "white", background = "#D7261E") %>% -->
<!--   scroll_box(width = "95%", height = "400px", fixed_thead = list(enabled = T, background = "darkgrey")) #%> % -->
<!-- ``` -->
<!-- ------------------------------------------------------------------------ -->
<!-- 詞向量讓我們能夠將詞彙語意做實際數值運算。 -->
<!-- ```{r} -->
<!-- library(Rtsne) -->
<!-- library(ggplot2) -->
<!-- # library(plotly) -->
<!-- # library(magrittr) -->
<!-- # library(dplyr) -->
<!-- library(ggrepel) -->
<!--   #word_vector <- readRDS("/Users/Alvin/Dropbox/Projects/LanguageModel/script/greg_script/20180617/Reference Corpus Processing/Sinica_word_embeddings_win10_dim300.rds") -->
<!--   #word_vector <- readRDS("/Users/Alvin/Dropbox/Corpus/Word_Embeddings_Pretrained/Chinese_word_embeddings_cc.zh.300.vec.rds") -->
<!--   #word_vector <- word_vector[1:60000,] -->
<!--   #saveRDS(word_vector, "Chinese_word_embeddings_cc.zh.300.vec.60000.words.rds") -->
<!-- word_vector <- readRDS("Chinese_word_embeddings_cc.zh.300.vec.60000.words.rds") -->
<!-- words_examples<- c("叔叔", "阿姨","男孩", "女孩", "爺爺", "奶奶","爸爸","媽媽", "兒子", "女兒","新郎", "新娘") -->
<!-- #words_examples<- c("希臘","雅典","北京","中國", "日本","東京", "美國", "紐約") -->
<!-- words<- words_examples -->
<!-- #words.index <- row.names(word_vector) %in% words -->
<!-- #words.index <- !stringr::str_detect(row.names(word_vector),"[A-Za-z]+") -->
<!-- set.seed(12) -->
<!-- tsne <- Rtsne(word_vector[words,], perplexity = (length(words)-1)/3, pca = FALSE) -->
<!-- # tsne_plot <- tsne$Y %>% -->
<!-- #   as.data.frame() %>% -->
<!-- #   mutate(word = words_examples) %>% -->
<!-- #   ggplot(aes(x = V1, y = V2, label = sprintf("%s_%s",c(1:length(words)),words))) +  -->
<!-- #   geom_point(color="blue", size= 5) + -->
<!-- #   geom_path(color="lightgrey")+ -->
<!-- #   geom_text_repel(size = 14, family = "Arial Unicode MS")  -->
<!-- # tsne_plot -->
<!-- path_color = "darkgrey" -->
<!-- tsne_plot <- tsne$Y %>% -->
<!--   as.data.frame() %>% -->
<!--   mutate(word = words_examples) ->data -->
<!--   h <- data %>% ggplot(aes(x = V1, y = V2, label = word, color=word)) + geom_point(size= 3) -->
<!--   h + geom_text_repel(family = "Arial Unicode MS") + -->
<!--     geom_path(data=data[c(1,2),], color=path_color, linetype="dashed") +  -->
<!--     geom_path(data=data[c(3,4),], color=path_color, linetype="dashed") + -->
<!--     geom_path(data=data[c(5,6),], color=path_color, linetype="dashed") + -->
<!--     geom_path(data=data[c(7,8),], color=path_color, linetype="dashed") + -->
<!--     geom_path(data=data[c(9,10),], color=path_color, linetype="dashed") + -->
<!--     geom_path(data=data[c(11,12),], color=path_color, linetype="dashed") + -->
<!--     labs(title = "詞向量", x="向量維度1", y="向量維度2")+ -->
<!--     theme(text = element_text(family="Arial Unicode MS")) + -->
<!--     scale_color_discrete(guide="none") -->
<!-- #rm(word_vector) -->
<!-- ``` -->
</section>

<section id="construction-construction-links" class="title-slide slide level3">
<h3>(3) Construction-Construction Links</h3>
<ul>
<li>Following the <strong>distributional semantics hypothesis</strong>, we can measure the pairwise semantic connections between the space particle constructions by their similarities in their co-occurring landmark collexemes.</li>
<li>If two space particle constructions co-occur with similar sets of landmark collexemes, they are also more likely to be semantically connected.</li>
</ul>
</section>

<section id="space-particle-network" class="title-slide slide level3">
<h3>Space Particle Network</h3>
<iframe src="MLC_VisNet_CC.html" width="100%" height="600" style="border:1px solid darkgrey;">
</iframe>
<aside class="notes">
<ul>
<li>In this network, there are two types of nodes (bipartite network). Lexical/Collexeme and constructions/SPC.</li>
<li>Within this network, we model three types of links.</li>
<li>For each SPC, we identify their co-occurring collexemes using the construction-word links.</li>
<li>For all the collexemes, we identify their semantic connections using the word-word links.</li>
<li>For all the SPCs, we identify their semantic connections using the construction-construction links.</li>
</ul>
</aside>
</section>

<section id="specific-particle-constructions-i" class="title-slide slide level3">
<h3>Specific Particle Constructions (I)</h3>
<!-- <small>[Dynamic Version](http://web.ntnu.edu.tw/~alvinchen/data/cllt-2020-0012/)</small> -->
<p><img src="images/MLC_Subnetwork_Community_%E5%85%A7_%E4%B8%8B_%E5%89%8D_%E8%A3%A1.png" width="80%" style="border:1px solid darkgrey;" style="display: block; margin: auto;" /></p>
</section>

<section id="specific-particle-constructions-ii" class="title-slide slide level3">
<h3>Specific Particle Constructions (II)</h3>
<!-- <small>[Dynamic Version](http://web.ntnu.edu.tw/~alvinchen/data/cllt-2020-0012/)</small> -->
<p><img src="images/MLC_Subnetwork_Community_%E4%B8%8A_%E5%BE%8C_%E4%B8%AD_%E5%A4%96.png" width="80%" style="border:1px solid darkgrey;" style="display: block; margin: auto;" /></p>
</section>
</section>
<section>
<section id="network-analysis-of-constructional-semantics" class="title-slide slide level2">
<h2>Network Analysis of Constructional Semantics</h2>

</section>
<section id="three-levels-of-network-analysis" class="title-slide slide level3">
<h3><span class="note">Three Levels of Network Analysis</span></h3>
<ul>
<li class="fragment">Macroscopic Analysis: Examine the properties of the entire <strong>graph</strong></li>
<li class="fragment">Microscopic Analysis: Examine the properties of the <strong>nodes</strong></li>
<li class="fragment">Mesoscopic Analysis: Examine the groupings of the nodes, i.e., the emergence of the <strong>Community</strong> in the graph</li>
</ul>
<div class="footnotes">
<ol type="1">
<li>Siew, Cynthia S. Q., Dirk U. Wulff, Nicole M. Beckage &amp; Yoed N. Kenett. 2019. Cognitive network science: A review of research on cognition through the lens of network representations, processes, and dynamics. <em>Complexity</em> 2019. 1–24.</li>
</ol>
</div>
</section>

<section id="macroscopic-analysis" class="title-slide slide level3">
<h3>Macroscopic Analysis</h3>

</section>

<section id="common-structures-in-social-networks" class="title-slide slide level3">
<h3>Common Structures in Social Networks</h3>
<ul>
<li><p><span class="note">Scale-free</span>:</p>
<ul>
<li class="fragment">a pattern in which there are usually only a few nodes with <strong>high degrees</strong> and many more with low degrees.</li>
</ul></li>
<li><p><span class="note">Small-world</span>:</p>
<ul>
<li class="fragment">a tendency in which a network usually consists of several small <strong>communities</strong> or <strong>clusters</strong> where the within-community edges are much stronger than across-community ones.</li>
</ul></li>
</ul>
<div class="footnotes">
<ol type="1">
<li>Barabási, Albert-László &amp; Re ́ka Albert. 1999. Emergence of scaling in random networks. <em>Science</em> 286(5439). 509–512.</li>
<li>Watts, Duncan J. &amp; Steven H. Strogatz. 1998. Collective dynamics of ’small-world’networks. <em>Nature</em> 393(6684). 440–442.</li>
</ol>
</div>
</section>

<section id="macroscopic-analysis-of-spc" class="title-slide slide level3">
<h3>Macroscopic Analysis of SPC</h3>
<!-- # ```{r echo=FALSE} -->
<!-- # knitr::include_graphics("images/SCALE_FREE_PLOT.png") -->
<!-- # ``` -->
<ul>
<li class="fragment">Small-world: Semantically connected nodes form small <strong>clusters</strong> in the network, suggesting the emergence of <strong>semantic fields</strong> of the collexemes</li>
<li class="fragment">Scale-free: Only a few linguistic units (<strong>central nodes</strong>) in the system are frequently connected to other linguistic units, likely to be important <strong>exemplars</strong>, or “cognitive reference points” (Diessel 2019: 34)</li>
</ul>
<aside class="notes">
<ul>
<li>The small-world and scale-free structures may be quantitative indicators of the emergent <strong>semantic coherence</strong> of the constructional schema in speaker’s mental representation.</li>
</ul>
</aside>
</section>

<section id="microscopic-analysis" class="title-slide slide level3">
<h3>Microscopic Analysis</h3>

</section>

<section id="node-level-metrics" class="title-slide slide level3">
<h3>Node-level Metrics</h3>
<ul>
<li class="fragment"><span class="note">Local Clustering Coefficient</span>: a metric that indicates the extent to which the neighbors of a node are interconnected – namely, whether a node’s neighbors are also neighbors of each other.</li>
<li class="fragment"><span class="note">Centrality</span>：a metric that characterizes the importance of a node beyond its immediate neighbors.</li>
</ul>
<div class="footnotes">
<ol type="1">
<li>Also known as <em>Local Transitivity</em>.</li>
<li>Common metrics for node centrality include: Betweenness, PageRank, Authority, Closeness.</li>
</ol>
</div>
</section>
<section class="slide level4">

<ul>
<li>Construction Nodes: The local clustering coefficient values indicate the degrees of <strong>semantic coherence</strong> of their collexemes.</li>
</ul>
<p><img src="images/MLC_Network_all_transitivity.png" width="85%" style="border:1px solid darkgrey;" style="display: block; margin: auto;" /></p>
<aside class="notes">
<ul>
<li>NEI has the highest LCC, suggesting its high degrees of semantic coherences in its collexemes.</li>
<li>WAI shows the lowest LCC, suggesting that its co-occurring collexemes are rather heterogeneous in their semantics.</li>
</ul>
</aside>
</section>
<section class="slide level4">

<ul>
<li>Lexical Nodes (Landmark): The centrality values indicate which nodes are more central, thus more likely to be an <strong>exemplar</strong> of the sub-network.</li>
</ul>
<p><img src="images/MLC_Network_all_pagerank.png" width="85%" style="border:1px solid darkgrey;" style="display: block; margin: auto;" /></p>
</section>

<section id="mesoscopic-analysis" class="title-slide slide level3">
<h3>Mesoscopic Analysis</h3>

</section>
<section class="slide level4">

<ul>
<li>In network science, we can identify small clusters as communities using the algorithms of <strong>community detection</strong>.</li>
</ul>
</section>
<section class="slide level4">

<p>These <strong>communities</strong> arising from the construction network may indicate the emergence of specific <strong>semantic fields</strong>.</p>
<p><img src="images/MLC_Subnetwork_Community_%E5%85%A7_%E4%B8%8B_%E5%89%8D_%E8%A3%A1.png" width="75%" style="border:1px solid darkgrey;" style="display: block; margin: auto;" /></p>
</section>
<section class="slide level4">

<iframe src="MLC_VisNet_CC_Community_下.html" width="100%" height="700" style="border:1px solid darkgrey;">
</iframe>
<aside class="notes">
<ul>
<li>Take the sub-network of XIA for instance.</li>
<li>The algorithm is able to identify quite a few communities from the sub-network.</li>
<li>Based on these groupings of collexemes, we can analyze the emerging semantic fields from this sub-network.</li>
<li>For each community we can interpret their semantics by identifying important members (nodes with high centrality values).</li>
<li>A for CONDITION, B for GUIDANCE.</li>
</ul>
</aside>
<!-- ### Capacity of Grammar Network -->
<!-- -   With the quantitative methods of network science, we can analyze the semantics of morpho-syntactic constructions as follows： -->
<!--     > -   For each SPC, which landmarks (collexemes) are more **prototypical**? -->
<!--     > -   For each SPC, how **semantically cohensive** are its co-occurring landmark collexemes? -->
<!--     > -   For the entire construction network, what are the emerging **semantic fields**? How are these semantic fields connected to each indivdual construction? -->
</section>

<section id="conclusion" class="title-slide slide level3">
<h3>Conclusion</h3>
<p><img src="images/summary.png" width="1593" style="border:1px solid darkgrey;" style="display: block; margin: auto;" /></p>
<aside class="notes">
<ul>
<li><p>With the quantitative methods of network science, we can analyze the semantics of morpho-syntactic constructions as follows：</p>
<ul>
<li class="fragment">For each SPC, which landmarks (collexemes) are more <strong>prototypical</strong>?</li>
<li class="fragment">For each SPC, how <strong>semantically cohensive</strong> are its co-occurring landmark collexemes?</li>
<li class="fragment">For the entire construction network, what are the emerging <strong>semantic fields</strong>? How are these semantic fields connected to each indivdual construction?</li>
</ul></li>
</ul>
</aside>
<!-- ## 結語 -->
<!-- >- 語法表徵 (Grammar Knowledge Representation) 可視為一個由不同層次語言單位交織而成的**語法網絡** (Grammar Network) 。 -->
<!-- >- 網絡不僅可以勾勒**詞彙**間的連結，更可成為語言構式的**語意**表徵方法。 -->
<!-- >- 網絡源自於**語言使用**(Language Use)，未來可透過網絡分析，觀察特定**群體**或**場域**的語法網絡生成。 -->
<!-- :::{.notes} -->
<!-- - 第一語言學習者 (Child Language Acquisition) -->
<!-- - 第二語言學習者 (Second Language Acquisition) -->
<!-- - 個人語言行為 -->
<!-- - 特定社群語言使用者 (Online Forums) -->
<!-- - 特定文化場域語言使用者 (Political Discourse Analysis) -->
<!-- ::: -->
</section>
</section>
<section id="section" class="title-slide slide level2" data-background="images/background.jpeg" data-background-transition="fade">
<h2 data-background="images/background.jpeg" data-background-transition="fade"></h2>
<p><span style="font-size: 2.5em">Thank you!</span> </br></p>
<hr>
<div class="footnotes">
<p><small>References：</small></p>
<ol type="1">
<li>Barabási, Albert-László. 2016. <em>Network science</em>. Cambridge: Cambridge University Press.</li>
<li>Diessel, Holger. 2019. <em>The grammar network: How linguistic structure is shaped by language use</em>. Cambridge, UK: Cambridge University Press.</li>
<li>Chen, Alvin Cheng-Hsien. In press. Words, constructions and corpora: Network representations of constructional semantics for Mandarin space particles. <em>Corpus Linguistics and Linguistic Theory</em> 19(1). <a href="https://doi.org/10.1515/cllt-2020-0012" class="uri">https://doi.org/10.1515/cllt-2020-0012</a>. (<a href="http://web.ntnu.edu.tw/~alvinchen/data/cllt-2020-0012/">Supplementary Materials</a>)</li>
</ol>
</div>
</section>
    </div>
  </div>

  <script src="ICCG11-Presentation_files/reveal.js-3.3.0.1/lib/js/head.min.js"></script>
  <script src="ICCG11-Presentation_files/reveal.js-3.3.0.1/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Display the page number of the current slide
        slideNumber: 'c/t',
        // Push each slide change to the browser history
        history: true,
        // Vertical centering of slides
        center: true,
        // Opens links in an iframe preview overlay
        previewLinks: true,
        // Transition style
        transition: 'fade', // none/fade/slide/convex/concave/zoom
        // Transition style for full page slide backgrounds
        backgroundTransition: 'default', // none/fade/slide/convex/concave/zoom

        menu: {
   
    
    
    
    
    
    
 
          custom: false,
          themes: false,
          transitions: false
        },



        chalkboard: {
          theme: 'blackboard',
        },

        keyboard: {
          67: function() { RevealChalkboard.toggleNotesCanvas() },    // toggle notes canvas when 'c' is pressed
          66: function() { RevealChalkboard.toggleChalkboard() }, // toggle chalkboard when 'b' is pressed
          46: function() { RevealChalkboard.clear() },    // clear chalkboard when 'DEL' is pressed
           8: function() { RevealChalkboard.reset() },    // reset chalkboard data on current slide when 'BACKSPACE' is pressed
          68: function() { RevealChalkboard.download() }, // downlad recorded chalkboard drawing when 'd' is pressed
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'ICCG11-Presentation_files/reveal.js-3.3.0.1/plugin/notes/notes.js', async: true },
          { src: 'ICCG11-Presentation_files/reveal.js-3.3.0.1/plugin/zoom-js/zoom.js', async: true },
          { src: 'ICCG11-Presentation_files/reveal.js-3.3.0.1/plugin/chalkboard/chalkboard.js', async: true },
          { src: 'ICCG11-Presentation_files/reveal.js-3.3.0.1/plugin/menu/menu.js', async: true },
        ]
      });
    </script>
  <!-- dynamically load mathjax for compatibility with self-contained -->
  <script>
    (function () {
      var script = document.createElement("script");
      script.type = "text/javascript";
      script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
      document.getElementsByTagName("head")[0].appendChild(script);
    })();
  </script>

<script>
  (function() {
    if (window.jQuery) {
      Reveal.addEventListener( 'slidechanged', function(event) {  
        window.jQuery(event.previousSlide).trigger('hidden');
        window.jQuery(event.currentSlide).trigger('shown');
      });
    }
  })();
</script>

<div class='footer'>
<p>20 August, 2021, ICCG11, Antwerp</p>
<p>alvinchen@ntnu.edu.tw &ensp;|&ensp; <a href='https://alvinchen.myftp.org' target='_blank'> alvinchen.myftp.org </a></a></p>
</div>

  </body>
</html>
