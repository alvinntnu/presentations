[["index.html", "Processing Chinese Data with R Introduction Structure of the Workshop Data Materials Resources", " Processing Chinese Data with R Alvin Cheng-Hsien Chen National Taiwan Normal University, Taiwan April 26, 2022, @ HKSYU (Online Workshop) Introduction The theme of this workshop is Chinese text processing with R. I will introduce a few useful R libraries that can be utilized for text analytic tasks. Also, I will focus on an important step in Chinese processing, namely, the word segmentation/tokenization, and show how to attend to this step in the pipeline. Finally, I will demonstrate a few potential applications of data analysis visualization with the help of the attractive informative graphs with R. Structure of the Workshop Environment Setup Working Pipeline for Text Processing Chinese Word Segmentation Applications Concordances Frequency Lists Word clouds Patterns N-gram/Lexical Bundles Data All the data sets used in this workshop are available in the Dropbox directory demo_data. Materials In the lecture notes, the text boxes in light blue refer to codes that you need to run in either terminal or your R/Python console. The text boxes in black background show the outputs of the code processing. We will follow this presentation convention throughout the entire lecture notes. print(&#39;Hello! R!&#39;) [1] &quot;Hello! R!&quot; Resources The course lectures will follow the materials provided on the course website. Please preview the lecture notes before the class. Also, for each topic, please review the assigned readings on your own. These readings will be part of the midterm and final exams as well. In particular, we will be referring to two useful reference books as our main reading materials– Stefanowitsch (2019), Gries (2016). In addition, there are a few more reference books listed at the end of the section (See References), which we will refer to for specific topics, including Gries (2021), Baayen (2008), Brezina (2018), McEnery and Hardie (2011), Wynne (2006), Winter (2020), Hunston (2022), Bird, Klein, and Loper (2009). Throughout the semester, we will follow the materials provided on our course website (see below). We will not use a particular textbook for the course. However, I do like to recommend Wickham and Grolemund (2017) for its simplicity. Also, another great book for R lovers, by Davies (2016): And two more comprehensive books for Python basics: Gerrard (2016) and Sweigart (2020): References "],["environment-setup.html", "Environment Setup R and RStudio Libraries Environment Checking", " Environment Setup R and RStudio In this workshop, I expect that all the participants have installed an working R environment and the IDE RStudio in their workstation. Libraries There are many useful and effective R libraries for text processing. I would highly recommend the official webpage, CRAN Task Views, which categorizes all these packages based on their topics. Of particular relevance in this workshop is the topic – Natural Language Processing. Specifically, in this workshop, I will use the following libraries. Based on my experiences with data processing, I think these libraries are quite effective and easy to learn. tidyverse: This library includes a collection of useful R libraries for data analysis. quanteda: This library provides many useful functions for exploratory quantitative text analysis. tidytext: This library provides many useful functions for computational text analysis under the same tidy structure framework. jiebar: This library provides functions for Chinese tokenization (i.e., word segmentation). These libraries can be easily installed in R using the following codes: install.packages(&quot;tidyverse&quot;) install.packages(&quot;quanteda&quot;) install.packages(&quot;quanteda.textplots&quot;) install.packages(&quot;quanteda.textstats&quot;) install.packages(&quot;readtext&quot;) install.packages(&quot;tidytext&quot;) install.packages(&quot;jiebaR&quot;) install.packages(&quot;wordcloud2&quot;) Environment Checking The R Version to produce these lecture notes: R version 4.1.2 (2021-11-01). After we installed the libraries, we can import these libraries in the current R session for the later use: library(tidyverse) library(quanteda) library(quanteda.textplots) library(quanteda.textstats) library(readtext) library(tidytext) library(jiebaR) library(wordcloud2) packageVersion(&quot;tidyverse&quot;) [1] &#39;1.3.1&#39; packageVersion(&quot;quanteda&quot;) [1] &#39;3.2.1&#39; packageVersion(&quot;quanteda.textplots&quot;) [1] &#39;0.94.1&#39; packageVersion(&quot;quanteda.textstats&quot;) [1] &#39;0.95&#39; packageVersion(&quot;tidytext&quot;) [1] &#39;0.3.2&#39; packageVersion(&quot;jiebaR&quot;) [1] &#39;0.11&#39; packageVersion(&quot;wordcloud2&quot;) [1] &#39;0.2.1&#39; Or alternatively, we can run the function sessionInfo() to get an overview of the current R environment: sessionInfo() R version 4.1.2 (2021-11-01) Platform: x86_64-apple-darwin17.0 (64-bit) Running under: macOS Catalina 10.15.7 Matrix products: default BLAS: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib LAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib locale: [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 attached base packages: [1] stats graphics grDevices utils datasets methods base other attached packages: [1] wordcloud2_0.2.1 jiebaR_0.11 [3] jiebaRD_0.1 tidytext_0.3.2 [5] readtext_0.81 quanteda.textstats_0.95 [7] quanteda.textplots_0.94.1 quanteda_3.2.1 [9] forcats_0.5.1 stringr_1.4.0 [11] dplyr_1.0.7 purrr_0.3.4 [13] readr_2.1.1 tidyr_1.1.4 [15] tibble_3.1.6 ggplot2_3.3.5 [17] tidyverse_1.3.1 loaded via a namespace (and not attached): [1] httr_1.4.2 sass_0.4.0 jsonlite_1.7.2 modelr_0.1.8 [5] bslib_0.3.1 RcppParallel_5.1.4 assertthat_0.2.1 highr_0.9 [9] cellranger_1.1.0 yaml_2.2.1 pillar_1.6.4 backports_1.4.1 [13] lattice_0.20-45 glue_1.6.0 digest_0.6.29 rvest_1.0.2 [17] colorspace_2.0-2 htmltools_0.5.2 Matrix_1.4-0 pkgconfig_2.0.3 [21] broom_0.7.11 haven_2.4.3 bookdown_0.24 scales_1.1.1 [25] nsyllable_1.0 tzdb_0.2.0 generics_0.1.1 ellipsis_0.3.2 [29] withr_2.4.3 cli_3.1.0 magrittr_2.0.1 crayon_1.4.2 [33] readxl_1.3.1 evaluate_0.14 stopwords_2.3 tokenizers_0.2.1 [37] janeaustenr_0.1.5 fs_1.5.2 fansi_0.5.0 SnowballC_0.7.0 [41] xml2_1.3.3 tools_4.1.2 data.table_1.14.2 hms_1.1.1 [45] lifecycle_1.0.1 munsell_0.5.0 reprex_2.0.1 compiler_4.1.2 [49] jquerylib_0.1.4 rlang_0.4.12 grid_4.1.2 rstudioapi_0.13 [53] htmlwidgets_1.5.4 rmarkdown_2.11 gtable_0.3.0 DBI_1.1.2 [57] R6_2.5.1 lubridate_1.8.0 knitr_1.37 fastmap_1.1.0 [61] utf8_1.2.2 fastmatch_1.1-3 stringi_1.7.6 Rcpp_1.0.8.3 [65] vctrs_0.3.8 dbplyr_2.1.1 tidyselect_1.1.1 xfun_0.29 If your R version is older than the above one, please consider updating your R. Details about updating R can be found in: 3 Methods to Update R &amp; Rstudio (For Windows &amp; Mac) Updating R, Rstudio, and Your Packages "],["loading-text-data.html", "Loading Text Data Text Data Types Libraries CSV Format Directory Format Zipped File Format", " Loading Text Data Text Data Types There are generally three types of formats for text data storage in the hard drive: CSV: demo_data/song-jay-amei-v1.csv Directory: demo_data/corp_dir A Zipped File: demo_data/song-jay-amei-v2.zip In this workshop, I will show you how to load the data of these three types in R. Libraries When we need to load text data from external files (e.g., txt, tar.gz files), there is a simple and powerful R package for loading texts: readtext, which goes hand in hand with text processing packages, such as quanteda or tidytext. CSV Format For small data sets, people often store their textual data as a spreadsheet table. We can easily load this type of data using read_csv() (or read_tsv()). corp &lt;- read_csv(file = &quot;demo_data/song-jay-amei-v1.csv&quot;) corp Directory Format Sometimes, if we have a lot of text documents, we may include all the documents of the corpus in one directory. With the data structure like this, we can load the entire corpus in the directory, using readtext(): corp &lt;- readtext(file=&quot;demo_data/corp_dir&quot;, docvarsfrom = &quot;filenames&quot;, dvsep=&quot;-&quot;, docvarnames = c(&quot;artist&quot;, &quot;index&quot;)) corp We often include metadata information of each document in the filenames. For example, in our example data set, we include the artist’s name as well as the index number for each document in its filename. So when we load the data set, readtext() comes with the functionality to parse these important pieces of metadata information from the filenames. Zipped File Format When the corpus is large, poeple may zip the entire corpus as an archived zipped file to save storage space on the hard drive. To load the data set like this, we can also use readtext(). corp &lt;- readtext(file = &quot;demo_data/song-jay-amei-v2.tar.gz&quot;, docvarsfrom = &quot;filenames&quot;, dvsep=&quot;-&quot;, docvarnames = c(&quot;artist&quot;, &quot;index&quot;)) corp The function readtext() works on: text (.txt) files; comma-separated-value (.csv) files; XML formatted data; data from the Facebook API, in JSON format; data from the Twitter API, in JSON format; and generic JSON data. "],["chinese-word-segmentation.html", "Chinese Word Segmentation Chinese Word Segmenter jiebaR Chinese Text Analytics Pipeline", " Chinese Word Segmentation Before we perform any analysis on the Chinese data, we need to fix the issue of word tokenization. Unlike English, where each word token is delimited by white spaces, Chinese word tokens are much less straightforward.A word, however, is an important semantic unit in many corpus analysis. In this workshop, I will introduce a frequently used library, jiebaR, for Chinese word segmentation. Chinese Word Segmenter jiebaR Start Now let us take a look at a quick example. Let us assume that in our corpus, we have collected only one text document, with only a short paragraph. sents &lt;- c(&quot;綠黨桃園市議員王浩宇爆料，指民眾黨不分區被提名人蔡壁如、黃瀞瑩，在昨（6）日才請辭是為領年終獎金。台灣民眾黨主席、台北市長柯文哲7日受訪時則說，都是按流程走，不要把人家想得這麼壞。&quot;, &quot;新北消防救護人員說，119在疫情的期間，除肩負協助確診患者送醫的任務，仍需負擔每天超過500件以上的緊急救護案件，疫情期間稍一不慎，極可能造成到院前，緊急救護能量和醫療體系的崩壞；目前確診案件不斷的增加，國家面臨疫情是在清零與共存政策間的戰略上，指引的調整勢必勢在必行。&quot;) There are two important steps in Chinese word segmentation: Initialize a jiebar object using worker() Tokenize the texts into words using the function segment() with the designated jiebar object created earlier seg1 &lt;- worker() segment(sents, jiebar = seg1) [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指民眾&quot; [7] &quot;黨&quot; &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; [13] &quot;蔡壁如&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; &quot;在昨&quot; &quot;6&quot; [19] &quot;日&quot; &quot;才&quot; &quot;請辭&quot; &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; [25] &quot;台灣民眾&quot; &quot;黨&quot; &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文&quot; [31] &quot;哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; [37] &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;不要&quot; [43] &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; &quot;新北&quot; [49] &quot;消防&quot; &quot;救護&quot; &quot;人員&quot; &quot;說&quot; &quot;119&quot; &quot;在&quot; [55] &quot;疫情&quot; &quot;的&quot; &quot;期間&quot; &quot;除&quot; &quot;肩負&quot; &quot;協助&quot; [61] &quot;確診&quot; &quot;患者&quot; &quot;送醫&quot; &quot;的&quot; &quot;任務&quot; &quot;仍&quot; [67] &quot;需&quot; &quot;負擔&quot; &quot;每天&quot; &quot;超過&quot; &quot;500&quot; &quot;件&quot; [73] &quot;以上&quot; &quot;的&quot; &quot;緊急&quot; &quot;救護&quot; &quot;案件&quot; &quot;疫情&quot; [79] &quot;期間&quot; &quot;稍&quot; &quot;一&quot; &quot;不慎&quot; &quot;極&quot; &quot;可能&quot; [85] &quot;造成&quot; &quot;到&quot; &quot;院前&quot; &quot;緊急&quot; &quot;救護&quot; &quot;能量&quot; [91] &quot;和&quot; &quot;醫療&quot; &quot;體系&quot; &quot;的&quot; &quot;崩壞&quot; &quot;目前&quot; [97] &quot;確診&quot; &quot;案件&quot; &quot;不斷&quot; &quot;的&quot; &quot;增加&quot; &quot;國家&quot; [103] &quot;面臨&quot; &quot;疫情&quot; &quot;是&quot; &quot;在&quot; &quot;清零&quot; &quot;與&quot; [109] &quot;共存&quot; &quot;政策&quot; &quot;間&quot; &quot;的&quot; &quot;戰略&quot; &quot;上&quot; [115] &quot;指引&quot; &quot;的&quot; &quot;調整&quot; &quot;勢必&quot; &quot;勢在必行&quot; class(seg1) [1] &quot;jiebar&quot; &quot;segment&quot; &quot;jieba&quot; To word-tokenize the document, text, you first initialize a jiebar object, i.e., seg1, using worker() and feed this jiebar to segment(jiebar = seg1)and tokenize text into words. Parameters Setting There are many different parameters you can specify when you initialize the jiebar object. You may get more detail via the documentation ?worker(). Some of the important arguments include: user = ...: This argument is to specify the path to a user-defined dictionary stop_word = ...: This argument is to specify the path to a stopword list symbol = FALSE: Whether to return symbols (the default is FALSE) bylines = FALSE: Whether to return a list or not (crucial if you are using tidytext::unnest_tokens()) User-defined dictionary From the above example, it is clear to see that some of the words have not been correctly identified by the current segmenter: for example, 民眾黨, 不分區, 黃瀞瑩, 柯文哲. It is always recommended to include a user-defined dictionary when tokenizing your texts because different corpora may have their own unique vocabulary (i.e., domain-specific lexicon). This can be done with the argument user = ... when you initialize the jiebar object, i.e, worker(..., user = ...). seg2 &lt;- worker(user = &quot;demo_data/dict-ch-user-demo.txt&quot;) segment(sents, seg2) [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指&quot; [7] &quot;民眾黨&quot; &quot;不分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; &quot;蔡壁如&quot; [13] &quot;黃瀞瑩&quot; &quot;在昨&quot; &quot;6&quot; &quot;日&quot; &quot;才&quot; &quot;請辭&quot; [19] &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; &quot;台灣&quot; &quot;民眾黨&quot; &quot;主席&quot; [25] &quot;台北&quot; &quot;市長&quot; &quot;柯文哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; [31] &quot;時則&quot; &quot;說&quot; &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; [37] &quot;走&quot; &quot;不要&quot; &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; [43] &quot;壞&quot; &quot;新北&quot; &quot;消防&quot; &quot;救護&quot; &quot;人員&quot; &quot;說&quot; [49] &quot;119&quot; &quot;在&quot; &quot;疫情&quot; &quot;的&quot; &quot;期間&quot; &quot;除&quot; [55] &quot;肩負&quot; &quot;協助&quot; &quot;確診&quot; &quot;患者&quot; &quot;送醫&quot; &quot;的&quot; [61] &quot;任務&quot; &quot;仍&quot; &quot;需&quot; &quot;負擔&quot; &quot;每天&quot; &quot;超過&quot; [67] &quot;500&quot; &quot;件&quot; &quot;以上&quot; &quot;的&quot; &quot;緊急&quot; &quot;救護&quot; [73] &quot;案件&quot; &quot;疫情&quot; &quot;期間&quot; &quot;稍&quot; &quot;一&quot; &quot;不慎&quot; [79] &quot;極&quot; &quot;可能&quot; &quot;造成&quot; &quot;到&quot; &quot;院前&quot; &quot;緊急&quot; [85] &quot;救護&quot; &quot;能量&quot; &quot;和&quot; &quot;醫療&quot; &quot;體系&quot; &quot;的&quot; [91] &quot;崩壞&quot; &quot;目前&quot; &quot;確診&quot; &quot;案件&quot; &quot;不斷&quot; &quot;的&quot; [97] &quot;增加&quot; &quot;國家&quot; &quot;面臨&quot; &quot;疫情&quot; &quot;是&quot; &quot;在&quot; [103] &quot;清零&quot; &quot;與&quot; &quot;共存&quot; &quot;政策&quot; &quot;間&quot; &quot;的&quot; [109] &quot;戰略&quot; &quot;上&quot; &quot;指引&quot; &quot;的&quot; &quot;調整&quot; &quot;勢必&quot; [115] &quot;勢在必行&quot; The format of the user-defined dictionary is a text file, with one word per line. Also, the default encoding of the dictionary is UTF-8. Please note that in Windows, the default encoding of a Chinese txt file created by Notepad may not be UTF-8. (Usually, it is encoded in big-5). Also, files created by MS Office applications tend to be less transparent in terms of their encoding. Creating a user-defined dictionary may take a lot of time. You may consult 搜狗詞庫, which includes many domain-specific dictionaries created by others. However, it should be noted that the format of the dictionaries is .scel. You may need to convert the .scel to .txt before you use it in jiebaR. To do the conversion automatically, please consult the library cidian. Also, you need to do the traditional-simplified Chinese conversion as well. For this, you may consult the library ropencc in R. Stopwords When you initialize the worker(), you can also specify a stopword list, i.e., words that you do not need to include in the later analyses. For example, in text mining, functional words are usually less informative, thus often excluded in the process of preprocessing. seg3 &lt;- worker(user = &quot;demo_data/dict-ch-user-demo.txt&quot;, stop_word = &quot;demo_data/stopwords-ch-demo.txt&quot;) segment(sents, seg3) [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指&quot; [7] &quot;民眾黨&quot; &quot;不分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; &quot;蔡壁如&quot; [13] &quot;黃瀞瑩&quot; &quot;在昨&quot; &quot;6&quot; &quot;才&quot; &quot;請辭&quot; &quot;為領&quot; [19] &quot;年終獎金&quot; &quot;台灣&quot; &quot;民眾黨&quot; &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; [25] &quot;柯文哲&quot; &quot;7&quot; &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; &quot;按&quot; [31] &quot;流程&quot; &quot;走&quot; &quot;不要&quot; &quot;把&quot; &quot;人家&quot; &quot;想得&quot; [37] &quot;這麼&quot; &quot;壞&quot; &quot;新北&quot; &quot;消防&quot; &quot;救護&quot; &quot;人員&quot; [43] &quot;說&quot; &quot;119&quot; &quot;在&quot; &quot;疫情&quot; &quot;的&quot; &quot;期間&quot; [49] &quot;除&quot; &quot;肩負&quot; &quot;協助&quot; &quot;確診&quot; &quot;患者&quot; &quot;送醫&quot; [55] &quot;的&quot; &quot;任務&quot; &quot;仍&quot; &quot;需&quot; &quot;負擔&quot; &quot;每天&quot; [61] &quot;超過&quot; &quot;500&quot; &quot;件&quot; &quot;以上&quot; &quot;的&quot; &quot;緊急&quot; [67] &quot;救護&quot; &quot;案件&quot; &quot;疫情&quot; &quot;期間&quot; &quot;稍&quot; &quot;一&quot; [73] &quot;不慎&quot; &quot;極&quot; &quot;可能&quot; &quot;造成&quot; &quot;到&quot; &quot;院前&quot; [79] &quot;緊急&quot; &quot;救護&quot; &quot;能量&quot; &quot;和&quot; &quot;醫療&quot; &quot;體系&quot; [85] &quot;的&quot; &quot;崩壞&quot; &quot;目前&quot; &quot;確診&quot; &quot;案件&quot; &quot;不斷&quot; [91] &quot;的&quot; &quot;增加&quot; &quot;國家&quot; &quot;面臨&quot; &quot;疫情&quot; &quot;在&quot; [97] &quot;清零&quot; &quot;與&quot; &quot;共存&quot; &quot;政策&quot; &quot;間&quot; &quot;的&quot; [103] &quot;戰略&quot; &quot;上&quot; &quot;指引&quot; &quot;的&quot; &quot;調整&quot; &quot;勢必&quot; [109] &quot;勢在必行&quot; Exercise 1 How do we quickly check which words in segment(text, seg2) were removed as compared to the results of segment(text, seg3)? (Note: seg2 and seg3 only differ in the stop_word=... argument.) [1] &quot;日&quot; &quot;是&quot; &quot;都&quot; POS Tagging So far we haven’t seen the parts-of-speech tags provided by the word segmenter. If you need the POS tags of the words, you need to specify the argument type = \"tag\" when you initialize the worker(). seg4 &lt;- worker(type = &quot;tag&quot;, #dict = &quot;demo_data/jieba-tw/dict.txt&quot;, user = &quot;demo_data/dict-ch-user-demo.txt&quot;, stop_word = &quot;demo_data/stopwords-ch-demo.txt&quot;, symbol = F) segment(sents, seg4) n ns n x n n x &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指&quot; &quot;民眾黨&quot; x p v n x x x &quot;不分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; &quot;蔡壁如&quot; &quot;黃瀞瑩&quot; &quot;在昨&quot; x d v x n x x &quot;6&quot; &quot;才&quot; &quot;請辭&quot; &quot;為領&quot; &quot;年終獎金&quot; &quot;台灣&quot; &quot;民眾黨&quot; n ns n x x v x &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文哲&quot; &quot;7&quot; &quot;受訪&quot; &quot;時則&quot; zg p n v df p n &quot;說&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;不要&quot; &quot;把&quot; &quot;人家&quot; x r a x n v n &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; &quot;新北&quot; &quot;消防&quot; &quot;救護&quot; &quot;人員&quot; zg m p n uj f p &quot;說&quot; &quot;119&quot; &quot;在&quot; &quot;疫情&quot; &quot;的&quot; &quot;期間&quot; &quot;除&quot; n v v n v uj n &quot;肩負&quot; &quot;協助&quot; &quot;確診&quot; &quot;患者&quot; &quot;送醫&quot; &quot;的&quot; &quot;任務&quot; zg v v r v m zg &quot;仍&quot; &quot;需&quot; &quot;負擔&quot; &quot;每天&quot; &quot;超過&quot; &quot;500&quot; &quot;件&quot; f uj a v n n f &quot;以上&quot; &quot;的&quot; &quot;緊急&quot; &quot;救護&quot; &quot;案件&quot; &quot;疫情&quot; &quot;期間&quot; zg m a d v v v &quot;稍&quot; &quot;一&quot; &quot;不慎&quot; &quot;極&quot; &quot;可能&quot; &quot;造成&quot; &quot;到&quot; s a v n c n n &quot;院前&quot; &quot;緊急&quot; &quot;救護&quot; &quot;能量&quot; &quot;和&quot; &quot;醫療&quot; &quot;體系&quot; uj v t v n d uj &quot;的&quot; &quot;崩壞&quot; &quot;目前&quot; &quot;確診&quot; &quot;案件&quot; &quot;不斷&quot; &quot;的&quot; v n v n p z zg &quot;增加&quot; &quot;國家&quot; &quot;面臨&quot; &quot;疫情&quot; &quot;在&quot; &quot;清零&quot; &quot;與&quot; v n f uj n f v &quot;共存&quot; &quot;政策&quot; &quot;間&quot; &quot;的&quot; &quot;戰略&quot; &quot;上&quot; &quot;指引&quot; uj vn d i &quot;的&quot; &quot;調整&quot; &quot;勢必&quot; &quot;勢在必行&quot; The returned object is a named character vector, i.e., the POS tags of the words are included in the names of the vectors. Every POS tagger has its own predefined tag set. The following table lists the annotations of the POS tag set used in jiebaR: Exercise 2 How do we convert the named word vector with POS tags returned by segment(text, seg4) into a long string as shown below? [1] &quot;綠黨/n 桃園市/ns 議員/n 王浩宇/x 爆料/n 指/n 民眾黨/x 不分區/x 被/p 提名/v 人/n 蔡壁如/x 黃瀞瑩/x 在昨/x 6/x 才/d 請辭/v 為領/x 年終獎金/n 台灣/x 民眾黨/x 主席/n 台北/ns 市長/n 柯文哲/x 7/x 受訪/v 時則/x 說/zg 按/p 流程/n 走/v 不要/df 把/p 人家/n 想得/x 這麼/r 壞/a 新北/x 消防/n 救護/v 人員/n 說/zg 119/m 在/p 疫情/n 的/uj 期間/f 除/p 肩負/n 協助/v 確診/v 患者/n 送醫/v 的/uj 任務/n 仍/zg 需/v 負擔/v 每天/r 超過/v 500/m 件/zg 以上/f 的/uj 緊急/a 救護/v 案件/n 疫情/n 期間/f 稍/zg 一/m 不慎/a 極/d 可能/v 造成/v 到/v 院前/s 緊急/a 救護/v 能量/n 和/c 醫療/n 體系/n 的/uj 崩壞/v 目前/t 確診/v 案件/n 不斷/d 的/uj 增加/v 國家/n 面臨/v 疫情/n 在/p 清零/z 與/zg 共存/v 政策/n 間/f 的/uj 戰略/n 上/f 指引/v 的/uj 調整/vn 勢必/d 勢在必行/i&quot; Chinese Text Analytics Pipeline Below is a quick overview of the Chinese Text Analytics Flowchart (Figure 1). In this workshop, I will introduce two ways to process Chinese corpus data in two different frameworks: Quanteda Framework Tidytext Framework As these two packages are more optimized for the English language, I will highlight a few important steps that require more attention with respect to Chinese word segmentation. Figure 1: Chinese Text Analytics Flowchart Quanteda Framework First, let’s start with the Quanteda-based framework of text processing. In quanteda, textual data are loaded as corpus object. With the corpus object, we can apply quanteda::summary(), and the statistics of tokens and types are based on the Quanteda-native word segmentation; Or we can use the Quanteda-native tokenization method, tokens(), to convert the corpus object into tokens object and apply quanteda::kwic() to get concordance lines. sents [1] &quot;綠黨桃園市議員王浩宇爆料，指民眾黨不分區被提名人蔡壁如、黃瀞瑩，在昨（6）日才請辭是為領年終獎金。台灣民眾黨主席、台北市長柯文哲7日受訪時則說，都是按流程走，不要把人家想得這麼壞。&quot; [2] &quot;新北消防救護人員說，119在疫情的期間，除肩負協助確診患者送醫的任務，仍需負擔每天超過500件以上的緊急救護案件，疫情期間稍一不慎，極可能造成到院前，緊急救護能量和醫療體系的崩壞；目前確診案件不斷的增加，國家面臨疫情是在清零與共存政策間的戰略上，指引的調整勢必勢在必行。&quot; ## create corpus object text_corpus &lt;- corpus(sents) ## summary summary(text_corpus) ## Create tokens object text_tokens &lt;- tokens(text_corpus) ## Check quanteda-native word tokenization result text_tokens[[1]] [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王&quot; &quot;浩&quot; &quot;宇&quot; [7] &quot;爆&quot; &quot;料&quot; &quot;，&quot; &quot;指&quot; &quot;民眾&quot; &quot;黨&quot; [13] &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; &quot;蔡&quot; [19] &quot;壁&quot; &quot;如&quot; &quot;、&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; [25] &quot;，&quot; &quot;在&quot; &quot;昨&quot; &quot;（&quot; &quot;6&quot; &quot;）&quot; [31] &quot;日&quot; &quot;才&quot; &quot;請辭&quot; &quot;是&quot; &quot;為&quot; &quot;領&quot; [37] &quot;年終獎金&quot; &quot;。&quot; &quot;台灣&quot; &quot;民眾&quot; &quot;黨主席&quot; &quot;、&quot; [43] &quot;台北市&quot; &quot;長&quot; &quot;柯&quot; &quot;文&quot; &quot;哲&quot; &quot;7&quot; [49] &quot;日&quot; &quot;受&quot; &quot;訪&quot; &quot;時&quot; &quot;則&quot; &quot;說&quot; [55] &quot;，&quot; &quot;都是&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;，&quot; [61] &quot;不要&quot; &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; [67] &quot;。&quot; ## KWIC kwic(text_tokens, pattern = &quot;柯文哲&quot;) kwic(text_tokens, pattern = &quot;柯&quot;) Please note that there are no tokens of concordance lines from kwic(text_corpus, pattern = \"柯文哲\"). It is clear to see that quite a few word tokens have not been successfully identified by the Quanteda-native word segmentation (e.g., several proper names in the text). This would also have great impact on the effectiveness of kwic() as well. Therefore analysis based on the Quanteda-native segmentation can be very limited. Now let’s improve the word segmentation by using self-defined word segmenter based on jiebaR. Quanteda Framework: Revised To perform word tokenization under the Quanteda framework, we need to create the tokens object using a self-defined tokenization function based on jiebar. With this tokens object, we can apply kiwc() or other Quanteda-supported processing to the corpus data. The steps are very straightforward. We utilize jiebar for word tokenization like before: (1) initialize a jiebar model and (2) use it to tokenize the corpus text with segment(). The key is that we need to convert the output of segment() from a list to a tokens using as.tokens(). # initialize segmenter my_seg &lt;- worker(bylines = T, user = &quot;demo_data/dict-ch-user-demo.txt&quot;, symbol=T) ## create tokens based on self-defined segmentation text_tokens &lt;- sents %&gt;% segment(jiebar = my_seg) %&gt;% as.tokens ## kwic on word tokens kwic(text_tokens, pattern = &quot;柯文哲&quot;) kwic(text_tokens, pattern = &quot;.*?[台市].*?&quot;, valuetype = &quot;regex&quot;) Tidytext Framework Now let’s turn to the Titytext framework of text processing. The tidytext package is made for the handling of the tidy text format of the corpus data, i.e., to process textual data on the basis of data frames. With a data frame format of the text data, we can manipulate the text data with a standard set of tidy tools and packages, including dplyr, tidyr, and ggplot2. In the tidytext format, the tokenization is taken care of by unnest_tokens(). # a text-based tidy corpus corp_df &lt;- data.frame(text_id = c(1,2), text = sents) corp_df corp_df Then, we use unnest_tokens() to tokenize the text-based data frame (corp_df) into a word-based data frame (corp_df_word). Texts included in the text column are tokenized into words, which are unnested into independent rows in the word column of the new data frame. # tokenization corp_df_word &lt;- corp_df %&gt;% unnest_tokens( word, ## new tokens unnested text, ## original larger units token = function(x) ## self-defined tokenization method segment(x, jiebar = my_seg) ) corp_df_word It can be seen that for the token parameter in unnest_tokens(), we use an anonymous function based on jieba and segment() for self-defined Chinese word segmentation. This is called anonymous functions because it has not been assigned to any object name in the current R session. You may check R language documentation for more detail on Writing Functions. It is important to note that when we specify a self-defined unnest_tokens(…,token=…) function, this function should take a character vector (i.e., a text-based vector) and return a list of character vectors (i.e., word-based vectors) of the same length. In other words, when initializing the Chinese word segmenter, we need to specify the argument worker(…, byline = TRUE). "],["applications.html", "Applications Loading Data Overview of the Data Set Initialize jiebaR Quanteda Framework Case Study 1: Concordances with kwic() Case Study 2: Collocations Case Study 3: Word Frequency and Wordcloud Case Study 4: Patterns Case Study 5: Lexical Bundles Recap", " Applications In this section, we will look at a few more examples of Chinese text processing based on the data set demo_data/song-jay-amei-v1.csv. It is a text collection of songs by Jay Chou and Amei. Loading Data corp &lt;- read_csv(&quot;demo_data/song-jay-amei-v1.csv&quot;) corp Overview of the Data Set A quick frequency counts of the songs from each artist: corp %&gt;% ggplot(aes(artist, fill=artist)) + geom_bar() Initialize jiebaR Because we are going to use jiebaR for word tokenization, we first initialize the jiebaR models. Here we created two jiebaR objects, one for word tokenization only and the other for parts-of-speech tagging. # initialize segmenter ## for word segmentation only my_seg &lt;- worker(bylines = T, #user = &quot;&quot;, symbol = T) ## for POS tagging my_seg_pos &lt;- worker( type = &quot;tag&quot;, bylines = F, #user = &quot;&quot;, symbol = T ) Even though we have specified a user-defined dictionary in the initialization of the worker(), we can also add add-hoc new words to the model. This can be very helpful when we spot any weird segmentation results in the output. By default, new_user_word() assigns each new word with a default n tag. #Add customized terms temp_new_words &lt;-c(&quot;&quot;) new_user_word(my_seg, temp_new_words) [1] TRUE new_user_word(my_seg_pos, temp_new_words) [1] TRUE Quanteda Framework First, we perform the tokenization and create the tokens object. ## create tokens based on self-defined segmentation corp_tokens &lt;- corp$lyric %&gt;% segment(jiebar = my_seg) %&gt;% as.tokens ## document-level variables docvars(corp_tokens) &lt;- corp[, c(&quot;artist&quot;,&quot;lyricist&quot;,&quot;composer&quot;,&quot;title&quot;,&quot;gender&quot;)] Case Study 1: Concordances with kwic() This is an example of processing the Chinese data under Quanteda framework. Without relying on the Quanteda-native tokenization, we can create the tokens object directly based on the output of the jiebar segment(). With this tokens object, we can perform the concordance analysis with kwic(). kwic(corp_tokens, &quot;快樂&quot;) corp_tokens_subset &lt;- tokens_subset(corp_tokens, str_detect(lyricist, &quot;方文山&quot;)) textplot_xray(kwic(corp_tokens_subset, &quot;快樂&quot;), kwic(corp_tokens_subset, &quot;難過&quot;)) corp_tokens %&gt;% dfm() %&gt;% dfm_group ( groups = artist) %&gt;% dfm_trim(min_termfreq = 10, min_docfreq = 2, verbose = F) %&gt;% textplot_wordcloud(comparison=TRUE,) Case Study 2: Collocations ## bigrams textstat_collocations(corp_tokens, size = 2, min_count = 10) %&gt;% arrange(-z) Case Study 3: Word Frequency and Wordcloud The following are examples of processing the Chinese texts under the tidy structure framework. Recall the three important steps: Load the corpus data using readtext() and create a text-based data frame of the corpus; Initialize a jieba word segmenter using worker() Tokenize the text-based data frame into a word-based tidy data frame using unnest_tokens() ## Create index corp %&gt;% mutate(doc_id = row_number()) -&gt; corp_text_df corp_df_word &lt;- corp_text_df %&gt;% unnest_tokens( output = word, input = lyric, token = function(x) segment(x, jiebar = my_seg) ) %&gt;% group_by(doc_id) %&gt;% mutate(word_id = row_number()) %&gt;% # create word index within each document ungroup corp_df_word Creating unique indices for your data is very important. In corpus linguistic analysis, we often need to keep track of the original context of the word, phrase or sentence in the concordances. All these unique indices (as well as the source text filenames) would make things a lot easier. Also, if the metadata of the source documents are available, these unique indices would allow us to connect the tokenized linguistic units to the metadata information (e.g., genres, registers, author profiles) for more interesting analysis. With a word-based data frame, we can easily create a word frequency list as well as a word cloud to have a quick overview of the word distribution of the corpus. It should be noted that before creating the frequency list, we need to consider whether to remove unimportant tokens (e.g., stopwords, symbols, punctuation, digits, alphabets.) ## load chinese stopwords stopwords_chi &lt;- readLines(&quot;demo_data/stopwords-ch-jiebar-zht.txt&quot;) ## create word freq list corp_word_freq &lt;- corp_df_word %&gt;% filter(!word %in% stopwords_chi) %&gt;% # remove stopwords filter(word %&gt;% str_detect(pattern = &quot;\\\\D+&quot;)) %&gt;% # remove words consisting of digits count(word) %&gt;% arrange(desc(n)) library(wordcloud2) corp_word_freq %&gt;% filter(n &gt; 20) %&gt;% filter(nchar(word) &gt;= 2) %&gt;% ## remove monosyllabic tokens wordcloud2(shape = &quot;pentagon&quot;, size = 0.3) Case Study 4: Patterns In this case study, we are looking at a more complex example. In corpus linguistic analysis, we often need to extract a particular pattern from the texts. In order to retrieve the target patterns at a high accuracy rate, we often need to make use of the additional annotations provided by the corpus. The most often-used information is the parts-of-speech tags of words. So here we demonstrate how to enrich our corpus data by adding POS tags information to our current tidy corpus design. Our steps are as follows: Initialize jiebar object, which performs not only word segmentation but also POS tagging; Create a self-defined function to word-seg and pos-tag each text and combine all tokens, word/tag, into a long string for each text; With the text-based apple_df, create a new column, which includes the enriched version of each text, using mutate() # define a function to word-seg and pos-tag a text tag_text &lt;- function(x, jiebar) { segment(x, jiebar) %&gt;% ## tokenize paste(names(.), sep = &quot;/&quot;, collapse = &quot; &quot;) ## reformat output } # demo of the function `tag_text()` tag_text(corp$lyric[1], my_seg_pos) [1] &quot;若非/c 狠下/d 心/n 拿/v 什麼/r 想/v 妳/zg \\n/x 想成/v 了/ul 風雨/n ‭/x /x 對不起/l \\n/x 保護/v 一顆/m 心看/x 多/m 了/ul 烏雲/nr \\n/x 兩忘/x 曾經/d 裡/zg /x 不/d 怨/v 妳/zg \\n/x ‬/x \\n/x 心中/s 有/v 心語/n ‭/x /x ‬/x ‬/x 妳/zg 我/r 是/v 雙影/n ‭/x \\n/x ‬/x 一半/m 無情/n ‭/x /x 另一半/d 深情/n \\n/x ‬/x 貪/v 一點/m 愛/zg 什麼/r 痛/a 也/d 允許/v ‭/x \\n/x ‬/x 用/p 懷疑/v 交換/v ‭/x /x ‬/x 秘密/n \\n/x 寵愛/v 和/c 被忘/x ‭/x /x 在/p 心中/s 交談/v ‭/x (/x 說來/v 迷惘/a )/x \\n/x 妳/zg 作證/v 我/r 的/uj 冷暖/an ‭/x /x 悲歡/v (/x 夢/n 短路/n 長/zg )/x \\n/x 妳/zg 拉/v 我/r 的/uj 手繡/n 一件/m 孤單/a (/x 絲綢/n 堆/v 了/ul 月光/n )/x \\n/x 說/zg 用來/v 取暖/v /x 誰/zg 敢/v \\n/x 命/n 在/p 誰/zg 命裡/x ‭/x /x ‬/x 愛恨/a 是/v 雙影/n \\n/x 一端/m 美麗/ns ‭/x /x 另一端/i 無語/nz \\n/x 遠遠/d 走來/v 沒有/v 字/n 的/uj 未來/t \\n/x 被/p 時間/n 教會/n ‭/x /x 也許/d \\n/x 情斷/x 留情/v 意/n ‭/x /x 忘記/v 是/v 雙影/n \\n/x 一天/m 冷淡/a 另/r 一天/m 想起/v \\n/x 但願/v 我們/r 永遠/d 走/v 在/p 光裡/x \\n/x 這/zg 一生/m 如此/c ‭/x /x 多雲/nr \\n/x 這/zg 一生/m 從此/c ‭/x /x 無/v 雲/ns \\n/x 感謝/v \\n/x 好/a 青年/t \\n/x 提供/v 歌詞/n&quot; # apply `tag_text()` function to each text corp_anno &lt;- corp %&gt;% mutate(lyric_tag = map_chr(lyric, tag_text, my_seg_pos)) corp_anno Now we have obtained an enriched version of the texts, we can make use of the POS tags for construction analysis. Let’s look at the example of X + 男人/女人 Construction. The data retrieval procedure is now very straightforward: we only need to create a regular expression that matches our construction and go through the enriched version of the texts (i.e., text_tag column in apple_df) to identify these matches with unnest_tokens(). 1.Define a regular expression \\\\b被/p\\\\s([^/]+/[^\\\\s]+\\\\s)*?[^/]+/v for BEI-Construction, i.e., 被 + VERB 2.Use unnest_tokens() and str_extract_all() to extract target patterns and create a pattern-based data frame. # define regex patterns #pat &lt;- &quot;\\\\s[^/]+/a[^\\\\s]*\\\\s[男女]./n&quot; pat &lt;- &quot;[^/\\\\s]+/a\\\\b&quot; # extract patterns from corp corp_anno %&gt;% #select(-lyric) %&gt;% # `text` is the column with original raw texts unnest_tokens( output = pat, ## pattern name input = lyric_tag, ## original base linguistic unit token = function(x) str_extract_all(x, pattern = pat) ) -&gt; result result %&gt;% mutate(word = str_replace_all(pat, &quot;/.+$&quot;,&quot;&quot;)) %&gt;% group_by(artist) %&gt;% count(word, sort = T) %&gt;% top_n(20) %&gt;% ungroup %&gt;% arrange(artist, -n) -&gt; result_df result_df %&gt;% mutate(word = reorder_within(word, n, artist)) %&gt;% ggplot(aes(word, n, fill=artist)) + geom_bar(stat=&quot;identity&quot;)+ coord_flip()+ facet_wrap(~artist,scales = &quot;free_y&quot;) + scale_y_reordered() + theme(text= element_text(family=&quot;Arial Unicode MS&quot;)) + labs(x = &quot;Frequency&quot;, y = &quot;Adjectives&quot;, title=&quot;Top 20 Adjectives of Each Artist&#39;s Songs&quot;) Case Study 5: Lexical Bundles N-grams Extraction With word boundaries, we can also analyze the recurrent multiword units in Chinese news. Here let’s take a look at the recurrent four-grams in our Chinese corpus. As the default n-gram tokenization in unnest_tokens(..., token = \"ngrams\") only works with the English data, we need to define our own ngram tokenization functions. The Chinese ngram tokenization function should: tokenize each text into word tokens create a set of ngrams from the word tokens of each text ## self defined ngram tokenizer tokenizer_ngrams &lt;- function(texts, jiebar, n = 2 , skip = 0, delimiter = &quot;_&quot;) { texts %&gt;% segment(jiebar) %&gt;% ## word tokenization as.tokens %&gt;% ## list to tokens tokens_ngrams(n, skip, concatenator = delimiter) %&gt;% ## ngram tokenization as.list ## tokens to list } In the above self-defined ngram tokenizer, we make use of tokens_ngrams() in quanteda, which creates a set of ngrams from already tokenized text objects, i.e., tokens. Because this function requires a tokens object as the input, we need to do the class conversion via as.tokens() and as.list(). Take a look at the following examples for a quick overview of tokens_ngrams(): sents &lt;- c(&quot;Jack and Jill went up the hill to fetch a pail of water&quot;, &quot;Jack fell down and broke his crown and Jill came tumbling after&quot;) sents_tokens &lt;- tokens(sents) ## English supported tokens_ngrams(sents_tokens, n = 2, skip = 0) Tokens consisting of 2 documents. text1 : [1] &quot;Jack_and&quot; &quot;and_Jill&quot; &quot;Jill_went&quot; &quot;went_up&quot; &quot;up_the&quot; &quot;the_hill&quot; [7] &quot;hill_to&quot; &quot;to_fetch&quot; &quot;fetch_a&quot; &quot;a_pail&quot; &quot;pail_of&quot; &quot;of_water&quot; text2 : [1] &quot;Jack_fell&quot; &quot;fell_down&quot; &quot;down_and&quot; &quot;and_broke&quot; [5] &quot;broke_his&quot; &quot;his_crown&quot; &quot;crown_and&quot; &quot;and_Jill&quot; [9] &quot;Jill_came&quot; &quot;came_tumbling&quot; &quot;tumbling_after&quot; tokens_ngrams(sents_tokens, n = 2, skip = 1) Tokens consisting of 2 documents. text1 : [1] &quot;Jack_Jill&quot; &quot;and_went&quot; &quot;Jill_up&quot; &quot;went_the&quot; &quot;up_hill&quot; [6] &quot;the_to&quot; &quot;hill_fetch&quot; &quot;to_a&quot; &quot;fetch_pail&quot; &quot;a_of&quot; [11] &quot;pail_water&quot; text2 : [1] &quot;Jack_down&quot; &quot;fell_and&quot; &quot;down_broke&quot; &quot;and_his&quot; [5] &quot;broke_crown&quot; &quot;his_and&quot; &quot;crown_Jill&quot; &quot;and_came&quot; [9] &quot;Jill_tumbling&quot; &quot;came_after&quot; tokens_ngrams(sents_tokens, n = 5, skip = 0) Tokens consisting of 2 documents. text1 : [1] &quot;Jack_and_Jill_went_up&quot; &quot;and_Jill_went_up_the&quot; &quot;Jill_went_up_the_hill&quot; [4] &quot;went_up_the_hill_to&quot; &quot;up_the_hill_to_fetch&quot; &quot;the_hill_to_fetch_a&quot; [7] &quot;hill_to_fetch_a_pail&quot; &quot;to_fetch_a_pail_of&quot; &quot;fetch_a_pail_of_water&quot; text2 : [1] &quot;Jack_fell_down_and_broke&quot; &quot;fell_down_and_broke_his&quot; [3] &quot;down_and_broke_his_crown&quot; &quot;and_broke_his_crown_and&quot; [5] &quot;broke_his_crown_and_Jill&quot; &quot;his_crown_and_Jill_came&quot; [7] &quot;crown_and_Jill_came_tumbling&quot; &quot;and_Jill_came_tumbling_after&quot; # examples texts &lt;- c(&quot;這是一個測試的句子&quot;, &quot;這句子&quot;, &quot;超短句&quot;, &quot;最後一個超長的句子測試&quot;) tokenizer_ngrams( texts = texts, jiebar = my_seg, n = 2, skip = 0, delimiter = &quot;_&quot; ) $text1 [1] &quot;這是_一個&quot; &quot;一個_測試&quot; &quot;測試_的&quot; &quot;的_句子&quot; $text2 [1] &quot;這_句子&quot; $text3 [1] &quot;超短_句&quot; $text4 [1] &quot;最後_一個&quot; &quot;一個_超長&quot; &quot;超長_的&quot; &quot;的_句子&quot; &quot;句子_測試&quot; tokenizer_ngrams( texts = texts, jiebar = my_seg, n = 2, skip = 1, delimiter = &quot;_&quot; ) $text1 [1] &quot;這是_測試&quot; &quot;一個_的&quot; &quot;測試_句子&quot; $text2 character(0) $text3 character(0) $text4 [1] &quot;最後_超長&quot; &quot;一個_的&quot; &quot;超長_句子&quot; &quot;的_測試&quot; tokenizer_ngrams( texts = texts, jiebar = my_seg, n = 5, skip=0, delimiter = &quot;/&quot; ) $text1 [1] &quot;這是/一個/測試/的/句子&quot; $text2 character(0) $text3 character(0) $text4 [1] &quot;最後/一個/超長/的/句子&quot; &quot;一個/超長/的/句子/測試&quot; With the self-defined ngram tokenizer, we can now perform the ngram tokenization on our Chinese corpus: We transform the text-based data frame into an ngram-based data frame using unnest_tokens(...) with the self-defined tokenization function tokenizer_ngrams() We remove empty and unwanted n-grams entries: Empty ngrams due to short texts Ngrams spanning punctuations, symbols, or paragraph breaks Ngrams including alphanumeric characters ## from text-based to ngram-based system.time( corp_text_df %&gt;% unnest_tokens( ngram, lyric, token = function(x) tokenizer_ngrams( texts = x, jiebar = my_seg, n = 4, skip = 0, delimiter = &quot;_&quot; ) ) -&gt; corp_ngram ) ## end system.time user system elapsed 0.590 0.016 0.332 ## remove unwanted ngrams corp_ngram2 &lt;- corp_ngram %&gt;% filter(nzchar(ngram)) %&gt;% ## empty strings filter(!str_detect(ngram, &quot;[^\\u4E00-\\u9FFF_]&quot;)) ## remove unwanted ngrams In the above regular expression, the Unicode range [\\u4E00-\\u9FFF] includes frequently used Chinese characters. Therefore, the way we remove unwanted ngrams is to identify all the ngrams that include non-Chinese characters that are outside of this Unicode range (as well as the delimiter _). For more information related to the Unicode range for the punctuations in CJK languages, please see this SO discussion thread. Frequency and Dispersion As we have discussed in Chapter ??, a multiword unit can be defined based on at least two important distributional properties (See Biber, Conrad, and Cortes (2004)): The frequency of the whole multiword unit (i.e., frequency) The number of texts where the multiword unit is observed (i.e., dispersion) Now that we have the ngram-based DF, we can compute their token frequencies and document frequencies in the corpus using the normal data manipulation tricks. We set cut-offs for four-grams at: dispersion &gt;= 5 (i.e., four-grams that occur in at least five different documents) system.time( corp_ngram_dist &lt;- corp_ngram2 %&gt;% group_by(ngram) %&gt;% summarize(freq = n(), dispersion = n_distinct(doc_id)) %&gt;% filter(dispersion &gt;= 3) ) #end system.time user system elapsed 0.706 0.005 0.712 Please take a look at the four-grams, arranged by frequency and dispersion respectively: # arrange by dispersion corp_ngram_dist %&gt;% arrange(desc(dispersion)) %&gt;% head(10) # arrange by freq corp_ngram_dist %&gt;% arrange(desc(freq)) %&gt;% head(10) We can also look at four-grams with particular lexical words: corp_ngram_dist %&gt;% filter(str_detect(ngram, &quot;我&quot;)) %&gt;% arrange(desc(dispersion)) corp_ngram_dist %&gt;% filter(str_detect(ngram, &quot;你&quot;)) %&gt;% arrange(desc(dispersion)) Recap Tokenizations are complex in Chinese text processing. Many factors may need to be taken into account when determining the right tokenization method. In particular, several important questions may be relevant to Chinese text tokenization: Do you need the parts-of-speech tags of words in your research? What is the base unit you would like to work with? Texts? Paragraphs? Chunks? Sentences? N-grams? Words? Do you need non-word tokens such as symbols, punctuation, digits, or alphabets in your analysis? Your answers to the above questions should help you determine the most effective structure of the tokenization methods for your data. 感謝聆聽! References "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
