[["index.html", "Processing Chinese Data with R Introduction Structure of the Workshop Data Materials Resources", " Processing Chinese Data with R Alvin Cheng-Hsien Chen alvinchen@ntnu.edu.tw National Taiwan Normal University, Taiwan April 26, 2022, @ HKSYU (Online Workshop) Introduction The theme of this workshop is Chinese text processing with R. I will introduce a few useful R libraries that can be utilized for text analytic tasks. Also, I will focus on an important step in Chinese processing, namely, the word segmentation/tokenization, and show how to attend to this step in the pipeline. Finally, I will demonstrate a few potential applications of data analysis &amp; visualization with the help of the attractive informative graphs with R. Structure of the Workshop Environment Setup Loading Text Data Chinese Word Segmentation Applications Data All the data sets and the R scripts used in this workshop can be downloaded here as a zipped file: WorkshopHKSYU.zip. (Click the Download button to download everything as a zipped file). After downloading, unzip the file and you will see a new directory, WorkshopHKSYU, in the directory where you save the zipped file. In this WorkshopHKSYU directory, you will find all the R script files (WorkshopHKSYU/*.R) and a sub-directory (WorkshopHKSYU/demo_data) required for this workshop. Please double click the R script file to start your RStudio/R. Materials In the workshop notes, the text boxes in light blue refer to the R codes that you need to run in the R console. The text boxes in black background show the outputs of the code processing. We will follow this presentation convention throughout the entire notes. print(&#39;Hello! R!&#39;) [1] &quot;Hello! R!&quot; Resources R Fundamentals (Wickham and Grolemund 2017; Davies 2016) Corpus Processing (Gries 2016; Stefanowitsch 2019) Statistics (Gries 2021; Baayen 2008; Brezina 2018; Winter 2020) References "],["environment-setup.html", "Environment Setup R and RStudio Libraries Environment Checking", " Environment Setup R and RStudio In this workshop, I expect that all the participants have installed an working R environment and the IDE RStudio in their workstation. Libraries There are many useful and effective R libraries for text processing. I would highly recommend the official webpage, CRAN Task Views, which provides a theme-based categorization of the R libraries. Of particular relevance in this workshop is the CRAN Task Topic – Natural Language Processing. Specifically, in this workshop, I will use the following libraries. Based on my experiences with data processing, I think these libraries are quite effective and easy to learn. tidyverse: This library includes a collection of useful R libraries for data analysis. quanteda: This library provides many useful functions for exploratory quantitative text analysis. tidytext: This library provides many useful functions for computational text analysis under the same tidy structure framework. jiebar: This library provides functions for Chinese tokenization (i.e., word segmentation). readtext: This library provides functions for effectively importing text files into R. These libraries can be easily installed in R using the following codes: ## Install these libraries if you haven&#39;t install.packages(&quot;tidyverse&quot;) install.packages(&quot;quanteda&quot;) install.packages(&quot;quanteda.textplots&quot;) install.packages(&quot;quanteda.textstats&quot;) install.packages(&quot;readtext&quot;) install.packages(&quot;tidytext&quot;) install.packages(&quot;jiebaR&quot;) install.packages(&quot;wordcloud2&quot;) Environment Checking The R Version to produce these lecture notes: R version 4.1.2 (2021-11-01). After you install the libraries, you can import these libraries in the current R session for the later use: ## Importing libraries library(tidyverse) library(quanteda) library(quanteda.textplots) library(quanteda.textstats) library(readtext) library(tidytext) library(jiebaR) library(wordcloud2) ## Checking library versions packageVersion(&quot;tidyverse&quot;) [1] &#39;1.3.1&#39; packageVersion(&quot;quanteda&quot;) [1] &#39;3.2.1&#39; packageVersion(&quot;quanteda.textplots&quot;) [1] &#39;0.94.1&#39; packageVersion(&quot;quanteda.textstats&quot;) [1] &#39;0.95&#39; packageVersion(&quot;tidytext&quot;) [1] &#39;0.3.2&#39; packageVersion(&quot;jiebaR&quot;) [1] &#39;0.11&#39; packageVersion(&quot;wordcloud2&quot;) [1] &#39;0.2.2&#39; Or alternatively, we can run the function sessionInfo() to get an overview of the current R environment: ## Check the session info sessionInfo() R version 4.1.2 (2021-11-01) Platform: x86_64-apple-darwin17.0 (64-bit) Running under: macOS Catalina 10.15.7 Matrix products: default BLAS: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib LAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib locale: [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 attached base packages: [1] stats graphics grDevices utils datasets methods base other attached packages: [1] wordcloud2_0.2.2 jiebaR_0.11 [3] jiebaRD_0.1 tidytext_0.3.2 [5] readtext_0.81 quanteda.textstats_0.95 [7] quanteda.textplots_0.94.1 quanteda_3.2.1 [9] forcats_0.5.1 stringr_1.4.0 [11] dplyr_1.0.7 purrr_0.3.4 [13] readr_2.1.1 tidyr_1.1.4 [15] tibble_3.1.6 ggplot2_3.3.5 [17] tidyverse_1.3.1 showtext_0.9-4 [19] showtextdb_3.0 sysfonts_0.8.5 loaded via a namespace (and not attached): [1] fs_1.5.2 lubridate_1.8.0 httr_1.4.2 SnowballC_0.7.0 [5] tools_4.1.2 backports_1.4.1 bslib_0.3.1 utf8_1.2.2 [9] R6_2.5.1 DBI_1.1.2 colorspace_2.0-2 withr_2.4.3 [13] tidyselect_1.1.1 compiler_4.1.2 cli_3.1.0 rvest_1.0.2 [17] xml2_1.3.3 bookdown_0.24 sass_0.4.0 scales_1.1.1 [21] digest_0.6.29 rmarkdown_2.11 pkgconfig_2.0.3 htmltools_0.5.2 [25] dbplyr_2.1.1 fastmap_1.1.0 highr_0.9 htmlwidgets_1.5.4 [29] rlang_0.4.12 readxl_1.3.1 rstudioapi_0.13 jquerylib_0.1.4 [33] generics_0.1.1 jsonlite_1.7.2 tokenizers_0.2.1 magrittr_2.0.1 [37] Matrix_1.4-0 Rcpp_1.0.8.3 munsell_0.5.0 fansi_0.5.0 [41] lifecycle_1.0.1 stringi_1.7.6 yaml_2.2.1 grid_4.1.2 [45] crayon_1.4.2 lattice_0.20-45 haven_2.4.3 hms_1.1.1 [49] knitr_1.37 pillar_1.6.4 stopwords_2.3 fastmatch_1.1-3 [53] reprex_2.0.1 glue_1.6.0 evaluate_0.14 data.table_1.14.2 [57] RcppParallel_5.1.4 modelr_0.1.8 vctrs_0.3.8 tzdb_0.2.0 [61] cellranger_1.1.0 gtable_0.3.0 assertthat_0.2.1 xfun_0.29 [65] broom_0.7.11 janeaustenr_0.1.5 nsyllable_1.0 ellipsis_0.3.2 If your R version is older than the above one, please consider updating your R. Details about updating R can be found in: 3 Methods to Update R &amp; Rstudio (For Windows &amp; Mac) Updating R, Rstudio, and Your Packages "],["loading-text-data.html", "Loading Text Data Text Data Types Data Description CSV Format Directory Format Zipped File Format Encoding Issues", " Loading Text Data Text Data Types There are generally three types of formats for text data storage on the hard drive: CSV: demo_data/song-jay-amei-v1.csv Directory: demo_data/corp_dir A Zipped File: demo_data/song-jay-amei-v2.tar.gz In this workshop, I will show you how to load the data of these three types into R. Data Description In this workshop, we will use the song lyrics data set for illustration. It is a text collection of song lyrics from two artists: Jay Chou (周杰倫) and Amei Chang (張惠妹). CSV Format For small data sets, people often store their textual data as a spreadsheet-like table. We can easily load this type of data using read_csv() (or read_tsv()). ## CSV Format corp &lt;- read_csv(file = &quot;demo_data/song-jay-amei-v1.csv&quot;, ## filename locale = locale(encoding = &quot;UTF-8&quot;)) ## encoding corp If you see garbled characters in the data frame, that may be due to the issue of encoding. Please see Encoding Issues. Directory Format Sometimes, if we have a lot of text documents, we may include all the documents of the corpus in one directory. With the data structure like this, we can load the entire corpus directory, using readtext(): ## Directory Format corp &lt;- readtext(file=&quot;demo_data/corp_dir&quot;, docvarsfrom = &quot;filenames&quot;, ## parsing filenames dvsep=&quot;-&quot;, ## delimiter docvarnames = c(&quot;artist&quot;, &quot;index&quot;), ## doc-level info encoding = &quot;UTF-8&quot;) ## encoding corp Very often the filenames of the text files are meaningful when we create the corpus. That is, we often include metadata information of each document in the filenames. For example, the text filename Amei-1.txt indicates both the artist’s name as well as its unique integer index So when we load the text collection, readtext() comes with the functionality to parse these important pieces of metadata information from the filenames. Zipped File Format When the corpus is large, people may zip the entire corpus as an archived zipped file to save storage space on the hard drive. To load the data set like this, we can also use readtext(). ## Zipped Format corp &lt;- readtext(file = &quot;demo_data/song-jay-amei-v2.tar.gz&quot;, docvarsfrom = &quot;filenames&quot;, dvsep=&quot;-&quot;, docvarnames = c(&quot;artist&quot;, &quot;index&quot;), encoding = &quot;UTF-8&quot;) corp The function readtext() works on: text (.txt) files; comma-separated-value (.csv) files; XML formatted data; data from the Facebook API, in JSON format; data from the Twitter API, in JSON format; and generic JSON data. Encoding Issues In computational text processing, the default encoding of text data is usually UTF-8. However, not all operating systems take UTF-8 as the default encoding for text file management. This has a lot to do with the operating system locale. In R, we can check the system locale with: Sys.getlocale() [1] &quot;en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8&quot; For Windows users, if your locale is English, you may see: [1] &quot;LC_COLLATE=English_United States.1252;LC_CTYPE=English_United States.1252;LC_MONETARY=English_United States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252&quot; If your locale is Chinese, you may see: [1] &quot;LC_COLLATE=Chinese (Traditional)_Taiwan.950;LC_CTYPE=Chinese (Traditional)_Taiwan.950;LC_MONETARY=Chinese (Traditional)_Taiwan.950;LC_NUMERIC=C;LC_TIME=Chinese (Traditional)_Taiwan.950&quot; For Mac, the default encoding is UTF-8. For Windows, things can get more complicated. In general, the default encoding in Windows is locale-dependent (i.e., depending on the OS language setting: big-5 for Chinese-Taiwan). In Windows, we can change the locale in R as below: ## from Chinese to English Sys.setlocale(category = &quot;LC_ALL&quot;, locale = &quot;UTF-8&quot;) ## from English to traditional Chinese Sys.setlocale(category = &quot;LC_ALL&quot;, locale = &quot;cht&quot;) Sys.setlocale(category = &quot;LC_ALL&quot;, locale = &quot;chs&quot;) To process Chinese data with R, for Windows users, please set the locale to the Chinese language. "],["chinese-word-segmentation.html", "Chinese Word Segmentation Chinese Word Segmenter jiebaR Chinese Text Analytics Pipeline", " Chinese Word Segmentation library(tidyverse) library(quanteda) library(quanteda.textplots) library(quanteda.textstats) library(readtext) library(tidytext) library(jiebaR) library(wordcloud2) ## Windows-only ## Uncomment to set locale # Sys.setlocale(category = &quot;LC_ALL&quot;, locale = &quot;cht&quot;) Before we perform any analysis on the Chinese data, we need to fix the issue of word tokenization. Unlike English, where each word token is delimited by white spaces, Chinese word tokens are much less straightforward. A word, however, is an important semantic unit in many linguistic analysis. In this workshop, I will introduce a frequently used library, jiebaR, for Chinese word segmentation. Chinese Word Segmenter jiebaR A Quick Example Now let’s take a look at a quick example. Let’s assume that in our corpus, we have collected only two documents, each consisting of only one short paragraph. ## Simple corpus data sents &lt;- c(&quot;綠黨桃園市議員王浩宇爆料，指民眾黨不分區被提名人蔡壁如、黃瀞瑩，在昨（6）日才請辭是為領年終獎金。台灣民眾黨主席、台北市長柯文哲7日受訪時則說，都是按流程走，不要把人家想得這麼壞。&quot;, &quot;新北消防救護人員說，119在疫情的期間，除肩負協助確診患者送醫的任務，仍需負擔每天超過500件以上的緊急救護案件，疫情期間稍一不慎，極可能造成到院前，緊急救護能量和醫療體系的崩壞；目前確診案件不斷的增加，國家面臨疫情是在清零與共存政策間的戰略上，指引的調整勢必勢在必行。&quot;) To use jiebaR, there are two important steps: Initialize a jiebar object using worker() Tokenize the texts into words using the function segment() with the initialized jiebaR object created earlier For example, in the following code chunk, we first initialize a jiebaR object, i.e., seg1, using worker() and use this as our segmentation model to tokenize our texts into words with segment(sents, jiebar = seg1). ## quick example of word-seg seg1 &lt;- worker() segment(sents, jiebar = seg1) [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指民眾&quot; [7] &quot;黨&quot; &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; [13] &quot;蔡壁如&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; &quot;在昨&quot; &quot;6&quot; [19] &quot;日&quot; &quot;才&quot; &quot;請辭&quot; &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; [25] &quot;台灣民眾&quot; &quot;黨&quot; &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文&quot; [31] &quot;哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; [37] &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;不要&quot; [43] &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; &quot;新北&quot; [49] &quot;消防&quot; &quot;救護&quot; &quot;人員&quot; &quot;說&quot; &quot;119&quot; &quot;在&quot; [55] &quot;疫情&quot; &quot;的&quot; &quot;期間&quot; &quot;除&quot; &quot;肩負&quot; &quot;協助&quot; [61] &quot;確診&quot; &quot;患者&quot; &quot;送醫&quot; &quot;的&quot; &quot;任務&quot; &quot;仍&quot; [67] &quot;需&quot; &quot;負擔&quot; &quot;每天&quot; &quot;超過&quot; &quot;500&quot; &quot;件&quot; [73] &quot;以上&quot; &quot;的&quot; &quot;緊急&quot; &quot;救護&quot; &quot;案件&quot; &quot;疫情&quot; [79] &quot;期間&quot; &quot;稍&quot; &quot;一&quot; &quot;不慎&quot; &quot;極&quot; &quot;可能&quot; [85] &quot;造成&quot; &quot;到&quot; &quot;院前&quot; &quot;緊急&quot; &quot;救護&quot; &quot;能量&quot; [91] &quot;和&quot; &quot;醫療&quot; &quot;體系&quot; &quot;的&quot; &quot;崩壞&quot; &quot;目前&quot; [97] &quot;確診&quot; &quot;案件&quot; &quot;不斷&quot; &quot;的&quot; &quot;增加&quot; &quot;國家&quot; [103] &quot;面臨&quot; &quot;疫情&quot; &quot;是&quot; &quot;在&quot; &quot;清零&quot; &quot;與&quot; [109] &quot;共存&quot; &quot;政策&quot; &quot;間&quot; &quot;的&quot; &quot;戰略&quot; &quot;上&quot; [115] &quot;指引&quot; &quot;的&quot; &quot;調整&quot; &quot;勢必&quot; &quot;勢在必行&quot; Parameters Setting of jiebaR There are many different parameters we can specify when we initialize the jiebar object (See the documentation in ?work()). Important parameters include: type = ...: The type of the segmenter’s model user = ...: A path to the user-defined dictionary stop_word = ...: A path to the stopword list symbol = FALSE: Whether to return symbols (the default is FALSE, i.e., jiebaR removes tokens symbols by default) bylines = FALSE: Whether to return a list or not (crucial if you are using tidytext::unnest_tokens()) User-defined dictionary From the above example, it is clear to see that some of the words have not been correctly identified by jiebaR: for example, 民眾黨, 不分區, 黃瀞瑩, 柯文哲. It is always recommended to include a user-defined dictionary when tokenizing your texts because different corpora may have their own unique vocabulary (i.e., domain-specific lexicon). So, we can pack all new words in one text file: ## check user-dictionary file readLines(&quot;demo_data/dict-ch-user-demo.txt&quot;, encoding = &quot;UTF-8&quot;) [1] &quot;民眾黨&quot; &quot;不分區&quot; &quot;黃瀞瑩&quot; &quot;柯文哲&quot; And include this user-defined dictionary path when initializing the jiebaR: ## User-defined dictionary seg2 &lt;- worker(user = &quot;demo_data/dict-ch-user-demo.txt&quot;) segment(sents, seg2) [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指&quot; [7] &quot;民眾黨&quot; &quot;不分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; &quot;蔡壁如&quot; [13] &quot;黃瀞瑩&quot; &quot;在昨&quot; &quot;6&quot; &quot;日&quot; &quot;才&quot; &quot;請辭&quot; [19] &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; &quot;台灣&quot; &quot;民眾黨&quot; &quot;主席&quot; [25] &quot;台北&quot; &quot;市長&quot; &quot;柯文哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; [31] &quot;時則&quot; &quot;說&quot; &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; [37] &quot;走&quot; &quot;不要&quot; &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; [43] &quot;壞&quot; &quot;新北&quot; &quot;消防&quot; &quot;救護&quot; &quot;人員&quot; &quot;說&quot; [49] &quot;119&quot; &quot;在&quot; &quot;疫情&quot; &quot;的&quot; &quot;期間&quot; &quot;除&quot; [55] &quot;肩負&quot; &quot;協助&quot; &quot;確診&quot; &quot;患者&quot; &quot;送醫&quot; &quot;的&quot; [61] &quot;任務&quot; &quot;仍&quot; &quot;需&quot; &quot;負擔&quot; &quot;每天&quot; &quot;超過&quot; [67] &quot;500&quot; &quot;件&quot; &quot;以上&quot; &quot;的&quot; &quot;緊急&quot; &quot;救護&quot; [73] &quot;案件&quot; &quot;疫情&quot; &quot;期間&quot; &quot;稍&quot; &quot;一&quot; &quot;不慎&quot; [79] &quot;極&quot; &quot;可能&quot; &quot;造成&quot; &quot;到&quot; &quot;院前&quot; &quot;緊急&quot; [85] &quot;救護&quot; &quot;能量&quot; &quot;和&quot; &quot;醫療&quot; &quot;體系&quot; &quot;的&quot; [91] &quot;崩壞&quot; &quot;目前&quot; &quot;確診&quot; &quot;案件&quot; &quot;不斷&quot; &quot;的&quot; [97] &quot;增加&quot; &quot;國家&quot; &quot;面臨&quot; &quot;疫情&quot; &quot;是&quot; &quot;在&quot; [103] &quot;清零&quot; &quot;與&quot; &quot;共存&quot; &quot;政策&quot; &quot;間&quot; &quot;的&quot; [109] &quot;戰略&quot; &quot;上&quot; &quot;指引&quot; &quot;的&quot; &quot;調整&quot; &quot;勢必&quot; [115] &quot;勢在必行&quot; The format of the user-defined dictionary is a text file, with one word per line. Also, the default encoding of the dictionary is UTF-8. Please note that in Windows, the default encoding of a Chinese txt file created by Notepad may not be UTF-8. (Usually, it is encoded in big-5). Also, files created by MS Office applications tend to be less transparent in terms of their encoding. Stopwords When we initialize the worker(), we can also specify a stopword list, i.e., words that we do not need to include in the later analyses. For example, in text mining, functional words are usually less informative, thus often excluded in data processing. ## check stopword demo file readLines(&quot;demo_data/stopwords-ch-demo.txt&quot;, encoding = &quot;UTF-8&quot;) [1] &quot;日&quot; &quot;都&quot; &quot;是&quot; &quot;的&quot; ## Stopwords seg3 &lt;- worker(user = &quot;demo_data/dict-ch-user-demo.txt&quot;, stop_word = &quot;demo_data/stopwords-ch-demo.txt&quot;) segment(sents, seg3) [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指&quot; [7] &quot;民眾黨&quot; &quot;不分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; &quot;蔡壁如&quot; [13] &quot;黃瀞瑩&quot; &quot;在昨&quot; &quot;6&quot; &quot;才&quot; &quot;請辭&quot; &quot;為領&quot; [19] &quot;年終獎金&quot; &quot;台灣&quot; &quot;民眾黨&quot; &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; [25] &quot;柯文哲&quot; &quot;7&quot; &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; &quot;按&quot; [31] &quot;流程&quot; &quot;走&quot; &quot;不要&quot; &quot;把&quot; &quot;人家&quot; &quot;想得&quot; [37] &quot;這麼&quot; &quot;壞&quot; &quot;新北&quot; &quot;消防&quot; &quot;救護&quot; &quot;人員&quot; [43] &quot;說&quot; &quot;119&quot; &quot;在&quot; &quot;疫情&quot; &quot;期間&quot; &quot;除&quot; [49] &quot;肩負&quot; &quot;協助&quot; &quot;確診&quot; &quot;患者&quot; &quot;送醫&quot; &quot;任務&quot; [55] &quot;仍&quot; &quot;需&quot; &quot;負擔&quot; &quot;每天&quot; &quot;超過&quot; &quot;500&quot; [61] &quot;件&quot; &quot;以上&quot; &quot;緊急&quot; &quot;救護&quot; &quot;案件&quot; &quot;疫情&quot; [67] &quot;期間&quot; &quot;稍&quot; &quot;一&quot; &quot;不慎&quot; &quot;極&quot; &quot;可能&quot; [73] &quot;造成&quot; &quot;到&quot; &quot;院前&quot; &quot;緊急&quot; &quot;救護&quot; &quot;能量&quot; [79] &quot;和&quot; &quot;醫療&quot; &quot;體系&quot; &quot;崩壞&quot; &quot;目前&quot; &quot;確診&quot; [85] &quot;案件&quot; &quot;不斷&quot; &quot;增加&quot; &quot;國家&quot; &quot;面臨&quot; &quot;疫情&quot; [91] &quot;在&quot; &quot;清零&quot; &quot;與&quot; &quot;共存&quot; &quot;政策&quot; &quot;間&quot; [97] &quot;戰略&quot; &quot;上&quot; &quot;指引&quot; &quot;調整&quot; &quot;勢必&quot; &quot;勢在必行&quot; POS Tagging In addition to word tokenization, jiebaR also supports parts-of-speech (POS) tagging. That is, for each word, jiebaR can also provide its part-of-speech tag. We can get the POS annotations by specifying type = \"tag\" when we initialize the worker(). The returned object is a named character vector, i.e., the POS tags of the words are included in the names of the vectors. ## POS Tagging seg4 &lt;- worker(type = &quot;tag&quot;, user = &quot;demo_data/dict-ch-user-demo.txt&quot;, stop_word = &quot;demo_data/stopwords-ch-demo.txt&quot;) segment(sents, seg4) n ns n x n n x &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指&quot; &quot;民眾黨&quot; x p v n x x x &quot;不分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; &quot;蔡壁如&quot; &quot;黃瀞瑩&quot; &quot;在昨&quot; x d v x n x x &quot;6&quot; &quot;才&quot; &quot;請辭&quot; &quot;為領&quot; &quot;年終獎金&quot; &quot;台灣&quot; &quot;民眾黨&quot; n ns n x x v x &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文哲&quot; &quot;7&quot; &quot;受訪&quot; &quot;時則&quot; zg p n v df p n &quot;說&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;不要&quot; &quot;把&quot; &quot;人家&quot; x r a x n v n &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; &quot;新北&quot; &quot;消防&quot; &quot;救護&quot; &quot;人員&quot; zg m p n f p n &quot;說&quot; &quot;119&quot; &quot;在&quot; &quot;疫情&quot; &quot;期間&quot; &quot;除&quot; &quot;肩負&quot; v v n v n zg v &quot;協助&quot; &quot;確診&quot; &quot;患者&quot; &quot;送醫&quot; &quot;任務&quot; &quot;仍&quot; &quot;需&quot; v r v m zg f a &quot;負擔&quot; &quot;每天&quot; &quot;超過&quot; &quot;500&quot; &quot;件&quot; &quot;以上&quot; &quot;緊急&quot; v n n f zg m a &quot;救護&quot; &quot;案件&quot; &quot;疫情&quot; &quot;期間&quot; &quot;稍&quot; &quot;一&quot; &quot;不慎&quot; d v v v s a v &quot;極&quot; &quot;可能&quot; &quot;造成&quot; &quot;到&quot; &quot;院前&quot; &quot;緊急&quot; &quot;救護&quot; n c n n v t v &quot;能量&quot; &quot;和&quot; &quot;醫療&quot; &quot;體系&quot; &quot;崩壞&quot; &quot;目前&quot; &quot;確診&quot; n d v n v n p &quot;案件&quot; &quot;不斷&quot; &quot;增加&quot; &quot;國家&quot; &quot;面臨&quot; &quot;疫情&quot; &quot;在&quot; z zg v n f n f &quot;清零&quot; &quot;與&quot; &quot;共存&quot; &quot;政策&quot; &quot;間&quot; &quot;戰略&quot; &quot;上&quot; v vn d i &quot;指引&quot; &quot;調整&quot; &quot;勢必&quot; &quot;勢在必行&quot; Every POS tagger has its own predefined tag set. The following table lists the annotations of the POS tag set used in jiebaR: Chinese Text Analytics Pipeline Below is a quick overview of the Chinese Text Analytics Flowchart (Figure 1). In this workshop, I will introduce two methodological frameworks to process Chinese corpus data: Tidytext Framework Quanteda Framework As these two packages are more optimized for the English language, I will highlight a few important steps that require more attention with respect to Chinese word segmentation. Figure 1: Chinese Text Analytics Flowchart Quanteda Framework To perform word tokenization using jiebaR under the Quanteda framework, we need to create the tokens object using the segment() from jiebaR. The steps are very straightforward. We utilize jiebaR for word tokenization like before: Initialize a jiebaR model; Use it to tokenize the corpus text with segment(); Convert the object into a tokens with as.tokens(). The key is that we need to convert the output of segment() from a list to a tokens using as.tokens(). # initialize segmenter my_seg &lt;- worker(user = &quot;demo_data/dict-ch-user-demo.txt&quot;, symbol= TRUE, bylines = TRUE) ## create tokens based on self-defined segmentation text_tokens &lt;- as.tokens(segment(sents, my_seg)) With this tokens object, we can apply kiwc() or other Quanteda-supported processing to the corpus data. ## keyword-in-context kwic(text_tokens, pattern = &quot;柯文哲&quot;) kwic(text_tokens, pattern = &quot;.*?[台市].*?&quot;, valuetype = &quot;regex&quot;) In Quanteda, there is a quanteda-native Chinese tokenizer, tokens(). However, its performance is very limited and it does not support user-defined dictionary. Therefore, I would suggest using one’s own self-defined tokenizer for Chinese word segmentation. ## Examples of using quanteda-native ## Word segmentation ## create corpus object text_corpus &lt;- corpus(sents) ## summary summary(text_corpus) ## Create tokens object text_tokens &lt;- tokens(text_corpus) ## Check quanteda-native word tokenization result text_tokens[[1]] [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王&quot; &quot;浩&quot; &quot;宇&quot; [7] &quot;爆&quot; &quot;料&quot; &quot;，&quot; &quot;指&quot; &quot;民眾&quot; &quot;黨&quot; [13] &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; &quot;蔡&quot; [19] &quot;壁&quot; &quot;如&quot; &quot;、&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; [25] &quot;，&quot; &quot;在&quot; &quot;昨&quot; &quot;（&quot; &quot;6&quot; &quot;）&quot; [31] &quot;日&quot; &quot;才&quot; &quot;請辭&quot; &quot;是&quot; &quot;為&quot; &quot;領&quot; [37] &quot;年終獎金&quot; &quot;。&quot; &quot;台灣&quot; &quot;民眾&quot; &quot;黨主席&quot; &quot;、&quot; [43] &quot;台北市&quot; &quot;長&quot; &quot;柯&quot; &quot;文&quot; &quot;哲&quot; &quot;7&quot; [49] &quot;日&quot; &quot;受&quot; &quot;訪&quot; &quot;時&quot; &quot;則&quot; &quot;說&quot; [55] &quot;，&quot; &quot;都是&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;，&quot; [61] &quot;不要&quot; &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; [67] &quot;。&quot; ## KWIC kwic(text_tokens, pattern = &quot;柯文哲&quot;) kwic(text_tokens, pattern = &quot;柯&quot;) From the above example, it is clear to see that quite a few word tokens have not been successfully identified by the Quanteda-native word segmentation (e.g., proper names like 柯文哲, 王浩宇). This would also have great impact on the effectiveness of kwic() as well. Therefore analysis based on the Quanteda-native segmentation can be very limited. Tidytext Framework Now let’s turn to the Titytext framework of text processing. The tidytext package is made for the handling of the tidy text format of the corpus data, i.e., to process textual data on the basis of data frames. With a data frame format of the text data, we can manipulate the text data with a standard set of tidy tools and packages in R, including dplyr, tidyr, and ggplot2. In the tidytext format, the tokenization is taken care of by unnest_tokens(). # Create a text-based data frame corp_df &lt;- data.frame(text_id = c(1,2), text = sents) corp_df Then, we use unnest_tokens() to tokenize the text-based data frame (corp_df) into a word-based data frame (corp_df_word). Texts included in the text column are tokenized into words, which are unnested into independent rows in the word column of the new data frame. # tokenization corp_df_word &lt;- corp_df %&gt;% unnest_tokens( word, ## name for new tokens to be unnested text, ## name for original larger units token = function(x) ## self-defined tokenization method segment(x, jiebar = my_seg) ) corp_df_word It can be seen that for the token parameter in unnest_tokens(), we use an anonymous function based on jiebaR and segment() for self-defined Chinese word segmentation. This is called anonymous functions because it has not been assigned to any object name in the current R session. You may check R language documentation for more detail on Writing Functions. "],["applications.html", "Applications Environment Loading Data Overview of the Corpus Data Preprocessing (Cleaning) Initialize jiebaR Tidytext Framework Quanteda Framework Conclusion", " Applications Environment ## clear up memory rm(list = ls(all=TRUE)) ## loading necessary libraries library(tidyverse) library(quanteda) library(quanteda.textplots) library(quanteda.textstats) library(readtext) library(tidytext) library(jiebaR) library(wordcloud2) ## Windows-only ## Uncomment to set locale # Sys.setlocale(category = &quot;LC_ALL&quot;, locale = &quot;cht&quot;) ## OS-specific fixing ## For Windows User #Sys.setlocale(category = &quot;LC_ALL&quot;, locale = &quot;cht&quot;) ## Fix Chinese fonts in visualization ## Please replace your system-supported Chinese font library(showtext) #font_add(&quot;Arial Unicode MS&quot;, &quot;Arial Unicode.ttf&quot;) ## &lt;- replace your font showtext_auto(enable = TRUE) In this section, we will look at a few more examples of Chinese text processing based on the data set demo_data/song-jay-amei-v1.csv. It is a text collection of songs by Jay Chou and Amei. Loading Data We use the CSV version for illustration: ## loading CSV corp_df_text &lt;- read_csv(file = &quot;demo_data/song-jay-amei-v1.csv&quot;, locale = locale(encoding = &quot;UTF-8&quot;)) ## creating doc_id corp_df_text &lt;- corp_df_text %&gt;% mutate(doc_id = row_number()) corp_df_text Overview of the Corpus The data set demo_data/song-jay-amei-v1.csv is a collection of songs by two artists, Jay Chou and Amei Chang. A quick frequency counts of the songs by artists in the data set: ## Data set over view str(corp_df_text) spec_tbl_df [473 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame) $ artist : chr [1:473] &quot;張惠妹&quot; &quot;張惠妹&quot; &quot;張惠妹&quot; &quot;張惠妹&quot; ... $ title : chr [1:473] &quot;雙影&quot; &quot;Are You Ready&quot; &quot;偷故事的人&quot; &quot;壞的好人&quot; ... $ lyricist: chr [1:473] &quot;作詞：易家揚&quot; &quot;作詞：張雨生&quot; &quot;作詞：艾怡良&quot; &quot;作詞：小寒&quot; ... $ composer: chr [1:473] &quot;作曲：丁薇‭&quot; &quot;作曲：張雨生&quot; &quot;作曲：艾怡良&quot; &quot;作曲：黃晟峰&quot; ... $ lyric : chr [1:473] &quot;若非狠下心拿什麼想妳\\n想成了風雨‭ 對不起\\n保護一顆心看多了烏雲\\n兩忘曾經裡 不怨妳\\n‬\\n心中有心語‭ ‬‬妳我是雙影‭\\n‬&quot;| __truncated__ &quot;Tell me baby are you ready天色已經黑\\nTell me baby are you crazy用力說聲嘿\\nTell me baby do you want it搖醒這世&quot;| __truncated__ &quot;當你走的那天 我不再說故事了\\n不再編織起承轉合 也不用為結局苦惱了\\n當你走的那天 你不再說故事了\\n不再安排我的對白&quot;| __truncated__ &quot;紅燈黃昏狂奔 不顧輿論\\n絶不能 讓心愛的他受冷\\n不問緣分名稱 怕會扯分\\n被一層 皮膚隔開的靈魂\\n我的青春 不否認\\n有&quot;| __truncated__ ... $ gender : chr [1:473] &quot;female&quot; &quot;female&quot; &quot;female&quot; &quot;female&quot; ... $ doc_id : int [1:473] 1 2 3 4 5 6 7 8 9 10 ... - attr(*, &quot;spec&quot;)= .. cols( .. artist = col_character(), .. title = col_character(), .. lyricist = col_character(), .. composer = col_character(), .. lyric = col_character(), .. gender = col_character() .. ) - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; ## Bar plot corp_df_text %&gt;% ggplot(aes(artist, fill=artist)) + geom_bar() Data Preprocessing (Cleaning) Raw texts usually include a lot of noise. For example, texts may include control characters, redundant white-spaces, and duplicate line breaks. These redundant characters may have an impact on the word segmentation performance. It is often suggested to clean up the raw texts before further analysis. ## Define a function normalize_document &lt;- function(texts) { texts %&gt;% str_replace_all(&quot;\\\\p{C}&quot;, &quot; &quot;) %&gt;% ## remove control chars str_replace_all(&quot;\\\\s+&quot;, &quot;\\n&quot;) ## replace whitespaces with linebreak } ## Example corp_df_text$lyric[1] [1] &quot;若非狠下心拿什麼想妳\\n想成了風雨‭ 對不起\\n保護一顆心看多了烏雲\\n兩忘曾經裡 不怨妳\\n‬\\n心中有心語‭ ‬‬妳我是雙影‭\\n‬一半無情‭ 另一半深情\\n‬貪一點愛什麼痛也允許‭\\n‬用懷疑交換‭ ‬秘密\\n寵愛和被忘‭ 在心中交談‭(說來迷惘)\\n妳作證我的冷暖‭ 悲歡(夢短路長)\\n妳拉我的手繡一件孤單(絲綢堆了月光)\\n說用來取暖 誰敢\\n命在誰命裡‭ ‬愛恨是雙影\\n一端美麗‭ 另一端無語\\n遠遠走來沒有字的未來\\n被時間教會‭ 也許\\n情斷留情意‭ 忘記是雙影\\n一天冷淡另一天想起\\n但願我們永遠走在光裡\\n這一生如此‭ 多雲\\n這一生從此‭ 無雲\\n感謝\\n好青年\\n提供歌詞&quot; ## Cleaned normalize_document(corp_df_text$lyric[1]) [1] &quot;若非狠下心拿什麼想妳\\n想成了風雨\\n對不起\\n保護一顆心看多了烏雲\\n兩忘曾經裡\\n不怨妳\\n心中有心語\\n妳我是雙影\\n一半無情\\n另一半深情\\n貪一點愛什麼痛也允許\\n用懷疑交換\\n秘密\\n寵愛和被忘\\n在心中交談\\n(說來迷惘)\\n妳作證我的冷暖\\n悲歡(夢短路長)\\n妳拉我的手繡一件孤單(絲綢堆了月光)\\n說用來取暖\\n誰敢\\n命在誰命裡\\n愛恨是雙影\\n一端美麗\\n另一端無語\\n遠遠走來沒有字的未來\\n被時間教會\\n也許\\n情斷留情意\\n忘記是雙影\\n一天冷淡另一天想起\\n但願我們永遠走在光裡\\n這一生如此\\n多雲\\n這一生從此\\n無雲\\n感謝\\n好青年\\n提供歌詞&quot; ## Apply cleaning to every document corp_df_text$lyric &lt;- normalize_document(corp_df_text$lyric) Initialize jiebaR Because we use jiebaR for word tokenization, we first need to initialize the jiebaR models. Here we create two jiebaR models, one for word tokenization only and the other for parts-of-speech tagging. ## initialize jiebaR models ## for word segmentation only my_seg &lt;- worker(bylines = TRUE, symbol = TRUE) ## for POS tagging my_seg_pos &lt;- worker( type = &quot;tag&quot;, bylines = FALSE, symbol = TRUE ) We can specify the path to the external user-defined dictionary in worker(..., user = \"\"). Alternatively, we can also add add-hoc new words to the jiebaR model. This can be very helpful when we spot any weird segmentation results in the output. By default, new_user_word() assigns each new word with a default n tag. #Add customized terms temp_new_words &lt;-c(&quot;夢短路長&quot;) new_user_word(my_seg, temp_new_words) [1] TRUE new_user_word(my_seg_pos, temp_new_words) [1] TRUE Tidytext Framework The following examples demonstrate how to process the Chinese texts under the tidytext framework. Recall the three important steps: Load the corpus data using readtext() and create a text-based data frame of the corpus; Initialize a jiebaR object using worker() Tokenize the text-based data frame into a word-based data frame using unnest_tokens(); ## Word Tokenization corp_df_word &lt;- corp_df_text %&gt;% unnest_tokens( output = word, ## new unit name input = lyric, ## old unit name token = function (x) ## tokenization method segment(x, jiebar = my_seg) ) %&gt;% group_by(doc_id) %&gt;% mutate(word_id = row_number()) %&gt;% ungroup() ## Check corp_df_word Creating unique indices for your data is very important. In corpus linguistic analysis, we often need to keep track of the original context of the word, phrase or sentence in the concordances. All these unique indices (as well as the source text filenames) would make things a lot easier. Also, if the metadata of the source documents are available, these unique indices would allow us to connect the tokenized linguistic units to the metadata information (e.g., genres, registers, author profiles) for more interesting analysis. Therefore, after tokenization, we have obtained a word-based data frame of our corpus data. Case Study: Word Frequency and Wordcloud With a word-based data frame, we can easily create a word frequency list as well as a word cloud to have a quick overview of the word distribution of the corpus. It should be noted that before creating the word frequency list, we often need to consider whether to remove unimportant word tokens. These unimportant tokens usually include: stopwords symbols and punctuation marks digits alphabets (e.g., English) ## load Chinese stopwords stopwords_chi &lt;- readLines(&quot;demo_data/stopwords-ch-jiebar-zht.txt&quot;, encoding = &quot;UTF-8&quot;) ## create word freq list corp_word_freq &lt;- corp_df_word %&gt;% filter(!word %in% stopwords_chi) %&gt;% # remove stopwords filter(word %&gt;% str_detect(pattern = &quot;[\\u4E00-\\u9FFF]+&quot;)) %&gt;% # including words consisting only of these Chinese chars count(word) %&gt;% arrange(desc(n)) ## Check top 50 words head(corp_word_freq, 50) ## Create Word Cloud library(wordcloud2) corp_word_freq %&gt;% filter(n &gt; 20) %&gt;% ## select words whose freq &gt; 20 filter(nchar(word) &gt;= 2) %&gt;% ## remove monosyllabic tokens wordcloud2(shape = &quot;pentagon&quot;, size = 0.5) We can represent any character in Unicode in the form of \\uXXXX, where the XXXX refers to the coding numbers of the character in Unicode (UTF-8) in hexadecimal format. In the above regular expression, the Unicode range [\\u4E00-\\u9FFF] refers to a set of frequently used Chinese characters. Therefore, the way we remove unimportant word tokens is to identify all word tokens consisting of these frequently used Chinese characters that fall within this Unicode range. For more information related to the Unicode range for the punctuation marks in CJK languages, please see this SO discussion thread. Case Study: Patterns In this case study, we are looking at a more complex example. In corpus linguistic analysis, we often need to extract a particular pattern from the texts. In order to retrieve the target patterns at a high accuracy rate, we often need to make use of NLP tools to enrich the raw texts with more annotations. The most often-used information is the parts-of-speech tags of words. In this example, we will demonstrate how to enrich our corpus data by adding POS tags to our current tidy corpus design. Our steps are as follows: Initialize jiebaR object, which performs not only word segmentation but also POS tagging; Create a self-defined function to word-seg and pos-tag each text and combine all tokens, word/tag, into a long string for each text; With the text-based data frame corp_df_text, create a new column, which includes the enriched version of each text, using mutate() ## Define a function to word-seg and pos-tag a text tag_text &lt;- function(x, jiebar) { segment(x, jiebar) %&gt;% ## tokenize + POS-tagging paste(names(.), sep = &quot;/&quot;, collapse = &quot; &quot;) ## reformat output } A quick example of the function’s usage: ## demo of the function `tag_text()` tag_text(corp_df_text$lyric[1], my_seg_pos) [1] &quot;若非/c 狠下/d 心/n 拿/v 什麼/r 想/v 妳/zg \\n/x 想成/v 了/ul 風雨/n \\n/x 對不起/l \\n/x 保護/v 一顆/m 心看/x 多/m 了/ul 烏雲/nr \\n/x 兩忘/x 曾經/d 裡/zg \\n/x 不怨/v 妳/zg \\n/x 心中/s 有/v 心語/n \\n/x 妳/zg 我/r 是/v 雙影/n \\n/x 一半/m 無情/n \\n/x 另一半/d 深情/n \\n/x 貪/v 一點/m 愛/zg 什麼/r 痛/a 也/d 允許/v \\n/x 用/p 懷疑/v 交換/v \\n/x 秘密/n \\n/x 寵愛/v 和/c 被/p 忘/v \\n/x 在/p 心中/s 交談/v \\n/x (/x 說來/v 迷惘/a )/x \\n/x 妳/zg 作證/v 我/r 的/uj 冷暖/an \\n/x 悲歡/v (/x 夢短路長/n )/x \\n/x 妳/zg 拉/v 我/r 的/uj 手繡/n 一件/m 孤單/a (/x 絲綢/n 堆/v 了/ul 月光/n )/x \\n/x 說/zg 用來/v 取暖/v \\n/x 誰/zg 敢/v \\n/x 命/n 在/p 誰/zg 命裡/x \\n/x 愛恨/a 是/v 雙影/n \\n/x 一端/m 美麗/ns \\n/x 另一端/i 無語/nz \\n/x 遠遠/d 走來/v 沒有/v 字/n 的/uj 未來/t \\n/x 被/p 時間/n 教會/n \\n/x 也許/d \\n/x 情斷/x 留情/v 意/n \\n/x 忘記/v 是/v 雙影/n \\n/x 一天/m 冷淡/a 另/r 一天/m 想起/v \\n/x 但願/v 我們/r 永遠/d 走/v 在/p 光裡/x \\n/x 這/zg 一生/m 如此/c \\n/x 多雲/nr \\n/x 這/zg 一生/m 從此/c \\n/x 無/v 雲/ns \\n/x 感謝/v \\n/x 好/a 青年/t \\n/x 提供/v 歌詞/n&quot; ## Create a new column ## by applying `tag_text()` function to each text corp_df_text &lt;- corp_df_text %&gt;% mutate(lyric_tag = map_chr(lyric, tag_text, my_seg_pos)) corp_df_text Now we have obtained an enriched version of all the texts, we can make use of the POS tags for more linguistic analyses. For example, we can examine the use of adjectives in lyrics. The data retrieval procedure is now very straightforward: we only need to create a regular expression that matches our interested pattern and go through the enriched version of the texts (i.e., lyric_tag column in corp_df_text) to extract these matches with unnest_tokens(). 1.Define a regular expression [^/\\\\s]+/a\\\\b for adjectives; 2.Use unnest_tokens() and str_extract_all() to extract target patterns and create a pattern-based data frame. ## define regex patterns pat &lt;- &quot;[^/\\\\s]+/a\\\\b&quot; ## extract patterns from corp corp_df_pat &lt;- corp_df_text %&gt;% unnest_tokens( output = pat, ## name for the new unit to be unnested input = lyric_tag, ## name for the old unit token = function(x) ## unnesting method str_extract_all(x, pattern = pat) ) %&gt;% select(doc_id, pat, artist, title) corp_df_pat Then we can further explore how different artists use the adjectives differently. We can identify the most frequently used top-20 adjectives for each artist and visualize the results in bar plots: ## Identify top 20 adjectives corp_df_pat_top20 &lt;- corp_df_pat %&gt;% mutate(word = str_replace_all(pat, &quot;/.+$&quot;,&quot;&quot;)) %&gt;% ## remove POS tags group_by(artist) %&gt;% ## split DF by artist count(word, sort = T) %&gt;% ## create pat freq list for each artist top_n(20, n) %&gt;% ## select top 20 for each artist ungroup %&gt;% ## merge DF again arrange(artist, -n) ## sort result ## Check corp_df_pat_top20 ## Data Visualization: Bar plot corp_df_pat_top20 %&gt;% mutate(word = reorder_within(word, n, artist)) %&gt;% ggplot(aes(word, n, fill=artist)) + geom_bar(stat=&quot;identity&quot;)+ coord_flip()+ facet_wrap(~artist,scales = &quot;free_y&quot;) + scale_x_reordered() + labs(x = &quot;Adjectives&quot;, y = &quot;Frequency&quot;, title=&quot;Top 20 Adjectives of Each Artist&#39;s Songs&quot;) Case Study: Lexical Bundles N-grams Extraction With word boundaries, we can also analyze the recurrent multiword units in the corpus. In this example, let’s take a look at the recurrent four-word sequences (i.e., four-grams) in our corpus. We define our own Chinese ngram tokenizer for unnest_tokens(). The Chinese ngram tokenization function should: Tokenize each raw text into word tokens; Create a set of ngrams from the word tokens of each text ## self defined ngram tokenizer tokenizer_ngrams &lt;- function(texts, jiebar, n = 2 , skip = 0, delimiter = &quot;_&quot;) { texts %&gt;% ## given a vector of texts segment(jiebar) %&gt;% ## word tokenization as.tokens %&gt;% ## list to tokens tokens_ngrams(n, skip, concatenator = delimiter) %&gt;% ## ngram tokenization as.list ## tokens to list } In the above self-defined ngram tokenizer, we make use of tokens_ngrams() in quanteda, which creates a set of ngrams from already tokenized text objects, i.e., tokens. Because this function requires a tokens object as the input, we need to do the class conversion via as.tokens() and as.list(). Take a look at the following examples for a quick overview of tokens_ngrams(): ## Examples of quanteda&#39;s tokens_grams() sents &lt;- c(&quot;Jack and Jill went up the hill to fetch a pail of water&quot;, &quot;Jack fell down and broke his crown and Jill came tumbling after&quot;) sents_tokens &lt;- tokens(sents) ## English tokenization tokens_ngrams(sents_tokens, n = 2, skip = 0) Tokens consisting of 2 documents. text1 : [1] &quot;Jack_and&quot; &quot;and_Jill&quot; &quot;Jill_went&quot; &quot;went_up&quot; &quot;up_the&quot; &quot;the_hill&quot; [7] &quot;hill_to&quot; &quot;to_fetch&quot; &quot;fetch_a&quot; &quot;a_pail&quot; &quot;pail_of&quot; &quot;of_water&quot; text2 : [1] &quot;Jack_fell&quot; &quot;fell_down&quot; &quot;down_and&quot; &quot;and_broke&quot; [5] &quot;broke_his&quot; &quot;his_crown&quot; &quot;crown_and&quot; &quot;and_Jill&quot; [9] &quot;Jill_came&quot; &quot;came_tumbling&quot; &quot;tumbling_after&quot; tokens_ngrams(sents_tokens, n = 2, skip = 1) Tokens consisting of 2 documents. text1 : [1] &quot;Jack_Jill&quot; &quot;and_went&quot; &quot;Jill_up&quot; &quot;went_the&quot; &quot;up_hill&quot; [6] &quot;the_to&quot; &quot;hill_fetch&quot; &quot;to_a&quot; &quot;fetch_pail&quot; &quot;a_of&quot; [11] &quot;pail_water&quot; text2 : [1] &quot;Jack_down&quot; &quot;fell_and&quot; &quot;down_broke&quot; &quot;and_his&quot; [5] &quot;broke_crown&quot; &quot;his_and&quot; &quot;crown_Jill&quot; &quot;and_came&quot; [9] &quot;Jill_tumbling&quot; &quot;came_after&quot; A quick example of how to use the self-defined function tokenizer_ngrams(): # examples texts &lt;- c(&quot;這是一個測試的句子&quot;, &quot;這句子&quot;, &quot;超短句&quot;, &quot;最後一個超長的句子測試&quot;) tokenizer_ngrams( texts = texts, jiebar = my_seg, n = 2, skip = 0, delimiter = &quot;_&quot; ) $text1 [1] &quot;這是_一個&quot; &quot;一個_測試&quot; &quot;測試_的&quot; &quot;的_句子&quot; $text2 [1] &quot;這_句子&quot; $text3 [1] &quot;超短_句&quot; $text4 [1] &quot;最後_一個&quot; &quot;一個_超長&quot; &quot;超長_的&quot; &quot;的_句子&quot; &quot;句子_測試&quot; tokenizer_ngrams( texts = texts, jiebar = my_seg, n = 2, skip = 1, delimiter = &quot;_&quot; ) $text1 [1] &quot;這是_測試&quot; &quot;一個_的&quot; &quot;測試_句子&quot; $text2 character(0) $text3 character(0) $text4 [1] &quot;最後_超長&quot; &quot;一個_的&quot; &quot;超長_句子&quot; &quot;的_測試&quot; tokenizer_ngrams( texts = texts, jiebar = my_seg, n = 5, skip=0, delimiter = &quot;_&quot; ) $text1 [1] &quot;這是_一個_測試_的_句子&quot; $text2 character(0) $text3 character(0) $text4 [1] &quot;最後_一個_超長_的_句子&quot; &quot;一個_超長_的_句子_測試&quot; With the above self-defined Chinese ngram tokenizer, we can now perform the ngram tokenization on our corpus. The principle is the same: We transform the text-based data frame into an ngram-based data frame using unnest_tokens(...) with the self-defined tokenization function tokenizer_ngrams(); We remove empty and unwanted n-grams entries: Empty ngrams due to short texts Ngrams spanning punctuation marks, symbols, or paragraph breaks Ngrams including alphanumeric characters ## Ngram tokenization ## from text-based to ngram-based DF corp_df_ngram &lt;- corp_df_text %&gt;% unnest_tokens( ngram, ## name for the new unit to be unnested lyric, ## name for the old unit token = function(x) ## unnesting method tokenizer_ngrams( texts = x, jiebar = my_seg, n = 4, skip = 0, delimiter = &quot;_&quot; ) ) %&gt;% select(doc_id, ngram, artist, title, lyricist) ## remove unwanted ngrams corp_df_ngram_2 &lt;- corp_df_ngram %&gt;% filter(nzchar(ngram)) %&gt;% ## empty strings filter(!str_detect(ngram, &quot;[^\\u4E00-\\u9FFF_]&quot;)) ## include ngrams consisting of Chinese characters + _ ## check head(corp_df_ngram_2,50) Frequency and Dispersion Now we can compute two important distributional properties of these 4-grams (Biber, Conrad, and Cortes 2004): The token frequency of the 4-gram; The number of different texts where the 4-gram is observed (i.e., dispersion) We set cut-offs for four-grams at: dispersion &gt;= 3 (i.e., four-grams that occur in at least three different songs) ## Compute ngram&#39;s frequency and dispersion corp_ngram_dist &lt;- corp_df_ngram_2 %&gt;% group_by(ngram) %&gt;% summarize(freq = n(), dispersion = n_distinct(doc_id)) %&gt;% filter(dispersion &gt;= 3) %&gt;% ungroup ## Check DF arranged by dispersion corp_ngram_dist %&gt;% arrange(desc(dispersion)) %&gt;% head(50) We can also look at four-grams with particular lexical words: ## Check ngrams with a specific word corp_ngram_dist %&gt;% filter(str_detect(ngram, &quot;我&quot;)) %&gt;% arrange(desc(dispersion)) corp_ngram_dist %&gt;% filter(str_detect(ngram, &quot;你&quot;)) %&gt;% arrange(desc(dispersion)) corp_ngram_dist %&gt;% filter(str_detect(ngram, &quot;媽媽&quot;)) %&gt;% arrange(desc(dispersion)) Quanteda Framework Now let’s look at a few examples of processing the data with the quanteda framework. For Chinese data, the most important base unit in Quanteda is the tokens object. So first we need to create the tokens object based on the jiebaR tokenization method. ## create tokens based on self-defined segmentation corp_tokens &lt;- as.tokens(segment(corp_df_text$lyric, my_seg)) ## add document-level metadata docvars(corp_tokens) &lt;- corp_df_text[, c(&quot;artist&quot;,&quot;lyricist&quot;,&quot;composer&quot;,&quot;title&quot;,&quot;gender&quot;)] ## check 1st doc corp_tokens[[1]] [1] &quot;若非&quot; &quot;狠下&quot; &quot;心&quot; &quot;拿&quot; &quot;什麼&quot; &quot;想&quot; [7] &quot;妳&quot; &quot;\\n&quot; &quot;想成&quot; &quot;了&quot; &quot;風雨&quot; &quot;\\n&quot; [13] &quot;對不起&quot; &quot;\\n&quot; &quot;保護&quot; &quot;一顆&quot; &quot;心看&quot; &quot;多&quot; [19] &quot;了&quot; &quot;烏雲&quot; &quot;\\n&quot; &quot;兩忘&quot; &quot;曾經&quot; &quot;裡&quot; [25] &quot;\\n&quot; &quot;不怨&quot; &quot;妳&quot; &quot;\\n&quot; &quot;心中&quot; &quot;有&quot; [31] &quot;心語&quot; &quot;\\n&quot; &quot;妳&quot; &quot;我&quot; &quot;是&quot; &quot;雙影&quot; [37] &quot;\\n&quot; &quot;一半&quot; &quot;無情&quot; &quot;\\n&quot; &quot;另一半&quot; &quot;深情&quot; [43] &quot;\\n&quot; &quot;貪&quot; &quot;一點&quot; &quot;愛&quot; &quot;什麼&quot; &quot;痛&quot; [49] &quot;也&quot; &quot;允許&quot; &quot;\\n&quot; &quot;用&quot; &quot;懷疑&quot; &quot;交換&quot; [55] &quot;\\n&quot; &quot;秘密&quot; &quot;\\n&quot; &quot;寵愛&quot; &quot;和&quot; &quot;被&quot; [61] &quot;忘&quot; &quot;\\n&quot; &quot;在&quot; &quot;心中&quot; &quot;交談&quot; &quot;\\n&quot; [67] &quot;(&quot; &quot;說來&quot; &quot;迷惘&quot; &quot;)&quot; &quot;\\n&quot; &quot;妳&quot; [73] &quot;作證&quot; &quot;我&quot; &quot;的&quot; &quot;冷暖&quot; &quot;\\n&quot; &quot;悲歡&quot; [79] &quot;(&quot; &quot;夢短路長&quot; &quot;)&quot; &quot;\\n&quot; &quot;妳&quot; &quot;拉&quot; [85] &quot;我&quot; &quot;的&quot; &quot;手繡&quot; &quot;一件&quot; &quot;孤單&quot; &quot;(&quot; [91] &quot;絲綢&quot; &quot;堆&quot; &quot;了&quot; &quot;月光&quot; &quot;)&quot; &quot;\\n&quot; [97] &quot;說&quot; &quot;用來&quot; &quot;取暖&quot; &quot;\\n&quot; &quot;誰&quot; &quot;敢&quot; [103] &quot;\\n&quot; &quot;命&quot; &quot;在&quot; &quot;誰&quot; &quot;命裡&quot; &quot;\\n&quot; [109] &quot;愛恨&quot; &quot;是&quot; &quot;雙影&quot; &quot;\\n&quot; &quot;一端&quot; &quot;美麗&quot; [115] &quot;\\n&quot; &quot;另一端&quot; &quot;無語&quot; &quot;\\n&quot; &quot;遠遠&quot; &quot;走來&quot; [121] &quot;沒有&quot; &quot;字&quot; &quot;的&quot; &quot;未來&quot; &quot;\\n&quot; &quot;被&quot; [127] &quot;時間&quot; &quot;教會&quot; &quot;\\n&quot; &quot;也許&quot; &quot;\\n&quot; &quot;情斷&quot; [133] &quot;留情&quot; &quot;意&quot; &quot;\\n&quot; &quot;忘記&quot; &quot;是&quot; &quot;雙影&quot; [139] &quot;\\n&quot; &quot;一天&quot; &quot;冷淡&quot; &quot;另&quot; &quot;一天&quot; &quot;想起&quot; [145] &quot;\\n&quot; &quot;但願&quot; &quot;我們&quot; &quot;永遠&quot; &quot;走&quot; &quot;在&quot; [151] &quot;光裡&quot; &quot;\\n&quot; &quot;這&quot; &quot;一生&quot; &quot;如此&quot; &quot;\\n&quot; [157] &quot;多雲&quot; &quot;\\n&quot; &quot;這&quot; &quot;一生&quot; &quot;從此&quot; &quot;\\n&quot; [163] &quot;無&quot; &quot;雲&quot; &quot;\\n&quot; &quot;感謝&quot; &quot;\\n&quot; &quot;好&quot; [169] &quot;青年&quot; &quot;\\n&quot; &quot;提供&quot; &quot;歌詞&quot; Case Study: Concordances with kwic() With this tokens object, we can perform the concordance analysis with kwic() and create a dispersion plot of the keyword. ## keyword-in-context kwic(corp_tokens, &quot;快樂&quot;, window = 10) ## Create dispersion plot based on kwic textplot_xray( kwic(corp_tokens, &quot;雙截棍&quot;)) Because we have also added the document-level information, we can utilize these document-level variables and perform more interesting analysis. For example, we can examine the concordance lines of a keyword in a subset of the corpus. In the following code chunk, we examine the concordance lines of 快樂 and 難過 in songs written by 方文山. ## Subsetting tokens corp_tokens_subset &lt;-tokens_subset(corp_tokens, ## tokens str_detect(lyricist, &quot;方文山&quot;)) ## subsetting rule ## Creating dispersion plot based on kwic textplot_xray( kwic(corp_tokens_subset, &quot;快樂&quot;), kwic(corp_tokens_subset, &quot;難過&quot;)) Case Study: Comparison Word Cloud In quanteda, we can quickly create a comparison word cloud, showing the the differences of the lexical distributions in different corpus subsets. For example, we can create a comparison word cloud to show the lexical differences of Jay’s and Amei’s songs. ## Create comparison word clouds corp_tokens %&gt;% dfm() %&gt;% ## from tokens to dfm objects dfm_remove(pattern= stopwords_chi, ## remove stopwords valuetype=&quot;fixed&quot;) %&gt;% dfm_keep(pattern = &quot;[\\u4E00-\\u9FFF]+&quot;, ## include words consisting of freq Chinese chars valuetype= &quot;regex&quot;) %&gt;% dfm_group (groups = artist) %&gt;% ## group by artist dfm_trim(min_termfreq = 10, ## distributional cutoffs min_docfreq = 2, verbose = F) %&gt;% textplot_wordcloud(comparison=TRUE, min_size = 0.8, max_size = 4, color = c(&quot;navyblue&quot;,&quot;gold&quot;)) Case Study: Collocations ## extract collocations corp_collocations &lt;- corp_tokens %&gt;% tokens_keep(pattern = &quot;[\\u4E00-\\u9FFF]+&quot;, ## include words consisting of freq Chinese chars valuetype= &quot;regex&quot;) %&gt;% textstat_collocations( size = 2, min_count = 10) ## Check top 20 collocations in the corpus top_n(corp_collocations, 20, z) Conclusion Tokenization is an important step in Chinese text processing. We may need to take into account many factors when determining the right tokenization method, including: What is the base unit we would like to work with? Texts? Paragraphs? Chunks? Sentences? N-grams? Words? Do we need an enriched version of the raw texts, e.g., the parts-of-speech tags of words in the later analysis? Do we need to include non-word tokens such as symbols, punctuation marks, digits, or alphabets in the analysis? Do we need to remove semantically irrelevant/unimportant words, i.e., stopwords? Do we have many domain-specific words in our corpus (e.g., terminology, proper names)? The answers to the above questions would give us more clues on how to determine the most effective tokenization methods for the data. Thank you! Questions? References "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
