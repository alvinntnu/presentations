[["index.html", "Processing Chinese Data with R Introduction Structure of the Workshop Data Materials Resources", " Processing Chinese Data with R Alvin Cheng-Hsien Chen National Taiwan Normal University, Taiwan April 26, 2022, @ HKSYU (Online Workshop) Introduction The theme of this workshop is Chinese text processing with R. I will introduce a few useful R libraries that can be utilized for text analytic tasks. Also, I will focus on an important step in Chinese processing, namely, the word segmentation/tokenization, and show how to attend to this step in the pipeline. Finally, I will demonstrate a few potential applications of data analysis visualization with the help of the attractive informative graphs with R. Structure of the Workshop Environment Setup Working Pipeline for Text Processing Chinese Word Segmentation Applications Concordances Frequency Lists Word clouds Patterns N-gram/Lexical Bundles Data All the data sets and the R scripts used in this workshop can be downloaded here. After downloading the file, please unzip the zipped file to get an R script file and a directory (demo_data) going with it. Then double click the R script file to start. Materials In the lecture notes, the text boxes in light blue refer to codes that you need to run in either terminal or your R/Python console. The text boxes in black background show the outputs of the code processing. We will follow this presentation convention throughout the entire lecture notes. print(&#39;Hello! R!&#39;) [1] &quot;Hello! R!&quot; Resources R Fundamentals (Wickham and Grolemund (2017), Davies (2016)) Corpus Processing (Gries (2016), Stefanowitsch (2019)) Statistics (Gries (2021), Baayen (2008), Brezina (2018), Winter (2020)) In addition, there are a few more reference books listed at the end of the section (See References), which we will refer to for specific topics, including Gries (2021), Baayen (2008), Brezina (2018), Winter (2020) References "],["environment-setup.html", "Environment Setup R and RStudio Libraries Environment Checking", " Environment Setup R and RStudio In this workshop, I expect that all the participants have installed an working R environment and the IDE RStudio in their workstation. Libraries There are many useful and effective R libraries for text processing. I would highly recommend the official webpage, CRAN Task Views, which categorizes all these packages based on their topics. Of particular relevance in this workshop is the CRAN Task Topic – Natural Language Processing. Specifically, in this workshop, I will use the following libraries. Based on my experiences with data processing, I think these libraries are quite effective and easy to learn. tidyverse: This library includes a collection of useful R libraries for data analysis. quanteda: This library provides many useful functions for exploratory quantitative text analysis. tidytext: This library provides many useful functions for computational text analysis under the same tidy structure framework. jiebar: This library provides functions for Chinese tokenization (i.e., word segmentation). These libraries can be easily installed in R using the following codes: install.packages(&quot;tidyverse&quot;) install.packages(&quot;quanteda&quot;) install.packages(&quot;quanteda.textplots&quot;) install.packages(&quot;quanteda.textstats&quot;) install.packages(&quot;readtext&quot;) install.packages(&quot;tidytext&quot;) install.packages(&quot;jiebaR&quot;) install.packages(&quot;wordcloud2&quot;) Environment Checking The R Version to produce these lecture notes: R version 4.1.2 (2021-11-01). After we installed the libraries, we can import these libraries in the current R session for the later use: library(tidyverse) library(quanteda) library(quanteda.textplots) library(quanteda.textstats) library(readtext) library(tidytext) library(jiebaR) library(wordcloud2) packageVersion(&quot;tidyverse&quot;) [1] &#39;1.3.1&#39; packageVersion(&quot;quanteda&quot;) [1] &#39;3.2.0&#39; packageVersion(&quot;quanteda.textplots&quot;) [1] &#39;0.94&#39; packageVersion(&quot;quanteda.textstats&quot;) [1] &#39;0.95&#39; packageVersion(&quot;tidytext&quot;) [1] &#39;0.3.2&#39; packageVersion(&quot;jiebaR&quot;) [1] &#39;0.11&#39; packageVersion(&quot;wordcloud2&quot;) [1] &#39;0.2.2&#39; Or alternatively, we can run the function sessionInfo() to get an overview of the current R environment: sessionInfo() R version 4.1.2 (2021-11-01) Platform: x86_64-apple-darwin17.0 (64-bit) Running under: macOS Catalina 10.15.7 Matrix products: default BLAS: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib LAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib locale: [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 attached base packages: [1] stats graphics grDevices utils datasets methods base other attached packages: [1] wordcloud2_0.2.2 jiebaR_0.11 jiebaRD_0.1 [4] tidytext_0.3.2 readtext_0.81 quanteda.textstats_0.95 [7] quanteda.textplots_0.94 quanteda_3.2.0 forcats_0.5.1 [10] stringr_1.4.0 dplyr_1.0.7 purrr_0.3.4 [13] readr_2.1.1 tidyr_1.1.4 tibble_3.1.6 [16] ggplot2_3.3.5 tidyverse_1.3.1 showtext_0.9-4 [19] showtextdb_3.0 sysfonts_0.8.5 loaded via a namespace (and not attached): [1] fs_1.5.2 lubridate_1.8.0 httr_1.4.2 SnowballC_0.7.0 [5] tools_4.1.2 backports_1.4.1 bslib_0.3.1 utf8_1.2.2 [9] R6_2.5.1 DBI_1.1.2 colorspace_2.0-2 withr_2.4.3 [13] tidyselect_1.1.1 compiler_4.1.2 cli_3.1.0 rvest_1.0.2 [17] xml2_1.3.3 bookdown_0.24 sass_0.4.0 scales_1.1.1 [21] digest_0.6.29 rmarkdown_2.11 pkgconfig_2.0.3 htmltools_0.5.2 [25] dbplyr_2.1.1 fastmap_1.1.0 highr_0.9 htmlwidgets_1.5.4 [29] rlang_0.4.12 readxl_1.3.1 rstudioapi_0.13 jquerylib_0.1.4 [33] generics_0.1.1 jsonlite_1.7.2 tokenizers_0.2.1 magrittr_2.0.1 [37] Matrix_1.4-0 Rcpp_1.0.8.3 munsell_0.5.0 fansi_0.5.0 [41] lifecycle_1.0.1 stringi_1.7.6 yaml_2.2.1 grid_4.1.2 [45] crayon_1.4.2 lattice_0.20-45 haven_2.4.3 hms_1.1.1 [49] knitr_1.37 pillar_1.6.4 stopwords_2.3 fastmatch_1.1-3 [53] reprex_2.0.1 glue_1.6.0 evaluate_0.14 data.table_1.14.2 [57] RcppParallel_5.1.4 modelr_0.1.8 vctrs_0.3.8 tzdb_0.2.0 [61] cellranger_1.1.0 gtable_0.3.0 assertthat_0.2.1 xfun_0.29 [65] broom_0.7.11 janeaustenr_0.1.5 nsyllable_1.0 ellipsis_0.3.2 If your R version is older than the above one, please consider updating your R. Details about updating R can be found in: 3 Methods to Update R &amp; Rstudio (For Windows &amp; Mac) Updating R, Rstudio, and Your Packages "],["loading-text-data.html", "Loading Text Data Text Data Types Libraries CSV Format Directory Format Zipped File Format Encoding Issues", " Loading Text Data Text Data Types There are generally three types of formats for text data storage in the hard drive: CSV: demo_data/song-jay-amei-v1.csv Directory: demo_data/corp_dir A Zipped File: demo_data/song-jay-amei-v2.zip In this workshop, I will show you how to load the data of these three types in R. Libraries When we need to load text data from external files (e.g., txt, tar.gz files), there is a simple and powerful R package for loading texts: readtext, which goes hand in hand with text processing packages, such as quanteda or tidytext. CSV Format For small data sets, people often store their textual data as a spreadsheet table. We can easily load this type of data using read_csv() (or read_tsv()). corp &lt;- read_csv(file = &quot;demo_data/song-jay-amei-v1.csv&quot;, ## filename locale = locale(encoding = &quot;UTF-8&quot;)) ## encoding corp If you see garbled characters in the data frame, that may be due to the issue of encoding. Please see Encoding Issues. Directory Format Sometimes, if we have a lot of text documents, we may include all the documents of the corpus in one directory. With the data structure like this, we can load the entire corpus in the directory, using readtext(): corp &lt;- readtext(file=&quot;demo_data/corp_dir&quot;, docvarsfrom = &quot;filenames&quot;, dvsep=&quot;-&quot;, docvarnames = c(&quot;artist&quot;, &quot;index&quot;), encoding = &quot;UTF-8&quot;) corp We often include metadata information of each document in the filenames. For example, in our example data set, we include the artist’s name as well as the index number for each document in its filename. So when we load the data set, readtext() comes with the functionality to parse these important pieces of metadata information from the filenames. Zipped File Format When the corpus is large, poeple may zip the entire corpus as an archived zipped file to save storage space on the hard drive. To load the data set like this, we can also use readtext(). corp &lt;- readtext(file = &quot;demo_data/song-jay-amei-v2.tar.gz&quot;, docvarsfrom = &quot;filenames&quot;, dvsep=&quot;-&quot;, docvarnames = c(&quot;artist&quot;, &quot;index&quot;), encoding = &quot;UTF-8&quot;) corp The function readtext() works on: text (.txt) files; comma-separated-value (.csv) files; XML formatted data; data from the Facebook API, in JSON format; data from the Twitter API, in JSON format; and generic JSON data. Encoding Issues In computational text processing, the default encoding of text data is UTF-8. However, not all operating systems take UTF-8 as the default encoding for text file management. This has a lot to do with the operating system locale. In R, we can check the system locale with: Sys.getlocale() [1] &quot;en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8&quot; For Windows users, if your locale is English, you may see: [1] &quot;LC_COLLATE=English_United States.1252;LC_CTYPE=English_United States.1252;LC_MONETARY=English_United States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252&quot; If your locale is Chinese, you may see: [1] &quot;LC_COLLATE=Chinese (Traditional)_Taiwan.950;LC_CTYPE=Chinese (Traditional)_Taiwan.950;LC_MONETARY=Chinese (Traditional)_Taiwan.950;LC_NUMERIC=C;LC_TIME=Chinese (Traditional)_Taiwan.950&quot; For Mac, the default encoding should be UTF-8. For Windows, things can get more complicated. In general, the default encoding in Windows is locale-dependent (i.e., depending on the OS language setting: big-5 for Chinese). In Windows, we can change the locale in R as below: ## from chinese to english Sys.setlocale(category = &quot;LC_ALL&quot;, locale = &quot;UTF-8&quot;) ## from english to traditional chinese Sys.setlocale(category = &quot;LC_ALL&quot;, locale = &quot;cht&quot;) Sys.setlocale(category = &quot;LC_ALL&quot;, locale = &quot;chs&quot;) To process Chinese data with R, for Windows users, please set the locale to the Chinese language. "],["chinese-word-segmentation.html", "Chinese Word Segmentation Chinese Word Segmenter jiebaR Chinese Text Analytics Pipeline", " Chinese Word Segmentation library(tidyverse) library(quanteda) library(quanteda.textplots) library(quanteda.textstats) library(readtext) library(tidytext) library(jiebaR) library(wordcloud2) ## Uncomment to set locale # Sys.setlocale(category = &quot;LC_ALL&quot;, locale = &quot;cht&quot;) Before we perform any analysis on the Chinese data, we need to fix the issue of word tokenization. Unlike English, where each word token is delimited by white spaces, Chinese word tokens are much less straightforward.A word, however, is an important semantic unit in many corpus analysis. In this workshop, I will introduce a frequently used library, jiebaR, for Chinese word segmentation. Chinese Word Segmenter jiebaR Start Now let us take a look at a quick example. Let us assume that in our corpus, we have collected only one text document, with only a short paragraph. sents &lt;- c(&quot;綠黨桃園市議員王浩宇爆料，指民眾黨不分區被提名人蔡壁如、黃瀞瑩，在昨（6）日才請辭是為領年終獎金。台灣民眾黨主席、台北市長柯文哲7日受訪時則說，都是按流程走，不要把人家想得這麼壞。&quot;, &quot;新北消防救護人員說，119在疫情的期間，除肩負協助確診患者送醫的任務，仍需負擔每天超過500件以上的緊急救護案件，疫情期間稍一不慎，極可能造成到院前，緊急救護能量和醫療體系的崩壞；目前確診案件不斷的增加，國家面臨疫情是在清零與共存政策間的戰略上，指引的調整勢必勢在必行。&quot;) There are two important steps in Chinese word segmentation: Initialize a jiebar object using worker() Tokenize the texts into words using the function segment() with the designated jiebar object created earlier seg1 &lt;- worker() segment(sents, jiebar = seg1) [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指民眾&quot; [7] &quot;黨&quot; &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; [13] &quot;蔡壁如&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; &quot;在昨&quot; &quot;6&quot; [19] &quot;日&quot; &quot;才&quot; &quot;請辭&quot; &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; [25] &quot;台灣民眾&quot; &quot;黨&quot; &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文&quot; [31] &quot;哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; [37] &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;不要&quot; [43] &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; &quot;新北&quot; [49] &quot;消防&quot; &quot;救護&quot; &quot;人員&quot; &quot;說&quot; &quot;119&quot; &quot;在&quot; [55] &quot;疫情&quot; &quot;的&quot; &quot;期間&quot; &quot;除&quot; &quot;肩負&quot; &quot;協助&quot; [61] &quot;確診&quot; &quot;患者&quot; &quot;送醫&quot; &quot;的&quot; &quot;任務&quot; &quot;仍&quot; [67] &quot;需&quot; &quot;負擔&quot; &quot;每天&quot; &quot;超過&quot; &quot;500&quot; &quot;件&quot; [73] &quot;以上&quot; &quot;的&quot; &quot;緊急&quot; &quot;救護&quot; &quot;案件&quot; &quot;疫情&quot; [79] &quot;期間&quot; &quot;稍&quot; &quot;一&quot; &quot;不慎&quot; &quot;極&quot; &quot;可能&quot; [85] &quot;造成&quot; &quot;到&quot; &quot;院前&quot; &quot;緊急&quot; &quot;救護&quot; &quot;能量&quot; [91] &quot;和&quot; &quot;醫療&quot; &quot;體系&quot; &quot;的&quot; &quot;崩壞&quot; &quot;目前&quot; [97] &quot;確診&quot; &quot;案件&quot; &quot;不斷&quot; &quot;的&quot; &quot;增加&quot; &quot;國家&quot; [103] &quot;面臨&quot; &quot;疫情&quot; &quot;是&quot; &quot;在&quot; &quot;清零&quot; &quot;與&quot; [109] &quot;共存&quot; &quot;政策&quot; &quot;間&quot; &quot;的&quot; &quot;戰略&quot; &quot;上&quot; [115] &quot;指引&quot; &quot;的&quot; &quot;調整&quot; &quot;勢必&quot; &quot;勢在必行&quot; class(seg1) [1] &quot;jiebar&quot; &quot;segment&quot; &quot;jieba&quot; To word-tokenize the document, text, you first initialize a jiebar object, i.e., seg1, using worker() and feed this jiebar to segment(jiebar = seg1)and tokenize text into words. Parameters Setting There are many different parameters you can specify when you initialize the jiebar object. You may get more detail via the documentation ?worker(). Some of the important arguments include: user = ...: This argument is to specify the path to a user-defined dictionary stop_word = ...: This argument is to specify the path to a stopword list symbol = FALSE: Whether to return symbols (the default is FALSE) bylines = FALSE: Whether to return a list or not (crucial if you are using tidytext::unnest_tokens()) User-defined dictionary From the above example, it is clear to see that some of the words have not been correctly identified by the current segmenter: for example, 民眾黨, 不分區, 黃瀞瑩, 柯文哲. It is always recommended to include a user-defined dictionary when tokenizing your texts because different corpora may have their own unique vocabulary (i.e., domain-specific lexicon). This can be done with the argument user = ... when you initialize the jiebar object, i.e, worker(..., user = ...). seg2 &lt;- worker(user = &quot;demo_data/dict-ch-user-demo.txt&quot;) segment(sents, seg2) [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指&quot; [7] &quot;民眾黨&quot; &quot;不分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; &quot;蔡壁如&quot; [13] &quot;黃瀞瑩&quot; &quot;在昨&quot; &quot;6&quot; &quot;日&quot; &quot;才&quot; &quot;請辭&quot; [19] &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; &quot;台灣&quot; &quot;民眾黨&quot; &quot;主席&quot; [25] &quot;台北&quot; &quot;市長&quot; &quot;柯文哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; [31] &quot;時則&quot; &quot;說&quot; &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; [37] &quot;走&quot; &quot;不要&quot; &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; [43] &quot;壞&quot; &quot;新北&quot; &quot;消防&quot; &quot;救護&quot; &quot;人員&quot; &quot;說&quot; [49] &quot;119&quot; &quot;在&quot; &quot;疫情&quot; &quot;的&quot; &quot;期間&quot; &quot;除&quot; [55] &quot;肩負&quot; &quot;協助&quot; &quot;確診&quot; &quot;患者&quot; &quot;送醫&quot; &quot;的&quot; [61] &quot;任務&quot; &quot;仍&quot; &quot;需&quot; &quot;負擔&quot; &quot;每天&quot; &quot;超過&quot; [67] &quot;500&quot; &quot;件&quot; &quot;以上&quot; &quot;的&quot; &quot;緊急&quot; &quot;救護&quot; [73] &quot;案件&quot; &quot;疫情&quot; &quot;期間&quot; &quot;稍&quot; &quot;一&quot; &quot;不慎&quot; [79] &quot;極&quot; &quot;可能&quot; &quot;造成&quot; &quot;到&quot; &quot;院前&quot; &quot;緊急&quot; [85] &quot;救護&quot; &quot;能量&quot; &quot;和&quot; &quot;醫療&quot; &quot;體系&quot; &quot;的&quot; [91] &quot;崩壞&quot; &quot;目前&quot; &quot;確診&quot; &quot;案件&quot; &quot;不斷&quot; &quot;的&quot; [97] &quot;增加&quot; &quot;國家&quot; &quot;面臨&quot; &quot;疫情&quot; &quot;是&quot; &quot;在&quot; [103] &quot;清零&quot; &quot;與&quot; &quot;共存&quot; &quot;政策&quot; &quot;間&quot; &quot;的&quot; [109] &quot;戰略&quot; &quot;上&quot; &quot;指引&quot; &quot;的&quot; &quot;調整&quot; &quot;勢必&quot; [115] &quot;勢在必行&quot; The format of the user-defined dictionary is a text file, with one word per line. Also, the default encoding of the dictionary is UTF-8. Please note that in Windows, the default encoding of a Chinese txt file created by Notepad may not be UTF-8. (Usually, it is encoded in big-5). Also, files created by MS Office applications tend to be less transparent in terms of their encoding. :::{.info} Creating a user-defined dictionary may take a lot of time. You may consult 搜狗詞庫, which includes many domain-specific dictionaries created by others. However, it should be noted that the format of the dictionaries is .scel. You may need to convert the .scel to .txt before you use it in jiebaR. To do the conversion automatically, please consult the library cidian. Also, you need to do the traditional-simplified Chinese conversion as well. For this, you may consult the library ropencc in R. :::{.info} Stopwords When you initialize the worker(), you can also specify a stopword list, i.e., words that you do not need to include in the later analyses. For example, in text mining, functional words are usually less informative, thus often excluded in the process of preprocessing. seg3 &lt;- worker(user = &quot;demo_data/dict-ch-user-demo.txt&quot;, stop_word = &quot;demo_data/stopwords-ch-demo.txt&quot;) segment(sents, seg3) [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指&quot; [7] &quot;民眾黨&quot; &quot;不分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; &quot;蔡壁如&quot; [13] &quot;黃瀞瑩&quot; &quot;在昨&quot; &quot;6&quot; &quot;才&quot; &quot;請辭&quot; &quot;為領&quot; [19] &quot;年終獎金&quot; &quot;台灣&quot; &quot;民眾黨&quot; &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; [25] &quot;柯文哲&quot; &quot;7&quot; &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; &quot;按&quot; [31] &quot;流程&quot; &quot;走&quot; &quot;不要&quot; &quot;把&quot; &quot;人家&quot; &quot;想得&quot; [37] &quot;這麼&quot; &quot;壞&quot; &quot;新北&quot; &quot;消防&quot; &quot;救護&quot; &quot;人員&quot; [43] &quot;說&quot; &quot;119&quot; &quot;在&quot; &quot;疫情&quot; &quot;的&quot; &quot;期間&quot; [49] &quot;除&quot; &quot;肩負&quot; &quot;協助&quot; &quot;確診&quot; &quot;患者&quot; &quot;送醫&quot; [55] &quot;的&quot; &quot;任務&quot; &quot;仍&quot; &quot;需&quot; &quot;負擔&quot; &quot;每天&quot; [61] &quot;超過&quot; &quot;500&quot; &quot;件&quot; &quot;以上&quot; &quot;的&quot; &quot;緊急&quot; [67] &quot;救護&quot; &quot;案件&quot; &quot;疫情&quot; &quot;期間&quot; &quot;稍&quot; &quot;一&quot; [73] &quot;不慎&quot; &quot;極&quot; &quot;可能&quot; &quot;造成&quot; &quot;到&quot; &quot;院前&quot; [79] &quot;緊急&quot; &quot;救護&quot; &quot;能量&quot; &quot;和&quot; &quot;醫療&quot; &quot;體系&quot; [85] &quot;的&quot; &quot;崩壞&quot; &quot;目前&quot; &quot;確診&quot; &quot;案件&quot; &quot;不斷&quot; [91] &quot;的&quot; &quot;增加&quot; &quot;國家&quot; &quot;面臨&quot; &quot;疫情&quot; &quot;在&quot; [97] &quot;清零&quot; &quot;與&quot; &quot;共存&quot; &quot;政策&quot; &quot;間&quot; &quot;的&quot; [103] &quot;戰略&quot; &quot;上&quot; &quot;指引&quot; &quot;的&quot; &quot;調整&quot; &quot;勢必&quot; [109] &quot;勢在必行&quot; POS Tagging So far we haven’t seen the parts-of-speech tags provided by the word segmenter. If you need the POS tags of the words, you need to specify the argument type = \"tag\" when you initialize the worker(). seg4 &lt;- worker(type = &quot;tag&quot;, #dict = &quot;demo_data/jieba-tw/dict.txt&quot;, user = &quot;demo_data/dict-ch-user-demo.txt&quot;, stop_word = &quot;demo_data/stopwords-ch-demo.txt&quot;, symbol = F) segment(sents, seg4) n ns n x n n x &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指&quot; &quot;民眾黨&quot; x p v n x x x &quot;不分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; &quot;蔡壁如&quot; &quot;黃瀞瑩&quot; &quot;在昨&quot; x d v x n x x &quot;6&quot; &quot;才&quot; &quot;請辭&quot; &quot;為領&quot; &quot;年終獎金&quot; &quot;台灣&quot; &quot;民眾黨&quot; n ns n x x v x &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文哲&quot; &quot;7&quot; &quot;受訪&quot; &quot;時則&quot; zg p n v df p n &quot;說&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;不要&quot; &quot;把&quot; &quot;人家&quot; x r a x n v n &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; &quot;新北&quot; &quot;消防&quot; &quot;救護&quot; &quot;人員&quot; zg m p n uj f p &quot;說&quot; &quot;119&quot; &quot;在&quot; &quot;疫情&quot; &quot;的&quot; &quot;期間&quot; &quot;除&quot; n v v n v uj n &quot;肩負&quot; &quot;協助&quot; &quot;確診&quot; &quot;患者&quot; &quot;送醫&quot; &quot;的&quot; &quot;任務&quot; zg v v r v m zg &quot;仍&quot; &quot;需&quot; &quot;負擔&quot; &quot;每天&quot; &quot;超過&quot; &quot;500&quot; &quot;件&quot; f uj a v n n f &quot;以上&quot; &quot;的&quot; &quot;緊急&quot; &quot;救護&quot; &quot;案件&quot; &quot;疫情&quot; &quot;期間&quot; zg m a d v v v &quot;稍&quot; &quot;一&quot; &quot;不慎&quot; &quot;極&quot; &quot;可能&quot; &quot;造成&quot; &quot;到&quot; s a v n c n n &quot;院前&quot; &quot;緊急&quot; &quot;救護&quot; &quot;能量&quot; &quot;和&quot; &quot;醫療&quot; &quot;體系&quot; uj v t v n d uj &quot;的&quot; &quot;崩壞&quot; &quot;目前&quot; &quot;確診&quot; &quot;案件&quot; &quot;不斷&quot; &quot;的&quot; v n v n p z zg &quot;增加&quot; &quot;國家&quot; &quot;面臨&quot; &quot;疫情&quot; &quot;在&quot; &quot;清零&quot; &quot;與&quot; v n f uj n f v &quot;共存&quot; &quot;政策&quot; &quot;間&quot; &quot;的&quot; &quot;戰略&quot; &quot;上&quot; &quot;指引&quot; uj vn d i &quot;的&quot; &quot;調整&quot; &quot;勢必&quot; &quot;勢在必行&quot; The returned object is a named character vector, i.e., the POS tags of the words are included in the names of the vectors. Every POS tagger has its own predefined tag set. The following table lists the annotations of the POS tag set used in jiebaR: Chinese Text Analytics Pipeline Below is a quick overview of the Chinese Text Analytics Flowchart (Figure 1). In this workshop, I will introduce two methodological frameworks to process Chinese corpus data: Tidytext Framework Quanteda Framework As these two packages are more optimized for the English language, I will highlight a few important steps that require more attention with respect to Chinese word segmentation. Figure 1: Chinese Text Analytics Flowchart Quanteda Framework To perform word tokenization using jiebaR under the Quanteda framework, we need to create the tokens object using the segment() from jiebaR. The steps are very straightforward. We utilize jiebaR for word tokenization like before: (1) initialize a jiebar model and (2) use it to tokenize the corpus text with segment(). The key is that we need to convert the output of segment() from a list to a tokens using as.tokens(). With this tokens object, we can apply kiwc() or other Quanteda-supported processing to the corpus data. # initialize segmenter my_seg &lt;- worker(bylines = T, user = &quot;demo_data/dict-ch-user-demo.txt&quot;, symbol=T) ## create tokens based on self-defined segmentation text_tokens &lt;- as.tokens(segment(sents, my_seg)) ## kwic on word tokens kwic(text_tokens, pattern = &quot;柯文哲&quot;) kwic(text_tokens, pattern = &quot;.*?[台市].*?&quot;, valuetype = &quot;regex&quot;) In Quanteda, there is a quanteda-native Chinese tokenizer, tokens(). However, its performance is very limited and it does not support user-defined dictionary. Therefore, I would suggest using one’s own self-defined tokenizer for Chinese word segmentation. ## Examples of using quanteda-native ## Word segmentation ## create corpus object text_corpus &lt;- corpus(sents) ## summary summary(text_corpus) ## Create tokens object text_tokens &lt;- tokens(text_corpus) ## Check quanteda-native word tokenization result text_tokens[[1]] [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王&quot; &quot;浩&quot; &quot;宇&quot; [7] &quot;爆&quot; &quot;料&quot; &quot;，&quot; &quot;指&quot; &quot;民眾&quot; &quot;黨&quot; [13] &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; &quot;蔡&quot; [19] &quot;壁&quot; &quot;如&quot; &quot;、&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; [25] &quot;，&quot; &quot;在&quot; &quot;昨&quot; &quot;（&quot; &quot;6&quot; &quot;）&quot; [31] &quot;日&quot; &quot;才&quot; &quot;請辭&quot; &quot;是&quot; &quot;為&quot; &quot;領&quot; [37] &quot;年終獎金&quot; &quot;。&quot; &quot;台灣&quot; &quot;民眾&quot; &quot;黨主席&quot; &quot;、&quot; [43] &quot;台北市&quot; &quot;長&quot; &quot;柯&quot; &quot;文&quot; &quot;哲&quot; &quot;7&quot; [49] &quot;日&quot; &quot;受&quot; &quot;訪&quot; &quot;時&quot; &quot;則&quot; &quot;說&quot; [55] &quot;，&quot; &quot;都是&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;，&quot; [61] &quot;不要&quot; &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; [67] &quot;。&quot; ## KWIC kwic(text_tokens, pattern = &quot;柯文哲&quot;) kwic(text_tokens, pattern = &quot;柯&quot;) From the above example, it is clear to see that quite a few word tokens have not been successfully identified by the Quanteda-native word segmentation (e.g., several proper names in the text). This would also have great impact on the effectiveness of kwic() as well. Therefore analysis based on the Quanteda-native segmentation can be very limited. Tidytext Framework Now let’s turn to the Titytext framework of text processing. The tidytext package is made for the handling of the tidy text format of the corpus data, i.e., to process textual data on the basis of data frames. With a data frame format of the text data, we can manipulate the text data with a standard set of tidy tools and packages, including dplyr, tidyr, and ggplot2. In the tidytext format, the tokenization is taken care of by unnest_tokens(). # a text-based tidy corpus corp_df &lt;- data.frame(text_id = c(1,2), text = sents) corp_df corp_df Then, we use unnest_tokens() to tokenize the text-based data frame (corp_df) into a word-based data frame (corp_df_word). Texts included in the text column are tokenized into words, which are unnested into independent rows in the word column of the new data frame. # tokenization corp_df_word &lt;- corp_df %&gt;% unnest_tokens( word, ## name for new tokens to be unnested text, ## name for original larger units token = function(x) ## self-defined tokenization method segment(x, jiebar = my_seg) ) corp_df_word It can be seen that for the token parameter in unnest_tokens(), we use an anonymous function based on jieba and segment() for self-defined Chinese word segmentation. This is called anonymous functions because it has not been assigned to any object name in the current R session. You may check R language documentation for more detail on Writing Functions. "],["applications.html", "Applications Environment Loading Data Overview of the Data Set Data Preprocessing (Cleaning) Initialize jiebaR Tidytext Framework Quanteda Framework Recap", " Applications Environment ## clear up the current session environment rm(list = ls(all=TRUE)) ## loading necessary libraries library(tidyverse) library(quanteda) library(quanteda.textplots) library(quanteda.textstats) library(readtext) library(tidytext) library(jiebaR) library(wordcloud2) ## OS-specific fixing ## For Windows User #Sys.setlocale(category = &quot;LC_ALL&quot;, locale = &quot;cht&quot;) ## Fix Chinese fonts in visualization library(showtext) font_add(&quot;Arial Unicode MS&quot;, &quot;Arial Unicode.ttf&quot;) showtext_auto(enable = TRUE) In this section, we will look at a few more examples of Chinese text processing based on the data set demo_data/song-jay-amei-v1.csv. It is a text collection of songs by Jay Chou and Amei. Loading Data ## loading corp_df_text &lt;- read_csv(file = &quot;demo_data/song-jay-amei-v1.csv&quot;, locale = locale(encoding = &quot;UTF-8&quot;)) ## creating doc_id corp_df_text &lt;- corp_df_text %&gt;% mutate(doc_id = row_number()) corp_df_text Overview of the Data Set The data set demo_data/song-jay-amei-v1.csv is a collection of songs by two artists, Jay Chou and Amei Chang. A quick frequency counts of the songs by artists in the data set: corp_df_text %&gt;% ggplot(aes(artist, fill=artist)) + geom_bar() Data Preprocessing (Cleaning) Raw texts usually include very much noise. For example, irrelevant symbols, characters, punctuation, or redundant white-spaces (e.g., line breaks, tabs, etc.). It is often suggested that we clean up the texts before doing the tokenization. ## Define a function normalize_document &lt;- function(texts) { texts %&gt;% str_replace_all(&quot;[\\n\\\\p{C}]+&quot;, &quot;\\n&quot;) %&gt;% ## remove redundant line breaks str_replace_all(&quot;[ \\u0020\\u00a0\\u3000\\t]+&quot;, &quot;&quot;) %&gt;% ## remove full-width ws and tabs str_replace_all(&quot; *\\n *&quot;, &quot;\\n&quot;) ## clean up linebreak + ws } ## Apply cleaning to every document corp_df_text$lyric &lt;- normalize_document(corp_df_text$lyric) Initialize jiebaR Because we use jiebaR for word tokenization, we first need to initialize the jiebaR models. Here we created two jiebaR objects, one for word tokenization only and the other for parts-of-speech tagging. # initialize segmenter ## for word segmentation only my_seg &lt;- worker(bylines = T, #user = &quot;&quot;, symbol = T) ## for POS tagging my_seg_pos &lt;- worker( type = &quot;tag&quot;, bylines = F, #user = &quot;&quot;, symbol = T ) We can specify the path to the external user-defined dictionary in worker(..., user = \"\"). Alternatively, we can also add add-hoc new words to the jiebaR model. This can be very helpful when we spot any weird segmentation results in the output. By default, new_user_word() assigns each new word with a default n tag. #Add customized terms temp_new_words &lt;-c(&quot;&quot;) new_user_word(my_seg, temp_new_words) [1] TRUE new_user_word(my_seg_pos, temp_new_words) [1] TRUE Tidytext Framework The following are examples of processing the Chinese texts under the tidy structure framework. Recall the three important steps: Load the corpus data using readtext() and create a text-based data frame of the corpus; Initialize a jieba word segmenter using worker() Tokenize the text-based data frame into a line-based data frame using unnest_tokens(); Tokenize the line-based data frame into a word-based data frame using unnest_tokens(); ## Line tokenization corp_df_line &lt;- corp_df_text %&gt;% unnest_tokens( output = line, ## new unit name input = lyric, ## old unit name token = function (x) ## tokenization method str_split(x, &quot;\\n+&quot;) ) %&gt;% group_by(doc_id) %&gt;% mutate(line_id = row_number()) %&gt;% ungroup() corp_df_line ## Word Tokenization corp_df_word &lt;- corp_df_line %&gt;% unnest_tokens( output = word, ## new unit name input = line, ## old unit name token = function(x) ## tokenization method segment(x, jiebar = my_seg) ) %&gt;% group_by(line_id) %&gt;% mutate(word_id = row_number()) %&gt;% # create word index within each document ungroup corp_df_word Creating unique indices for your data is very important. In corpus linguistic analysis, we often need to keep track of the original context of the word, phrase or sentence in the concordances. All these unique indices (as well as the source text filenames) would make things a lot easier. Also, if the metadata of the source documents are available, these unique indices would allow us to connect the tokenized linguistic units to the metadata information (e.g., genres, registers, author profiles) for more interesting analysis. Therefore, after tokenization, we have obtained a line-based and a word-based data frame of our corpus data. Case Study: Word Frequency and Wordcloud With a word-based data frame, we can easily create a word frequency list as well as a word cloud to have a quick overview of the word distribution of the corpus. It should be noted that before creating the frequency list, we often need to consider whether to remove unimportant tokens (e.g., stopwords, symbols, punctuation, digits, alphabets.) We can represent any character in Unicode in the form of \\uXXXX, where the XXXX refers to the coding numbers of the character in Unicode (UTF-8) in hexadecimal format. For example, can you tell which character \\u6211 refers to? How about \\u4f60? In the above regular expression, the Unicode range [\\u4E00-\\u9FFF] refers to a set of frequently used Chinese characters. Therefore, the way we remove unimportant word tokens is to identify all word tokens consisting of these frequently used Chinese characters that fall within this Unicode range. For more information related to the Unicode range for the punctuation marks in CJK languages, please see this SO discussion thread. ## load chinese stopwords stopwords_chi &lt;- readLines(&quot;demo_data/stopwords-ch.txt&quot;, encoding = &quot;UTF-8&quot;) ## create word freq list corp_word_freq &lt;- corp_df_word %&gt;% filter(!word %in% stopwords_chi) %&gt;% # remove stopwords filter(word %&gt;% str_detect(pattern = &quot;[\\u4E00-\\u9FFF]+&quot;)) %&gt;% # remove words consisting of digits count(word) %&gt;% arrange(desc(n)) library(wordcloud2) corp_word_freq %&gt;% filter(n &gt; 20) %&gt;% filter(nchar(word) &gt;= 2) %&gt;% ## remove monosyllabic tokens wordcloud2(shape = &quot;pentagon&quot;, size = 0.3) Case Study: Patterns In this case study, we are looking at a more complex example. In corpus linguistic analysis, we often need to extract a particular pattern from the texts. In order to retrieve the target patterns at a high accuracy rate, we often need to make use of the additional annotations provided by the corpus.The most often-used information is the parts-of-speech tags of words. In this example, we will demonstrate how to enrich our corpus data by adding POS tags information to our current tidy corpus design. Our steps are as follows: Initialize jiebar object, which performs not only word segmentation but also POS tagging; Create a self-defined function to word-seg and pos-tag each text and combine all tokens, word/tag, into a long string for each text; With the line-based data frame apple_df_line, create a new column, which includes the enriched version of each text chunk, using mutate() # define a function to word-seg and pos-tag a text fragment tag_text &lt;- function(x, jiebar) { segment(x, jiebar) %&gt;% ## tokenize + POS-tagging paste(names(.), sep = &quot;/&quot;, collapse = &quot; &quot;) ## reformat output } A quick example of the function’s usage: # demo of the function `tag_text()` tag_text(corp_df_line$line[10], my_seg_pos) [1] &quot;貪/v 一點/m 愛/zg 什麼/r 痛/a 也/d 允許/v&quot; # apply `tag_text()` function to each text corp_df_line &lt;- corp_df_line %&gt;% mutate(line_tag = map_chr(line, tag_text, my_seg_pos)) corp_df_line Now we have obtained an enriched version of all the texts, we can make use of the POS tags for more linguistic analyses. For example, we can examine the use of adjectives in lyrics. The data retrieval procedure is now very straightforward: we only need to create a regular expression that matches our interested pattern and go through the enriched version of the texts (i.e., line_tag column in apple_df_line) to identify these matches with unnest_tokens(). 1.Define a regular expression [^/\\\\s]+/a\\\\b for adjectives; 2.Use unnest_tokens() and str_extract_all() to extract target patterns and create a pattern-based data frame. ## define regex patterns pat &lt;- &quot;[^/\\\\s]+/a\\\\b&quot; ## extract patterns from corp corp_df_pat &lt;- corp_df_line %&gt;% unnest_tokens( output = pat, ## new unit name input = line_tag, ## old unit name token = function(x) ## unnesting method str_extract_all(x, pattern = pat) ) ## Data wrangling/cleaning corp_df_pat_2 &lt;- corp_df_pat %&gt;% mutate(word = str_replace_all(pat, &quot;/.+$&quot;,&quot;&quot;)) %&gt;% ## clean regex group_by(artist) %&gt;% ## split df by artist count(word, sort = T) %&gt;% ## create freq list for each artist top_n(20, n) %&gt;% ## select top 20 for each artist ungroup %&gt;% ## merge df again arrange(artist, -n) ## sort result ## Visulization output corp_df_pat_2 %&gt;% mutate(word = reorder_within(word, n, artist)) %&gt;% ggplot(aes(word, n, fill=artist)) + geom_bar(stat=&quot;identity&quot;)+ coord_flip()+ facet_wrap(~artist,scales = &quot;free_y&quot;) + scale_x_reordered() + labs(x = &quot;Frequency&quot;, y = &quot;Adjectives&quot;, title=&quot;Top 20 Adjectives of Each Artist&#39;s Songs&quot;) Case Study: Lexical Bundles N-grams Extraction With word boundaries, we can also analyze the recurrent multiword units in the corpus. In this example, let’s take a look at the recurrent four-word sequences (i.e., four-grams) in our corpus. As the default n-gram tokenization in unnest_tokens(..., token = \"ngrams\") only works with the English data, we need to define our own ngram tokenization functions. The Chinese ngram tokenization function should: Tokenize each text fragment (i.e., lines) into word tokens Create a set of ngrams from the word tokens of each text ## self defined ngram tokenizer tokenizer_ngrams &lt;- function(texts, jiebar, n = 2 , skip = 0, delimiter = &quot;_&quot;) { texts %&gt;% ## given a vector of lines/chunks segment(jiebar) %&gt;% ## word tokenization as.tokens %&gt;% ## list to tokens tokens_ngrams(n, skip, concatenator = delimiter) %&gt;% ## ngram tokenization as.list ## tokens to list } In the above self-defined ngram tokenizer, we make use of tokens_ngrams() in quanteda, which creates a set of ngrams from already tokenized text objects, i.e., tokens. Because this function requires a tokens object as the input, we need to do the class conversion via as.tokens() and as.list(). Take a look at the following examples for a quick overview of tokens_ngrams(): sents &lt;- c(&quot;Jack and Jill went up the hill to fetch a pail of water&quot;, &quot;Jack fell down and broke his crown and Jill came tumbling after&quot;) sents_tokens &lt;- tokens(sents) ## English tokenization tokens_ngrams(sents_tokens, n = 2, skip = 0) Tokens consisting of 2 documents. text1 : [1] &quot;Jack_and&quot; &quot;and_Jill&quot; &quot;Jill_went&quot; &quot;went_up&quot; &quot;up_the&quot; &quot;the_hill&quot; [7] &quot;hill_to&quot; &quot;to_fetch&quot; &quot;fetch_a&quot; &quot;a_pail&quot; &quot;pail_of&quot; &quot;of_water&quot; text2 : [1] &quot;Jack_fell&quot; &quot;fell_down&quot; &quot;down_and&quot; &quot;and_broke&quot; [5] &quot;broke_his&quot; &quot;his_crown&quot; &quot;crown_and&quot; &quot;and_Jill&quot; [9] &quot;Jill_came&quot; &quot;came_tumbling&quot; &quot;tumbling_after&quot; tokens_ngrams(sents_tokens, n = 2, skip = 1) Tokens consisting of 2 documents. text1 : [1] &quot;Jack_Jill&quot; &quot;and_went&quot; &quot;Jill_up&quot; &quot;went_the&quot; &quot;up_hill&quot; [6] &quot;the_to&quot; &quot;hill_fetch&quot; &quot;to_a&quot; &quot;fetch_pail&quot; &quot;a_of&quot; [11] &quot;pail_water&quot; text2 : [1] &quot;Jack_down&quot; &quot;fell_and&quot; &quot;down_broke&quot; &quot;and_his&quot; [5] &quot;broke_crown&quot; &quot;his_and&quot; &quot;crown_Jill&quot; &quot;and_came&quot; [9] &quot;Jill_tumbling&quot; &quot;came_after&quot; A quick example of how to use the self-defined function tokenizer_ngrams(): # examples texts &lt;- c(&quot;這是一個測試的句子&quot;, &quot;這句子&quot;, &quot;超短句&quot;, &quot;最後一個超長的句子測試&quot;) tokenizer_ngrams( texts = texts, jiebar = my_seg, n = 2, skip = 0, delimiter = &quot;_&quot; ) $text1 [1] &quot;這是_一個&quot; &quot;一個_測試&quot; &quot;測試_的&quot; &quot;的_句子&quot; $text2 [1] &quot;這_句子&quot; $text3 [1] &quot;超短_句&quot; $text4 [1] &quot;最後_一個&quot; &quot;一個_超長&quot; &quot;超長_的&quot; &quot;的_句子&quot; &quot;句子_測試&quot; tokenizer_ngrams( texts = texts, jiebar = my_seg, n = 2, skip = 1, delimiter = &quot;_&quot; ) $text1 [1] &quot;這是_測試&quot; &quot;一個_的&quot; &quot;測試_句子&quot; $text2 character(0) $text3 character(0) $text4 [1] &quot;最後_超長&quot; &quot;一個_的&quot; &quot;超長_句子&quot; &quot;的_測試&quot; tokenizer_ngrams( texts = texts, jiebar = my_seg, n = 5, skip=0, delimiter = &quot;_&quot; ) $text1 [1] &quot;這是_一個_測試_的_句子&quot; $text2 character(0) $text3 character(0) $text4 [1] &quot;最後_一個_超長_的_句子&quot; &quot;一個_超長_的_句子_測試&quot; With the self-defined ngram tokenizer, we can now perform the ngram tokenization on our corpus. We will use the line-based data frame (corp_df_line) as our starting point: We transform the line-based data frame into an ngram-based data frame using unnest_tokens(...) with the self-defined tokenization function tokenizer_ngrams() We remove empty and unwanted n-grams entries: Empty ngrams due to short texts Ngrams spanning punctuations, symbols, or paragraph breaks Ngrams including alphanumeric characters ## from line-based to ngram-based corp_df_ngram &lt;- corp_df_line %&gt;% unnest_tokens( ngram, ## new unit name line, ## old unit name token = function(x) ## unnesting method tokenizer_ngrams( texts = x, jiebar = my_seg, n = 4, skip = 0, delimiter = &quot;_&quot; ) ) ## remove unwanted ngrams corp_df_ngram_2 &lt;- corp_df_ngram %&gt;% filter(nzchar(ngram)) %&gt;% ## empty strings filter(!str_detect(ngram, &quot;[^\\u4E00-\\u9FFF_]&quot;)) ## remove unwanted ngrams Frequency and Dispersion A multiword unit can be defined based on at least two important distributional properties (See Biber, Conrad, and Cortes (2004)): The frequency of the whole multiword unit (i.e., frequency) The number of different texts where the multiword unit is observed (i.e., dispersion) Now that we have the ngram-based data frame, we can compute their token frequencies and document frequencies in the corpus using the normal data manipulation tricks. We set cut-offs for four-grams at: dispersion &gt;= 5 (i.e., four-grams that occur in at least five different documents) corp_ngram_dist &lt;- corp_df_ngram_2 %&gt;% group_by(ngram) %&gt;% summarize(freq = n(), dispersion = n_distinct(doc_id)) %&gt;% filter(dispersion &gt;= 3) Please take a look at the four-grams, arranged by frequency and dispersion respectively: # arrange by dispersion corp_ngram_dist %&gt;% arrange(desc(dispersion)) %&gt;% head(10) # arrange by freq corp_ngram_dist %&gt;% arrange(desc(freq)) %&gt;% head(10) We can also look at four-grams with particular lexical words: corp_ngram_dist %&gt;% filter(str_detect(ngram, &quot;我&quot;)) %&gt;% arrange(desc(dispersion)) corp_ngram_dist %&gt;% filter(str_detect(ngram, &quot;你&quot;)) %&gt;% arrange(desc(dispersion)) Quanteda Framework All the above examples demonstrate the Chinese data processing with the tidytext framework. Here, we look at a few more examples of processing the data with the quanteda framework. For Chinese data, the most important base unit in Quanteda is the tokens object. So first we need to create the tokens object based on the jiebaR tokenization method. ## create tokens based on self-defined segmentation corp_tokens &lt;- corp_df_text$lyric %&gt;% map(str_split,&quot;\\n+&quot;, simplify=TRUE) %&gt;% ## line tokenization map(segment, my_seg) %&gt;% ## word segmentation map(unlist) %&gt;% ## reformat structure as.tokens ## list to tokens ## add document-level metadata docvars(corp_tokens) &lt;- corp_df_text[, c(&quot;artist&quot;,&quot;lyricist&quot;,&quot;composer&quot;,&quot;title&quot;,&quot;gender&quot;)] ## check 1st doc head(corp_tokens[[1]],20) [1] &quot;若非&quot; &quot;狠下&quot; &quot;心&quot; &quot;拿&quot; &quot;什麼&quot; &quot;想&quot; &quot;妳&quot; &quot;想成&quot; [9] &quot;了&quot; &quot;風雨&quot; &quot;對不起&quot; &quot;保護&quot; &quot;一顆&quot; &quot;心看&quot; &quot;多&quot; &quot;了&quot; [17] &quot;烏雲&quot; &quot;兩忘&quot; &quot;曾經&quot; &quot;裡不怨&quot; Case Study: Concordances with kwic() This is an example of processing the Chinese data under Quanteda framework. Without relying on the Quanteda-native tokenization, we have created the tokens object directly based on the output of segment(). With this tokens object, we can perform the concordance analysis with kwic(). kwic(corp_tokens, &quot;快樂&quot;, window = 10) Because we have also added the document-level information, we can utilize this metadata and perform more interesting analysis. For example, we can examine the concordance lines of a keyword in a subet of the corpus: corp_tokens_subset &lt;-tokens_subset(corp_tokens, str_detect(lyricist, &quot;方文山&quot;)) textplot_xray( kwic(corp_tokens_subset, &quot;快樂&quot;), kwic(corp_tokens_subset, &quot;難過&quot;)) Case Study: Comparison Word Cloud In quanteda, we can quickly create a comparison cloud, showing the the differences of the lexical distributions in different corpus subsets. corp_tokens %&gt;% dfm() %&gt;% dfm_remove(pattern= stopwords_chi, ## remove stopwords valuetype=&quot;fixed&quot;) %&gt;% dfm_keep(pattern = &quot;[\\u4E00-\\u9FFF]+&quot;, ## include freq chinese char valuetype= &quot;regex&quot;) %&gt;% dfm_group (groups = artist) %&gt;% ## group by artist dfm_trim(min_termfreq = 10, ## distributional cutoffs min_docfreq = 2, verbose = F) %&gt;% textplot_wordcloud(comparison=TRUE, min_size = 0.8, max_size = 4, color = c(&quot;navyblue&quot;,&quot;gold&quot;)) Case Study: Collocations ## extract collocations corp_collocations &lt;- corp_tokens %&gt;% tokens_keep(pattern = &quot;[\\u4E00-\\u9FFF]+&quot;, ## include freq chinese char valuetype= &quot;regex&quot;) %&gt;% textstat_collocations( size = 2, min_count = 10) top_n(corp_collocations, 20, z) Recap Tokenization is an important step in Chinese text processing. We may need to take into account many factors when determining the right tokenization method. In particular, several important questions may be relevant to Chinese text tokenization: Do we need the parts-of-speech tags of words in our research? What is the base unit we would like to work with? Texts? Paragraphs? Chunks? Sentences? N-grams? Words? Do we need non-word tokens such as symbols, punctuation, digits, or alphabets in your analysis? The answers to the above questions would give us more clues on how to determine the most effective structure of the tokenization methods for the data. Thank you! References "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
