[["applications.html", "Applications Environment Loading Data Overview of the Data Set Data Preprocessing (Cleaning) Initialize jiebaR Tidytext Framework Quanteda Framework Recap", " Applications Environment In this section, we will look at a few more examples of Chinese text processing based on the data set demo_data/song-jay-amei-v1.csv. It is a text collection of songs by Jay Chou and Amei. Loading Data ## loading corp_df_text &lt;- read_csv(file = &quot;demo_data/song-jay-amei-v1.csv&quot;, locale = locale(encoding = &quot;UTF-8&quot;)) ## creating doc_id corp_df_text &lt;- corp_df_text %&gt;% mutate(doc_id = row_number()) corp_df_text Overview of the Data Set The data set demo_data/song-jay-amei-v1.csv is a collection of songs by two artists, Jay Chou and Amei Chang. A quick frequency counts of the songs by artists in the data set: corp_df_text %&gt;% ggplot(aes(artist, fill=artist)) + geom_bar() Data Preprocessing (Cleaning) Raw texts usually include very much noise. For example, irrelevant symbols, characters, punctuation, or redundant white-spaces (e.g., line breaks, tabs, etc.). It is often suggested that we clean up the texts before doing the tokenization. ## Define a function normalize_document &lt;- function(texts) { texts %&gt;% str_replace_all(&quot;[\\n\\\\p{C}]+&quot;, &quot;\\n&quot;) %&gt;% ## remove redundant line breaks str_replace_all(&quot;[ \\u0020\\u00a0\\u3000\\t]+&quot;, &quot;&quot;) %&gt;% ## remove full-width ws and tabs str_replace_all(&quot; *\\n *&quot;, &quot;\\n&quot;) ## clean up linebreak + ws } ## Apply cleaning to every document corp_df_text$lyric &lt;- normalize_document(corp_df_text$lyric) # # # # my_segment &lt;- function(texts) { # ## texts that have been preprocessed # texts %&gt;% # map(str_split,&quot;\\n&quot;) %&gt;% ## line tokenize # map(unlist) %&gt;% # map(segment, my_seg) %&gt;% # map(unlist) # } # # my_segment(corp$lyric[1:3]) # corp$lyric[1] %&gt;% # normalize_document() %&gt;% str_split(&quot;\\n&quot;) %&gt;% unlist %&gt;% # segment(jiebar = my_seg) %&gt;% unlist # # Initialize jiebaR Because we use jiebaR for word tokenization, we first need to initialize the jiebaR models. Here we created two jiebaR objects, one for word tokenization only and the other for parts-of-speech tagging. # initialize segmenter ## for word segmentation only my_seg &lt;- worker(bylines = T, #user = &quot;&quot;, symbol = T) ## for POS tagging my_seg_pos &lt;- worker( type = &quot;tag&quot;, bylines = F, #user = &quot;&quot;, symbol = T ) We can specify the path to the external user-defined dictionary in worker(..., user = \"\"). Alternatively, we can also add add-hoc new words to the jiebaR model. This can be very helpful when we spot any weird segmentation results in the output. By default, new_user_word() assigns each new word with a default n tag. #Add customized terms temp_new_words &lt;-c(&quot;&quot;) new_user_word(my_seg, temp_new_words) [1] TRUE new_user_word(my_seg_pos, temp_new_words) [1] TRUE Tidytext Framework The following are examples of processing the Chinese texts under the tidy structure framework. Recall the three important steps: Load the corpus data using readtext() and create a text-based data frame of the corpus; Initialize a jieba word segmenter using worker() Tokenize the text-based data frame into a line-based data frame using unnest_tokens(); Tokenize the line-based data frame into a word-based data frame using unnest_tokens(); ## Line tokenization corp_df_line &lt;- corp_df_text %&gt;% unnest_tokens( output = line, ## new unit name input = lyric, ## old unit name token = function (x) ## tokenization method str_split(x, &quot;\\n+&quot;) ) %&gt;% group_by(doc_id) %&gt;% mutate(line_id = row_number()) %&gt;% ungroup() corp_df_line ## Word Tokenization corp_df_word &lt;- corp_df_line %&gt;% unnest_tokens( output = word, ## new unit name input = line, ## old unit name token = function(x) ## tokenization method segment(x, jiebar = my_seg) ) %&gt;% group_by(line_id) %&gt;% mutate(word_id = row_number()) %&gt;% # create word index within each document ungroup corp_df_word Creating unique indices for your data is very important. In corpus linguistic analysis, we often need to keep track of the original context of the word, phrase or sentence in the concordances. All these unique indices (as well as the source text filenames) would make things a lot easier. Also, if the metadata of the source documents are available, these unique indices would allow us to connect the tokenized linguistic units to the metadata information (e.g., genres, registers, author profiles) for more interesting analysis. Therefore, after tokenization, we have obtained a line-based and a word-based data frame of our corpus data. Case Study: Word Frequency and Wordcloud With a word-based data frame, we can easily create a word frequency list as well as a word cloud to have a quick overview of the word distribution of the corpus. It should be noted that before creating the frequency list, we often need to consider whether to remove unimportant tokens (e.g., stopwords, symbols, punctuation, digits, alphabets.) We can represent any character in Unicode in the form of \\uXXXX, where the XXXX refers to the coding numbers of the character in Unicode (UTF-8) in hexadecimal format. For example, can you tell which character \\u6211 refers to? How about \\u4f60? In the above regular expression, the Unicode range [\\u4E00-\\u9FFF] refers to a set of frequently used Chinese characters. Therefore, the way we remove unimportant word tokens is to identify all word tokens consisting of these frequently used Chinese characters that fall within this Unicode range. For more information related to the Unicode range for the punctuation marks in CJK languages, please see this SO discussion thread. ## load chinese stopwords stopwords_chi &lt;- readLines(&quot;demo_data/stopwords-ch-jiebar-zht.txt&quot;) ## create word freq list corp_word_freq &lt;- corp_df_word %&gt;% filter(!word %in% stopwords_chi) %&gt;% # remove stopwords filter(word %&gt;% str_detect(pattern = &quot;[\\u4E00-\\u9FFF]+&quot;)) %&gt;% # remove words consisting of digits count(word) %&gt;% arrange(desc(n)) library(wordcloud2) corp_word_freq %&gt;% filter(n &gt; 20) %&gt;% filter(nchar(word) &gt;= 2) %&gt;% ## remove monosyllabic tokens wordcloud2(shape = &quot;pentagon&quot;, size = 0.3) Case Study: Patterns In this case study, we are looking at a more complex example. In corpus linguistic analysis, we often need to extract a particular pattern from the texts. In order to retrieve the target patterns at a high accuracy rate, we often need to make use of the additional annotations provided by the corpus.The most often-used information is the parts-of-speech tags of words. In this example, we will demonstrate how to enrich our corpus data by adding POS tags information to our current tidy corpus design. Our steps are as follows: Initialize jiebar object, which performs not only word segmentation but also POS tagging; Create a self-defined function to word-seg and pos-tag each text and combine all tokens, word/tag, into a long string for each text; With the line-based data frame apple_df_line, create a new column, which includes the enriched version of each text chunk, using mutate() # define a function to word-seg and pos-tag a text fragment tag_text &lt;- function(x, jiebar) { segment(x, jiebar) %&gt;% ## tokenize + POS-tagging paste(names(.), sep = &quot;/&quot;, collapse = &quot; &quot;) ## reformat output } A quick example of the function’s usage: # demo of the function `tag_text()` tag_text(corp_df_line$line[10], my_seg_pos) [1] &quot;貪/v 一點/m 愛/zg 什麼/r 痛/a 也/d 允許/v&quot; # apply `tag_text()` function to each text corp_df_line &lt;- corp_df_line %&gt;% mutate(line_tag = map_chr(line, tag_text, my_seg_pos)) corp_df_line Now we have obtained an enriched version of all the texts, we can make use of the POS tags for more linguistic analyses. For example, we can examine the use of adjectives in lyrics. The data retrieval procedure is now very straightforward: we only need to create a regular expression that matches our interested pattern and go through the enriched version of the texts (i.e., line_tag column in apple_df_line) to identify these matches with unnest_tokens(). 1.Define a regular expression [^/\\\\s]+/a\\\\b for adjectives; 2.Use unnest_tokens() and str_extract_all() to extract target patterns and create a pattern-based data frame. ## define regex patterns pat &lt;- &quot;[^/\\\\s]+/a\\\\b&quot; ## extract patterns from corp corp_df_pat &lt;- corp_df_line %&gt;% unnest_tokens( output = pat, ## new unit name input = line_tag, ## old unit name token = function(x) ## unnesting method str_extract_all(x, pattern = pat) ) ## Data wrangling/cleaning corp_df_pat_2 &lt;- corp_df_pat %&gt;% mutate(word = str_replace_all(pat, &quot;/.+$&quot;,&quot;&quot;)) %&gt;% ## clean regex group_by(artist) %&gt;% ## split df by artist count(word, sort = T) %&gt;% ## create freq list for each artist top_n(20, n) %&gt;% ## select top 20 for each artist ungroup %&gt;% ## merge df again arrange(artist, -n) ## sort result ## Visulization output corp_df_pat_2 %&gt;% mutate(word = reorder_within(word, n, artist)) %&gt;% ggplot(aes(word, n, fill=artist)) + geom_bar(stat=&quot;identity&quot;)+ coord_flip()+ facet_wrap(~artist,scales = &quot;free_y&quot;) + scale_x_reordered() + labs(x = &quot;Frequency&quot;, y = &quot;Adjectives&quot;, title=&quot;Top 20 Adjectives of Each Artist&#39;s Songs&quot;) Case Study: Lexical Bundles N-grams Extraction With word boundaries, we can also analyze the recurrent multiword units in the corpus. In this example, let’s take a look at the recurrent four-word sequences (i.e., four-grams) in our corpus. As the default n-gram tokenization in unnest_tokens(..., token = \"ngrams\") only works with the English data, we need to define our own ngram tokenization functions. The Chinese ngram tokenization function should: Tokenize each text fragment (i.e., lines) into word tokens Create a set of ngrams from the word tokens of each text ## self defined ngram tokenizer tokenizer_ngrams &lt;- function(texts, jiebar, n = 2 , skip = 0, delimiter = &quot;_&quot;) { texts %&gt;% ## given a vector of lines/chunks segment(jiebar) %&gt;% ## word tokenization as.tokens %&gt;% ## list to tokens tokens_ngrams(n, skip, concatenator = delimiter) %&gt;% ## ngram tokenization as.list ## tokens to list } In the above self-defined ngram tokenizer, we make use of tokens_ngrams() in quanteda, which creates a set of ngrams from already tokenized text objects, i.e., tokens. Because this function requires a tokens object as the input, we need to do the class conversion via as.tokens() and as.list(). Take a look at the following examples for a quick overview of tokens_ngrams(): sents &lt;- c(&quot;Jack and Jill went up the hill to fetch a pail of water&quot;, &quot;Jack fell down and broke his crown and Jill came tumbling after&quot;) sents_tokens &lt;- tokens(sents) ## English tokenization tokens_ngrams(sents_tokens, n = 2, skip = 0) Tokens consisting of 2 documents. text1 : [1] &quot;Jack_and&quot; &quot;and_Jill&quot; &quot;Jill_went&quot; &quot;went_up&quot; &quot;up_the&quot; &quot;the_hill&quot; [7] &quot;hill_to&quot; &quot;to_fetch&quot; &quot;fetch_a&quot; &quot;a_pail&quot; &quot;pail_of&quot; &quot;of_water&quot; text2 : [1] &quot;Jack_fell&quot; &quot;fell_down&quot; &quot;down_and&quot; &quot;and_broke&quot; [5] &quot;broke_his&quot; &quot;his_crown&quot; &quot;crown_and&quot; &quot;and_Jill&quot; [9] &quot;Jill_came&quot; &quot;came_tumbling&quot; &quot;tumbling_after&quot; tokens_ngrams(sents_tokens, n = 2, skip = 1) Tokens consisting of 2 documents. text1 : [1] &quot;Jack_Jill&quot; &quot;and_went&quot; &quot;Jill_up&quot; &quot;went_the&quot; &quot;up_hill&quot; [6] &quot;the_to&quot; &quot;hill_fetch&quot; &quot;to_a&quot; &quot;fetch_pail&quot; &quot;a_of&quot; [11] &quot;pail_water&quot; text2 : [1] &quot;Jack_down&quot; &quot;fell_and&quot; &quot;down_broke&quot; &quot;and_his&quot; [5] &quot;broke_crown&quot; &quot;his_and&quot; &quot;crown_Jill&quot; &quot;and_came&quot; [9] &quot;Jill_tumbling&quot; &quot;came_after&quot; A quick example of how to use the self-defined function tokenizer_ngrams(): # examples texts &lt;- c(&quot;這是一個測試的句子&quot;, &quot;這句子&quot;, &quot;超短句&quot;, &quot;最後一個超長的句子測試&quot;) tokenizer_ngrams( texts = texts, jiebar = my_seg, n = 2, skip = 0, delimiter = &quot;_&quot; ) $text1 [1] &quot;這是_一個&quot; &quot;一個_測試&quot; &quot;測試_的&quot; &quot;的_句子&quot; $text2 [1] &quot;這_句子&quot; $text3 [1] &quot;超短_句&quot; $text4 [1] &quot;最後_一個&quot; &quot;一個_超長&quot; &quot;超長_的&quot; &quot;的_句子&quot; &quot;句子_測試&quot; tokenizer_ngrams( texts = texts, jiebar = my_seg, n = 2, skip = 1, delimiter = &quot;_&quot; ) $text1 [1] &quot;這是_測試&quot; &quot;一個_的&quot; &quot;測試_句子&quot; $text2 character(0) $text3 character(0) $text4 [1] &quot;最後_超長&quot; &quot;一個_的&quot; &quot;超長_句子&quot; &quot;的_測試&quot; tokenizer_ngrams( texts = texts, jiebar = my_seg, n = 5, skip=0, delimiter = &quot;_&quot; ) $text1 [1] &quot;這是_一個_測試_的_句子&quot; $text2 character(0) $text3 character(0) $text4 [1] &quot;最後_一個_超長_的_句子&quot; &quot;一個_超長_的_句子_測試&quot; With the self-defined ngram tokenizer, we can now perform the ngram tokenization on our corpus. We will use the line-based data frame (corp_df_line) as our starting point: We transform the line-based data frame into an ngram-based data frame using unnest_tokens(...) with the self-defined tokenization function tokenizer_ngrams() We remove empty and unwanted n-grams entries: Empty ngrams due to short texts Ngrams spanning punctuations, symbols, or paragraph breaks Ngrams including alphanumeric characters ## from line-based to ngram-based corp_df_ngram &lt;- corp_df_line %&gt;% unnest_tokens( ngram, ## new unit name line, ## old unit name token = function(x) ## unnesting method tokenizer_ngrams( texts = x, jiebar = my_seg, n = 4, skip = 0, delimiter = &quot;_&quot; ) ) Warning: Outer names are only allowed for unnamed scalar atomic inputs ## remove unwanted ngrams corp_df_ngram_2 &lt;- corp_df_ngram %&gt;% filter(nzchar(ngram)) %&gt;% ## empty strings filter(!str_detect(ngram, &quot;[^\\u4E00-\\u9FFF_]&quot;)) ## remove unwanted ngrams Frequency and Dispersion A multiword unit can be defined based on at least two important distributional properties (See Biber, Conrad, and Cortes (2004)): The frequency of the whole multiword unit (i.e., frequency) The number of different texts where the multiword unit is observed (i.e., dispersion) Now that we have the ngram-based data frame, we can compute their token frequencies and document frequencies in the corpus using the normal data manipulation tricks. We set cut-offs for four-grams at: dispersion &gt;= 5 (i.e., four-grams that occur in at least five different documents) corp_ngram_dist &lt;- corp_df_ngram_2 %&gt;% group_by(ngram) %&gt;% summarize(freq = n(), dispersion = n_distinct(doc_id)) %&gt;% filter(dispersion &gt;= 3) Please take a look at the four-grams, arranged by frequency and dispersion respectively: # arrange by dispersion corp_ngram_dist %&gt;% arrange(desc(dispersion)) %&gt;% head(10) # arrange by freq corp_ngram_dist %&gt;% arrange(desc(freq)) %&gt;% head(10) We can also look at four-grams with particular lexical words: corp_ngram_dist %&gt;% filter(str_detect(ngram, &quot;我&quot;)) %&gt;% arrange(desc(dispersion)) corp_ngram_dist %&gt;% filter(str_detect(ngram, &quot;你&quot;)) %&gt;% arrange(desc(dispersion)) Quanteda Framework All the above examples demonstrate the Chinese data processing with the tidytext framework. Here, we look at a few more examples of processing the data with the quanteda framework. For Chinese data, the most important base unit in Quanteda is the tokens object. So first we need to create the tokens object based on the jiebaR tokenization method. ## create tokens based on self-defined segmentation corp_tokens &lt;- corp_df_text$lyric %&gt;% map(str_split,&quot;\\n+&quot;, simplify=TRUE) %&gt;% ## line tokenization map(segment, my_seg) %&gt;% ## word segmentation map(unlist) %&gt;% ## reformat structure as.tokens ## list to tokens ## add document-level metadata docvars(corp_tokens) &lt;- corp_df_text[, c(&quot;artist&quot;,&quot;lyricist&quot;,&quot;composer&quot;,&quot;title&quot;,&quot;gender&quot;)] Case Study: Concordances with kwic() This is an example of processing the Chinese data under Quanteda framework. Without relying on the Quanteda-native tokenization, we have created the tokens object directly based on the output of segment(). With this tokens object, we can perform the concordance analysis with kwic(). kwic(corp_tokens, &quot;快樂&quot;) Because we have also added the document-level information, we can utilize this metadata and perform more interesting analysis. For example, we can examine the concordance lines of a keyword in a subet of the corpus: corp_tokens_subset &lt;-tokens_subset(corp_tokens, str_detect(lyricist, &quot;方文山&quot;)) textplot_xray( kwic(corp_tokens_subset, &quot;快樂&quot;), kwic(corp_tokens_subset, &quot;難過&quot;)) Case Study: Comparison Word Cloud In quanteda, we can quickly create a comparison cloud, showing the the differences of the lexical distributions in different corpus subsets. corp_tokens %&gt;% dfm() %&gt;% dfm_remove(pattern= stopwords_chi, ## remove stopwords valuetype=&quot;fixed&quot;) %&gt;% dfm_keep(pattern = &quot;[\\u4E00-\\u9FFF]+&quot;, ## include freq chinese char valuetype= &quot;regex&quot;) %&gt;% dfm_group (groups = artist) %&gt;% ## group by artist dfm_trim(min_termfreq = 10, ## distributional cutoffs min_docfreq = 2, verbose = F) %&gt;% textplot_wordcloud(comparison=TRUE, min_size = 0.8, max_size = 4) Case Study 2: Collocations ## extract collocations corp_collocations &lt;- corp_tokens %&gt;% tokens_keep(pattern = &quot;[\\u4E00-\\u9FFF]+&quot;, ## include freq chinese char valuetype= &quot;regex&quot;) %&gt;% textstat_collocations( size = 2, min_count = 10) top_n(corp_collocations, 20, z) Recap Tokenizations are complex in Chinese text processing. Many factors may need to be taken into account when determining the right tokenization method. In particular, several important questions may be relevant to Chinese text tokenization: Do you need the parts-of-speech tags of words in your research? What is the base unit you would like to work with? Texts? Paragraphs? Chunks? Sentences? N-grams? Words? Do you need non-word tokens such as symbols, punctuation, digits, or alphabets in your analysis? Your answers to the above questions should help you determine the most effective structure of the tokenization methods for your data. 感謝聆聽! References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
